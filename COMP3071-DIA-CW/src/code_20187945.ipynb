{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aa03741",
   "metadata": {},
   "source": [
    "**MALVI BID - 20187945**\n",
    "\n",
    "This is my coursework submission for COMP3071 Designing Intelligent Agents (Spring 2022/2023) at the School of Computer Science, University of Nottingham Malaysia.\n",
    "\n",
    "Please check the submitted `README.md` file for instructions on how to set up the python environment to run this code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79ac604f",
   "metadata": {},
   "source": [
    "**Useful links:**\n",
    "\n",
    "Project repository: https://github.com/malvibid/COMP3071-Designing-Intelligent-Agents/tree/coursework/COMP3071-DIA-CW/src\n",
    "\n",
    "Best trained model: https://github.com/malvibid/COMP3071-Designing-Intelligent-Agents/tree/coursework/COMP3071-DIA-CW/src/best_trained_model\n",
    "\n",
    "Video featuring a selection of test rounds demonstrating the best trained agent playing the game: https://youtu.be/QFf0_4FCh0w\n",
    "\n",
    "All trained models: https://github.com/malvibid/COMP3071-Designing-Intelligent-Agents/tree/coursework/COMP3071-DIA-CW/src/trained_models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57dec060-bf08-4a6e-a1d4-1839b4730f7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Custom Dino Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e94c8c1-c5e5-4b8f-8c22-242647e9b880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64147db4-ea80-49a1-b50e-1fce4f97af98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Environment Components\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "# Selenium for automatically loading and play the game\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa02f78b-c77c-4ac9-9a35-7ab41ed4e123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoEnvironment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0937a3c-edcf-487a-af0d-a178a1f5b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dino Game Environment\n",
    "class DinoEnvironment(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "\n",
    "        self.driver = self._create_driver()\n",
    "\n",
    "        # Setup spaces\n",
    "        low_values = np.array(\n",
    "            [0, 0, 0, 6, -1, -1, -1, -1, -1, -1], dtype=np.float32)  # Initial speed is 6, while max speed is 13\n",
    "        high_values = np.array(\n",
    "            [150, 1, 1, 13, 600, 3, 600, 150, 50, 50], dtype=np.float32)  # Canvas dimensions are 600x150\n",
    "        self.observation_space = Box(\n",
    "            low=low_values, high=high_values, shape=(10,), dtype=np.float32)\n",
    "\n",
    "        # Start jumping, Start ducking, Stop ducking, Do nothing - Ducking has been divided into two actions because the agent should also learn the correct ducking duration\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.actions_map = [\n",
    "            (Keys.ARROW_UP, \"key_down\"),  # Start jumping\n",
    "            (Keys.ARROW_DOWN, \"key_down\"),  # Start ducking\n",
    "            (Keys.ARROW_DOWN, \"key_up\"),  # Stop ducking\n",
    "            (Keys.ARROW_RIGHT, \"key_down\")  # Do nothing\n",
    "        ]\n",
    "\n",
    "        # Keep track of number of obstacles the agent has passed\n",
    "        self.passed_obstacles = 0\n",
    "\n",
    "    # Create and return an instance of the Chrome Driver\n",
    "    def _create_driver(self):\n",
    "\n",
    "        # Set options for the WebDriver\n",
    "        options = Options()\n",
    "\n",
    "        # Turn off logging to keep terminal clean\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "        # Keep the browser running after the code finishes executing\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        # Create a Service instance for running the ChromeDriver executable\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "        # Create an instance of the Chrome WebDriver with the specified service and options - The driver object can be used to automate interactions with the Chrome browser\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Maximize the Chrome window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # Encode the obstacle type as an integer\n",
    "    def _encode_obstacle_type(self, obstacle_type):\n",
    "        if obstacle_type == 'CACTUS_SMALL':\n",
    "            return 0\n",
    "        elif obstacle_type == 'CACTUS_LARGE':\n",
    "            return 1\n",
    "        elif obstacle_type == 'PTERODACTYL':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown obstacle type: {obstacle_type}\")\n",
    "\n",
    "    # Get obstacles that are currently on the screen\n",
    "    def _get_obstacles(self):\n",
    "        obstacles = self.driver.execute_script(\n",
    "            \"return Runner.instance_.horizon.obstacles\")\n",
    "        obstacle_info = []\n",
    "        for obstacle in obstacles:\n",
    "            obstacle_type = obstacle['typeConfig']['type']\n",
    "            # Encode the obstacle type as an integer\n",
    "            encoded_obstacle_type = self._encode_obstacle_type(obstacle_type)\n",
    "            obstacle_x = obstacle['xPos']\n",
    "            obstacle_y = obstacle['yPos']\n",
    "            obstacle_width = obstacle['typeConfig']['width']\n",
    "            obstacle_height = obstacle['typeConfig']['height']\n",
    "            obstacle_info.append(\n",
    "                (encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height))\n",
    "        return obstacle_info\n",
    "\n",
    "    # Get Trex's state (Jumping, Ducking or Running/Do nothing)\n",
    "    def _get_trex_info(self):\n",
    "        trex = self.driver.execute_script(\"return Runner.instance_.tRex\")\n",
    "        # xpos remains the same throughout the game - don't need it\n",
    "        trex_y = trex['yPos']\n",
    "        trex_is_jumping = trex['jumping']\n",
    "        trex_is_ducking = trex['ducking']\n",
    "        return trex_y, trex_is_jumping, trex_is_ducking\n",
    "\n",
    "    # Get current game speed\n",
    "    def _get_game_speed(self):\n",
    "        game_speed = self.driver.execute_script(\n",
    "            \"return Runner.instance_.currentSpeed\")\n",
    "        return game_speed\n",
    "\n",
    "    # Get the distance between the Trex and the next obstacle\n",
    "    def _get_distance_to_next_obstacle(self):\n",
    "        trex_x = self.driver.execute_script(\n",
    "            \"return Runner.instance_.tRex.xPos\")  # xpos of trex\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            next_obstacle = obstacles[0]\n",
    "            obstacle_x = next_obstacle[1]  # xpos of next obstacle\n",
    "            distance_to_next_obstacle = obstacle_x - trex_x\n",
    "        else:\n",
    "            distance_to_next_obstacle = None\n",
    "        return distance_to_next_obstacle\n",
    "\n",
    "    # Check if the agent has passed an obstacle\n",
    "    def _passed_obstacle(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            # next_obstacle: [encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height]\n",
    "            next_obstacle = obstacles[0]\n",
    "            trex_x = self.driver.execute_script(\n",
    "                \"return Runner.instance_.tRex.xPos\")\n",
    "            obstacle_x = next_obstacle[1]  # Next obstacles xpos\n",
    "            obstacle_width = next_obstacle[3]  # Next obstacles width\n",
    "            return obstacle_x + obstacle_width < trex_x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Get and return the score for the last game played\n",
    "    def _get_current_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.digits\")))\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Get and return the high score for all games played in current browser session\n",
    "    def _get_high_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.highScore.slice(-5)\")))  # MaxScore=99999, MaxScoreUnits=5\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Capture screenshot of current game state and return the image captured for rendering\n",
    "    def _get_image(self):\n",
    "        # Capture a screenshot of the game canvas as a data URL - string that represents the image in base64-encoded format\n",
    "        data_url = self.driver.execute_script(\n",
    "            \"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "\n",
    "        # Remove the leading text from the data URL using string slicing and decode the remaining base64-encoded data\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        image_data = base64.b64decode(data_url[len(LEADING_TEXT):])\n",
    "\n",
    "        # Convert the binary data in 'image_data' to a 1D NumPy array\n",
    "        image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "\n",
    "        # Decode the image data and create an OpenCV image object - OpenCV Image Shape format (H, W, C) ( rows, columns, and channels )\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Load and Reset the game environment\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Navigate to the Chrome Dino website\n",
    "            self.driver.get(\"chrome://dino/\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            # Ignore \"ERR_INTERNET_DISCONNECTED\" error thrown because this game is available offline\n",
    "            if \"ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                pass  # Ignore the exception.\n",
    "            else:\n",
    "                raise e  # Handle other WebDriverExceptions\n",
    "\n",
    "        # Avoid errors that can arise due to the 'runner-canvas' element not being present - Using WebDriverWait and EC together ensures that the code does not proceed until the required element is present\n",
    "        timeout = 10\n",
    "        WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
    "\n",
    "        # Start game\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the current state of the game and return it as the observation\n",
    "    def get_observation(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        trex_y, trex_is_jumping, trex_is_ducking = self._get_trex_info()\n",
    "        game_speed = self._get_game_speed()\n",
    "        distance_to_next_obstacle = self._get_distance_to_next_obstacle()\n",
    "\n",
    "        state = (\n",
    "            trex_y,\n",
    "            trex_is_jumping,\n",
    "            trex_is_ducking,\n",
    "            game_speed,\n",
    "            distance_to_next_obstacle,\n",
    "            # Unpack the tuple of the first obstacle\n",
    "            *(obstacles[0] if obstacles else (None, None, None, None, None))\n",
    "        )\n",
    "\n",
    "        # Set dtype for state to float32 for consistency and compatibility with the RL algorithm\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        # Replace NaN values with -1\n",
    "        state[np.isnan(state)] = -1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Check if the game is over and return True or False\n",
    "    def is_game_over(self):\n",
    "        # Done if either Trex crashed into an obstacle or reached max score which is 99999\n",
    "        # Check if Trex crashed\n",
    "        crashed = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "        # Get the maximum score from the game\n",
    "        max_score = self.driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.maxScore\")\n",
    "        current_score = self._get_current_score()\n",
    "\n",
    "        return crashed or (current_score >= max_score)\n",
    "\n",
    "    # Calculate and return the reward for the current state of the game\n",
    "    def get_reward(self, done):\n",
    "        # Must maintain the relative importance of different rewards so that the agent can differentiate between the various outcomes and is encouraged to learn a good policy\n",
    "        reward = 0\n",
    "        if done:\n",
    "            # Penalize for crashing into an obstacle\n",
    "            reward -= 10\n",
    "        else:\n",
    "            if self._passed_obstacle():\n",
    "                # Reward for passing an obstacle\n",
    "                reward += 1\n",
    "                self.passed_obstacles += 1\n",
    "            else:\n",
    "                # Small reward for staying alive\n",
    "                reward += 0.1\n",
    "\n",
    "        current_score = self._get_current_score()\n",
    "        high_score = self._get_high_score()\n",
    "\n",
    "        if current_score > high_score:\n",
    "            # Bonus reward for surpassing the high score\n",
    "            reward += 1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Take a step in the game environment based on the given action\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take action\n",
    "        # Get key and action mapping\n",
    "        key, action_type = self.actions_map[action]\n",
    "\n",
    "        # Create a new ActionChains object\n",
    "        action_chains = ActionChains(self.driver)\n",
    "\n",
    "        # Perform the key press action\n",
    "        if action_type == \"key_down\":\n",
    "            action_chains.key_down(key).perform()\n",
    "        # Perform the key release action\n",
    "        elif action_type == \"key_up\":\n",
    "            action_chains.key_up(key).perform()\n",
    "\n",
    "        # Get next observation\n",
    "        obs = self.get_observation()\n",
    "\n",
    "        # Check whether game is over\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        # Get reward\n",
    "        reward = self.get_reward(done)\n",
    "\n",
    "        info = {\n",
    "            'current_score': self._get_current_score(),\n",
    "            'high_score': self._get_high_score()\n",
    "        }\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Visualise the game\n",
    "    def render(self, mode: str = 'human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb-array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            cv2.imshow('Dino Game', img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    # Close the game environment and the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a42599e-123f-4932-bbe4-8e97663b4222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the Custom Game Environment\n",
    "\n",
    "This section is for testing the Game Environment to ensure it is defined correctly before using it with the Agent for RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7bc403f4-7400-4656-ba2a-bd79673640d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper class to format and print observations properly\n",
    "def print_formatted_obs(observations):\n",
    "    obs_titles = [\"trex_y\", \"trex_jumping\", \"trex_ducking\", \"game_speed\", \"obst_dist\", \"obst_type\", \"obst_x\", \"obst_y\", \"obst_width\", \"obst_height\"]\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(observations, columns=obs_titles)\n",
    "\n",
    "    # Set the pandas display options for better readability (optional)\n",
    "    pd.set_option(\"display.width\", 140)\n",
    "    # pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cfb1a4d7-bf2a-492e-bbea-96ff3ecda78c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83.   ,  1.   ,  0.   ,  6.011, -1.   , -1.   , -1.   , -1.   ,\n",
       "       -1.   , -1.   ], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DinoEnvironment()\n",
    "env.reset() # returns an observation from the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dcc8c9b8-87a7-407e-bea4-bdbf7585fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0.  0.  0.  6. -1. -1. -1. -1. -1. -1.], [150.   1.   1.  13. 600.   3. 600. 150.  50.  50.], (10,), float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82fba9fa-0135-4ca0-ac02-e2620fe05f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd240adb-926c-4f23-b692-1063bf2942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fedc098e-9ef4-4fde-9ec2-1ed964c28bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "908c10c2-7cbb-40e1-9289-aaa14a95c7b9",
   "metadata": {},
   "source": [
    "**Note:** Render function works better if using `.py` python files instead of the `.ipynb` notebook to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75ab681d-89a2-41bc-9491-ab58d48e497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trex_y  trex_jumping  trex_ducking  game_speed  obst_dist  obst_type  obst_x  obst_y  obst_width  obst_height\n",
      "0      73.0           1.0           0.0       6.017       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "1      51.0           1.0           0.0       6.029       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "2      33.0           1.0           0.0       6.041       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "3      19.0           1.0           0.0       6.051       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "4      13.0           1.0           0.0       6.063       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "..      ...           ...           ...         ...        ...        ...     ...     ...         ...          ...\n",
      "137    93.0           0.0           0.0       7.719       99.0        0.0   105.0   105.0        17.0         35.0\n",
      "138    93.0           0.0           0.0       7.731       84.0        0.0    87.0   105.0        17.0         35.0\n",
      "139    93.0           0.0           0.0       7.741       66.0        0.0    72.0   105.0        17.0         35.0\n",
      "140    93.0           0.0           1.0       7.755       48.0        0.0    51.0   105.0        17.0         35.0\n",
      "141    93.0           0.0           1.0       7.759       38.0        0.0    39.0   105.0        17.0         35.0\n",
      "\n",
      "[142 rows x 10 columns]\n",
      "Episode: 0, Total Reward: 134.09999999999965, , Current Score: 60, High Score: 60\n"
     ]
    }
   ],
   "source": [
    "# Test loop - Play 1 game\n",
    "env = DinoEnvironment()\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = []\n",
    "    # images = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(obs)\n",
    "        all_observations.append(obs)  # Print obs formatted nicely in a table\n",
    "        total_reward += reward\n",
    "\n",
    "        # env.render(mode='human')\n",
    "        # img = env.render(mode='rgb-array')\n",
    "        # images.append(img) # Can use some image library to create a gif using collected images\n",
    "\n",
    "    print_formatted_obs(all_observations)\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, , Current Score: {info['current_score']}, High Score: {info['high_score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d77b60bb-0129-423a-b552-4add2bc034ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ModifiedDinoEnvironment Class\n",
    "\n",
    "In this version of the Environment I added information about the Trex to the state: Trex's standing height and width, and its ducking height and width. This increased the observation space shape from 10 to 14. My goal was to give the Trex enough information so that it make a decision whether to jump or duck and avoid unnecessary actions, to achieve this I experimented with different reward structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0872d9a1-3572-47d2-83de-22b6434ba516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dino Game Environment\n",
    "class ModifiedDinoEnvironment(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "\n",
    "        self.driver = self._create_driver()\n",
    "\n",
    "        # Setup spaces\n",
    "        low_values = np.array(\n",
    "            [0, -1, -1, -1, -1, 0, 0, 6, -1, -1, -1, -1, -1, -1], dtype=np.float32)  # Initial speed is 6, while max speed is 13\n",
    "        high_values = np.array(\n",
    "            [150, 50, 50, 50, 60, 1, 1, 13, 600, 3, 600, 150, 50, 50], dtype=np.float32)  # Canvas dimensions are 600x150\n",
    "        self.observation_space = Box(\n",
    "            low=low_values, high=high_values, shape=(14,), dtype=np.float32)\n",
    "\n",
    "        # Start jumping, Start ducking, Stop ducking, Do nothing - Ducking has been divided into two actions because the agent should also learn the correct ducking duration\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.actions_map = [\n",
    "            (Keys.ARROW_UP, \"key_down\"),  # Start jumping\n",
    "            (Keys.ARROW_DOWN, \"key_down\"),  # Start ducking\n",
    "            (Keys.ARROW_DOWN, \"key_up\"),  # Stop ducking\n",
    "            (Keys.ARROW_RIGHT, \"key_down\")  # Do nothing\n",
    "        ]\n",
    "\n",
    "        # Keep track of number of obstacles the agent has passed\n",
    "        self.passed_obstacles = 0\n",
    "\n",
    "    # Create and return an instance of the Chrome Driver\n",
    "    def _create_driver(self):\n",
    "\n",
    "        # Set options for the WebDriver\n",
    "        options = Options()\n",
    "\n",
    "        # Turn off logging to keep terminal clean\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "        # Keep the browser running after the code finishes executing\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        # Create a Service instance for running the ChromeDriver executable\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "        # Create an instance of the Chrome WebDriver with the specified service and options - The driver object can be used to automate interactions with the Chrome browser\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Maximize the Chrome window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # Encode the obstacle type as an integer\n",
    "    def _encode_obstacle_type(self, obstacle_type):\n",
    "        if obstacle_type == 'CACTUS_SMALL':\n",
    "            return 0\n",
    "        elif obstacle_type == 'CACTUS_LARGE':\n",
    "            return 1\n",
    "        elif obstacle_type == 'PTERODACTYL':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown obstacle type: {obstacle_type}\")\n",
    "\n",
    "    # Get obstacles that are currently on the screen\n",
    "    def _get_obstacles(self):\n",
    "        obstacles = self.driver.execute_script(\n",
    "            \"return Runner.instance_.horizon.obstacles\")\n",
    "        obstacle_info = []\n",
    "        for obstacle in obstacles:\n",
    "            obstacle_type = obstacle['typeConfig']['type']\n",
    "            # Encode the obstacle type as an integer\n",
    "            encoded_obstacle_type = self._encode_obstacle_type(obstacle_type)\n",
    "            obstacle_x = obstacle['xPos']\n",
    "            obstacle_y = obstacle['yPos']\n",
    "            obstacle_width = obstacle['typeConfig']['width']\n",
    "            obstacle_height = obstacle['typeConfig']['height']\n",
    "            obstacle_info.append(\n",
    "                (encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height))\n",
    "        return obstacle_info\n",
    "\n",
    "    # Get Trex's state (Jumping, Ducking or Running/Do nothing)\n",
    "    def _get_trex_info(self):\n",
    "        trex = self.driver.execute_script(\"return Runner.instance_.tRex\")\n",
    "        # xpos remains the same throughout the game - don't need it\n",
    "        trex_y = trex['yPos']\n",
    "        trex_height = trex['config']['HEIGHT']\n",
    "        trex_width = trex['config']['WIDTH']\n",
    "        trex_duck_height = trex['config']['HEIGHT_DUCK']\n",
    "        trex_duck_width = trex['config']['WIDTH_DUCK']\n",
    "        trex_is_jumping = trex['jumping']\n",
    "        trex_is_ducking = trex['ducking']\n",
    "        return trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking\n",
    "\n",
    "    # Get current game speed\n",
    "    def _get_game_speed(self):\n",
    "        game_speed = self.driver.execute_script(\n",
    "            \"return Runner.instance_.currentSpeed\")\n",
    "        return game_speed\n",
    "\n",
    "    # Get the distance between the Trex and the next obstacle\n",
    "    def _get_distance_to_next_obstacle(self):\n",
    "        trex_x = self.driver.execute_script(\n",
    "            \"return Runner.instance_.tRex.xPos\")  # xpos of trex\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            next_obstacle = obstacles[0]\n",
    "            obstacle_x = next_obstacle[1]  # xpos of next obstacle\n",
    "            distance_to_next_obstacle = obstacle_x - trex_x\n",
    "        else:\n",
    "            distance_to_next_obstacle = None\n",
    "        return distance_to_next_obstacle\n",
    "\n",
    "    # Check if the agent has passed an obstacle\n",
    "    def _passed_obstacle(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            # next_obstacle: [encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height]\n",
    "            next_obstacle = obstacles[0]\n",
    "            trex_x = self.driver.execute_script(\n",
    "                \"return Runner.instance_.tRex.xPos\")\n",
    "            obstacle_x = next_obstacle[1]  # Next obstacles xpos\n",
    "            obstacle_width = next_obstacle[3]  # Next obstacles width\n",
    "            return obstacle_x + obstacle_width < trex_x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Get and return the score for the last game played\n",
    "    def _get_current_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.digits\")))\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Get and return the high score for all games played in current browser session\n",
    "    def _get_high_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.highScore.slice(-5)\")))  # MaxScore=99999, MaxScoreUnits=5\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Capture screenshot of current game state and return the image captured for rendering\n",
    "    def _get_image(self):\n",
    "        # Capture a screenshot of the game canvas as a data URL - string that represents the image in base64-encoded format\n",
    "        data_url = self.driver.execute_script(\n",
    "            \"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "\n",
    "        # Remove the leading text from the data URL using string slicing and decode the remaining base64-encoded data\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        image_data = base64.b64decode(data_url[len(LEADING_TEXT):])\n",
    "\n",
    "        # Convert the binary data in 'image_data' to a 1D NumPy array\n",
    "        image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "\n",
    "        # Decode the image data and create an OpenCV image object - OpenCV Image Shape format (H, W, C) ( rows, columns, and channels )\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Load and Reset the game environment\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Navigate to the Chrome Dino website\n",
    "            self.driver.get(\"chrome://dino/\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            # Ignore \"ERR_INTERNET_DISCONNECTED\" error thrown because this game is available offline\n",
    "            if \"ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                pass  # Ignore the exception.\n",
    "            else:\n",
    "                raise e  # Handle other WebDriverExceptions\n",
    "\n",
    "        # Avoid errors that can arise due to the 'runner-canvas' element not being present - Using WebDriverWait and EC together ensures that the code does not proceed until the required element is present\n",
    "        timeout = 10\n",
    "        WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
    "\n",
    "        # Start game\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the current state of the game and return it as the observation\n",
    "    def get_observation(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking = self._get_trex_info()\n",
    "        game_speed = self._get_game_speed()\n",
    "        distance_to_next_obstacle = self._get_distance_to_next_obstacle()\n",
    "\n",
    "        state = (\n",
    "            trex_y,\n",
    "            trex_height,\n",
    "            trex_width,\n",
    "            trex_duck_height,\n",
    "            trex_duck_width,\n",
    "            trex_is_jumping,\n",
    "            trex_is_ducking,\n",
    "            game_speed,\n",
    "            distance_to_next_obstacle,\n",
    "            # Unpack the tuple of the first obstacle\n",
    "            *(obstacles[0] if obstacles else (None, None, None, None, None))\n",
    "        )\n",
    "\n",
    "        # Set dtype for state to float32 for consistency and compatibility with the RL algorithm\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        # Replace NaN values with -1\n",
    "        state[np.isnan(state)] = -1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Check if the game is over and return True or False\n",
    "    def is_game_over(self):\n",
    "        # Done if either Trex crashed into an obstacle or reached max score which is 99999\n",
    "        # Check if Trex crashed\n",
    "        crashed = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "        # Get the maximum score from the game\n",
    "        max_score = self.driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.maxScore\")\n",
    "        current_score = self._get_current_score()\n",
    "\n",
    "        return crashed or (current_score >= max_score)\n",
    "\n",
    "    # Calculate and return the reward for the current state of the game\n",
    "    def get_reward(self, obs, done, info):\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        current_score = info['current_score']\n",
    "        high_score = info['high_score']\n",
    "\n",
    "        if done:\n",
    "            # Penalize for crashing into an obstacle\n",
    "            reward -= 10\n",
    "\n",
    "            if current_score > high_score:\n",
    "                # Bonus reward for surpassing the high score\n",
    "                reward += 10\n",
    "        else:\n",
    "\n",
    "            # Reward for staying alive\n",
    "            reward += 1\n",
    "\n",
    "            trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking, game_speed, distance_to_next_obstacle, obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height = obs\n",
    "\n",
    "            # Penalize unnecessary jumps and ducks when there are no obstacles\n",
    "            if obstacle_type == -1:\n",
    "                if trex_is_jumping:\n",
    "                    # Penalize for jumping when there are no obstacles\n",
    "                    reward -= 0.5\n",
    "                if trex_is_ducking:\n",
    "                    # Penalize for ducking when there are no obstacles\n",
    "                    reward -= 0.5\n",
    "\n",
    "            # Penalize for taking incorrect action when there are obtacles\n",
    "            if obstacle_type != -1:\n",
    "                if trex_is_jumping and (obstacle_y + obstacle_height) < trex_duck_height:\n",
    "                    # Penalize for jumping when the obstacle is flying and there's enough space to duck\n",
    "                    reward -= 0.1\n",
    "\n",
    "                if trex_is_ducking and (trex_y + trex_height) > (obstacle_y):\n",
    "                    # Penalize for ducking when the obstacle is on the ground\n",
    "                    reward -= 0.1\n",
    "\n",
    "            if self._passed_obstacle():\n",
    "                # Reward for passing an obstacle\n",
    "                reward += 1\n",
    "                self.passed_obstacles += 1\n",
    "\n",
    "            if current_score > high_score:\n",
    "                # Small reward for every step the high score surpasses current score\n",
    "                reward += 0.1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Take a step in the game environment based on the given action\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take action\n",
    "        # Get key and action mapping\n",
    "        key, action_type = self.actions_map[action]\n",
    "\n",
    "        # Create a new ActionChains object\n",
    "        action_chains = ActionChains(self.driver)\n",
    "\n",
    "        # Perform the key press action\n",
    "        if action_type == \"key_down\":\n",
    "            action_chains.key_down(key).perform()\n",
    "        # Perform the key release action\n",
    "        elif action_type == \"key_up\":\n",
    "            action_chains.key_up(key).perform()\n",
    "\n",
    "        # Get next observation\n",
    "        obs = self.get_observation()\n",
    "\n",
    "        # Check whether game is over\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        info = {\n",
    "            'current_score': self._get_current_score(),\n",
    "            'high_score': self._get_high_score()\n",
    "        }\n",
    "\n",
    "        # Get reward\n",
    "        reward = self.get_reward(obs, done, info)\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Visualise the game\n",
    "    def render(self, mode: str = 'human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb-array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            cv2.imshow('Dino Game', img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    # Close the game environment and the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "973cc5ce-01c7-413a-bf88-454348abada9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the Modified Custom Game Environment\n",
    "\n",
    "This section is for testing the Game Environment to ensure it is defined correctly before using it with the Agent for RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99ceb6bd-ad44-4fca-92bd-cde697be6ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper class to format and print observations properly\n",
    "def print_formatted_obs(observations):\n",
    "    obs_titles = [\"trex_y\", \"trex_h\",\"trex_w\",\"tr_duck_h\",\"tr_duck_w\",\"trex_jump\", \"trex_duck\", \"game_speed\", \"obst_dist\", \"obst_type\", \"obst_x\", \"obst_y\", \"obst_w\", \"obst_h\"]\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(observations, columns=obs_titles)\n",
    "\n",
    "    # Set the pandas display options for better readability (optional)\n",
    "    pd.set_option(\"display.width\", 140)\n",
    "    # pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3431e73-de5f-4495-b4ac-517628999b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ModifiedDinoEnvironment()\n",
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d782d8a5-5e13-4a63-8597-7b2a2706705d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trex_y  trex_h  trex_w  tr_duck_h  tr_duck_w  trex_jump  trex_duck  game_speed  obst_dist  obst_type  obst_x  obst_y  obst_w  obst_h\n",
      "0      67.0    47.0    44.0       25.0       59.0        1.0        0.0       6.019       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "1      47.0    47.0    44.0       25.0       59.0        1.0        0.0       6.031       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "2      55.0    47.0    44.0       25.0       59.0        1.0        0.0       6.041       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "3      72.0    47.0    44.0       25.0       59.0        1.0        0.0       6.051       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "4      80.0    47.0    44.0       25.0       59.0        1.0        0.0       6.061       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "..      ...     ...     ...        ...        ...        ...        ...         ...        ...        ...     ...     ...     ...     ...\n",
      "144    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.501       93.0        1.0    98.0    90.0    25.0    50.0\n",
      "145    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.511       78.0        1.0    84.0    90.0    25.0    50.0\n",
      "146    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.523       60.0        1.0    66.0    90.0    25.0    50.0\n",
      "147    93.0    47.0    44.0       25.0       59.0        1.0        1.0       7.533       46.0        1.0    51.0    90.0    25.0    50.0\n",
      "148    93.0    47.0    44.0       25.0       59.0        1.0        1.0       7.537       40.0        1.0    41.0    90.0    25.0    50.0\n",
      "\n",
      "[149 rows x 14 columns]\n",
      "Episode: 0, Total Reward: 104.49999999999986, , Current Score: 52, High Score: 52\n"
     ]
    }
   ],
   "source": [
    "# Test loop - Play 1 game\n",
    "env = ModifiedDinoEnvironment()\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = []\n",
    "    # images = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(obs)\n",
    "        all_observations.append(obs)  # Print obs formatted nicely in a table\n",
    "        total_reward += reward\n",
    "\n",
    "        # env.render(mode='human')\n",
    "        # img = env.render(mode='rgb-array')\n",
    "        # images.append(img) # Can use some image library to create a gif using collected images\n",
    "\n",
    "    print_formatted_obs(all_observations)\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, , Current Score: {info['current_score']}, High Score: {info['high_score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf346759-391a-4a48-81c2-e5943ed23ad5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# DQN Dino Agent\n",
    "\n",
    "\n",
    "The DQN algorithm for this agent has been adopted from these two papers:\n",
    "\n",
    "1. Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. https://doi.org/10.48550/ARXIV.1312.5602\n",
    "2. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533. https://doi.org/10.1038/nature14236"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc910af-22e2-45ed-8022-ace974d90ed2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4efbeb-851d-4a23-b8c6-0f33f2a27645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c0862b2-3e86-4c79-ad98-078ce2e5ddcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoDQNAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2b9a48-e081-4727-b071-807a90f131e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoDQNAgent():\n",
    "    def __init__(self, env,\n",
    "                 gamma=0.95,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=32,\n",
    "                 memory_size=100000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]  # 10\n",
    "        self.action_size = env.action_space.n  # 4\n",
    "        self.hidden_sizes = [64, 128]  # number of hidden neurons for the model\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma  # discounting factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min  # min exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # exploration decay per step\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the DQN model architecture - This model will be used to approximate the Q-values of the agent's actions given a state.\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], self.action_size)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Store agents experiences as a tuple\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Determine which action to take given a state\n",
    "    def act(self, state):\n",
    "        # Explore randomly or exploit given the current epsilon value\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            return action\n",
    "\n",
    "    # Update the DQN model using a batch of experiences sampled from the memory\n",
    "    def replay(self):\n",
    "        # Check if the number of experiences (state, action, reward, next_state, done) in the memory is less than the batch size\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # Don't do anything since there's not enough data to create a minibatch for training\n",
    "            return\n",
    "\n",
    "        # Create minibatch from a random sample of experiences from the memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Calculate the expected Q-value for the current state-action pair (q_target)\n",
    "            # If done, - Game has ended, don't need to make predictions about future rewards\n",
    "            q_target = reward\n",
    "            if not done:\n",
    "                # Calculate the Q-values for the next state using the DQN model, i.e., estimate future reward\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "                q_values_next = self.model(next_state)\n",
    "                # Update the target value by adding the discounted maximum Q-value of the next state to the current reward\n",
    "                q_target = reward + self.gamma * \\\n",
    "                    torch.max(q_values_next).item()\n",
    "\n",
    "            # Calculate the Q-values for the current state using the DQN model\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "\n",
    "            # Update/Map the expected Q-value of the chosen action with the calculated target value\n",
    "            q_values_expected = q_values.clone().detach()\n",
    "\n",
    "            q_values_expected[action] = q_target\n",
    "\n",
    "            # Note: q_values_expected is the ground truth for the action that the agent took in the current state vs q_values is the models prediction of what should happen\n",
    "\n",
    "            # Reset the gradients of the optimizer before performing backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss using the Mean Squared Error (MSE) between the current Q-values and the expected Q-values\n",
    "            loss = self.loss_fn(q_values, q_values_expected)\n",
    "\n",
    "            # Perform backpropagation to calculate the gradients of the model's parameters with respect to the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's parameters using the calculated gradients and the optimizer's learning rate\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decrease episolon over time to reduce exploration and increase exploitation of the models learnt knowledge\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # Return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "    # Save the current state of the DQN model and optimizer to a file.\n",
    "    def save_model(self, model_name, model_output_dir, log_to_wandb):\n",
    "        # Create a dictionary to store the state of the model, optimizer and any other additional information\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            model_output_dir, model_name)\n",
    "\n",
    "        # Save the state dictionary to a file\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "        if log_to_wandb:\n",
    "            # Save model as a wandb artifact\n",
    "            artifact = wandb.Artifact(model_name, type='model')\n",
    "            artifact.add_file(save_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "    # Load the DQN model and optimizer state from a file.\n",
    "    def load_model(self, file_path, older_model, for_training):\n",
    "\n",
    "        if older_model:\n",
    "            self.model.load_state_dict(torch.load(file_path))\n",
    "        else:\n",
    "            # Load the state dictionary from the file using the torch.load() function\n",
    "            state = torch.load(file_path)\n",
    "\n",
    "            # Restore the state of the model and optimizer\n",
    "            self.model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "            # Set for_training to true if using the model to continue training from a previously saved state\n",
    "            if for_training:\n",
    "                self.optimizer.load_state_dict(state['optimizer_state_dict'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1648970-0a2b-4fcd-b73f-e2c59c443464",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train and Test Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b742893d-09ed-4817-bc9e-2381bba3a6b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c883a5b-c4ac-4d5f-a20c-2817711396b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392289b8-6d05-438e-b037-60f46efe93b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "578dbe60-f461-4c8a-8303-3fcd1fd4ba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d966df0-fd81-489d-b867-a35a62148678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, env, episodes, model_output_dir, save_interval=10, log_to_wandb=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_dqn_agent', name='train_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "    \n",
    "    # test_interval = 10  # The interval at which to test the agent's knowledge\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        # # Set epsilon to 0 for every n episodes and then reset it back to the previous value\n",
    "        # previous_epsilon = None\n",
    "        # if episode % test_interval == 0:\n",
    "        #     previous_epsilon = agent.epsilon\n",
    "        #     agent.epsilon = 0\n",
    "\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Remember agents experience after every step\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Train/Update the model every episode\n",
    "        loss = agent.replay()\n",
    "        episode_loss.append(loss)\n",
    "        \n",
    "\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_episode_loss = sum(episode_loss) / len(episode_loss)\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Episode Loss: {loss:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "        \n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"episode_loss\": loss,\n",
    "                \"mean_loss\": mean_episode_loss,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "            \n",
    "        # # Reset epsilon back to its previous value if needed\n",
    "        # if previous_epsilon is not None:\n",
    "        #     agent.epsilon = previous_epsilon\n",
    "        #     previous_epsilon = None\n",
    "\n",
    "        # Save the model every save_interval episodes\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            model_name = f\"dino_dqn_episode_{episode + 1}.pth\"\n",
    "            agent.save_model(model_name, model_output_dir, log_to_wandb)\n",
    "            print(f\"Model saved after episode {episode + 1}\")\n",
    "            \n",
    "    # Finish wandb logging\n",
    "    if log_to_wandb:\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3de6bef-757a-49f3-bdb9-04dc2a6130db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6824fd1b-7cd1-4445-94c3-f20f6b6ee68e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### `train_5`\n",
    "\n",
    "Reverted back to original Reward Strategy as in (train_1)\n",
    "\n",
    "- Trained for 200 episodes\n",
    "- Replay after every epoch\n",
    "- Epsilon decay = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16ad5ada-035b-493b-8ea7-23a567d7243b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4e6cf8cd-45f6-408a-af41-fc49a96283d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "18f92077-d7f1-4f8b-bb68-47a8c0bbaf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_214120-cwainy14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Highest Score: 107, Episode Score: 107, Episode Reward: 97.3000, Episode Epsilon: 0.9950, Episode Loss: 2.1888, Mean Score: 107.0000, Mean Reward 97.3000\n",
      "Episode 2/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.3000, Episode Epsilon: 0.9900, Episode Loss: 16.3613, Mean Score: 80.0000, Mean Reward 49.3000\n",
      "Episode 3/200, Highest Score: 107, Episode Score: 54, Episode Reward: 2.6000, Episode Epsilon: 0.9851, Episode Loss: 26.8944, Mean Score: 71.3333, Mean Reward 33.7333\n",
      "Episode 4/200, Highest Score: 107, Episode Score: 54, Episode Reward: 2.5000, Episode Epsilon: 0.9801, Episode Loss: 19.7332, Mean Score: 67.0000, Mean Reward 25.9250\n",
      "Episode 5/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.9752, Episode Loss: 46.6583, Mean Score: 64.0000, Mean Reward 20.9600\n",
      "Episode 6/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.9704, Episode Loss: 46.9283, Mean Score: 62.0000, Mean Reward 17.8667\n",
      "Episode 7/200, Highest Score: 107, Episode Score: 74, Episode Reward: 14.4000, Episode Epsilon: 0.9655, Episode Loss: 2.0431, Mean Score: 63.7143, Mean Reward 17.3714\n",
      "Episode 8/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.0000, Episode Epsilon: 0.9607, Episode Loss: 0.2355, Mean Score: 62.1250, Mean Reward 15.4500\n",
      "Episode 9/200, Highest Score: 107, Episode Score: 65, Episode Reward: 6.5000, Episode Epsilon: 0.9559, Episode Loss: 39.5528, Mean Score: 62.4444, Mean Reward 14.4556\n",
      "Episode 10/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.9511, Episode Loss: 11.6301, Mean Score: 61.4000, Mean Reward 13.0800\n",
      "Model saved after episode 10\n",
      "Episode 11/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.4000, Episode Epsilon: 0.9464, Episode Loss: 2.8436, Mean Score: 60.5455, Mean Reward 11.9273\n",
      "Episode 12/200, Highest Score: 107, Episode Score: 61, Episode Reward: 9.1000, Episode Epsilon: 0.9416, Episode Loss: 0.3191, Mean Score: 60.5833, Mean Reward 11.6917\n",
      "Episode 13/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.9369, Episode Loss: 3.7230, Mean Score: 59.9231, Mean Reward 10.9154\n",
      "Episode 14/200, Highest Score: 107, Episode Score: 62, Episode Reward: 8.2000, Episode Epsilon: 0.9322, Episode Loss: 0.1091, Mean Score: 60.0714, Mean Reward 10.7214\n",
      "Episode 15/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.9276, Episode Loss: 1.0112, Mean Score: 59.5333, Mean Reward 10.0600\n",
      "Episode 16/200, Highest Score: 107, Episode Score: 55, Episode Reward: 5.4000, Episode Epsilon: 0.9229, Episode Loss: 0.0656, Mean Score: 59.2500, Mean Reward 9.7687\n",
      "Episode 17/200, Highest Score: 107, Episode Score: 62, Episode Reward: 2.1000, Episode Epsilon: 0.9183, Episode Loss: 0.0057, Mean Score: 59.4118, Mean Reward 9.3176\n",
      "Episode 18/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.8000, Episode Epsilon: 0.9137, Episode Loss: 0.2931, Mean Score: 59.0556, Mean Reward 8.9000\n",
      "Episode 19/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.9092, Episode Loss: 0.0705, Mean Score: 58.6842, Mean Reward 8.5526\n",
      "Episode 20/200, Highest Score: 107, Episode Score: 55, Episode Reward: 8.2000, Episode Epsilon: 0.9046, Episode Loss: 0.7471, Mean Score: 58.5000, Mean Reward 8.5350\n",
      "Model saved after episode 20\n",
      "Episode 21/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.8000, Episode Epsilon: 0.9001, Episode Loss: 1.9995, Mean Score: 58.2381, Mean Reward 8.2619\n",
      "Episode 22/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.8956, Episode Loss: 8.4445, Mean Score: 57.9545, Mean Reward 7.9955\n",
      "Episode 23/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.8911, Episode Loss: 0.5921, Mean Score: 57.6522, Mean Reward 7.7696\n",
      "Episode 24/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.8867, Episode Loss: 2.8903, Mean Score: 57.4167, Mean Reward 7.5250\n",
      "Episode 25/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.8822, Episode Loss: 0.6163, Mean Score: 57.2000, Mean Reward 7.3000\n",
      "Episode 26/200, Highest Score: 107, Episode Score: 71, Episode Reward: 11.4000, Episode Epsilon: 0.8778, Episode Loss: 0.2983, Mean Score: 57.7308, Mean Reward 7.4577\n",
      "Episode 27/200, Highest Score: 107, Episode Score: 53, Episode Reward: 7.9000, Episode Epsilon: 0.8734, Episode Loss: 13.5564, Mean Score: 57.5556, Mean Reward 7.4741\n",
      "Episode 28/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.8691, Episode Loss: 6.6913, Mean Score: 57.3571, Mean Reward 7.3214\n",
      "Episode 29/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.3000, Episode Epsilon: 0.8647, Episode Loss: 16.3935, Mean Score: 57.1724, Mean Reward 7.0793\n",
      "Episode 30/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.3000, Episode Epsilon: 0.8604, Episode Loss: 106.1849, Mean Score: 57.0000, Mean Reward 6.8867\n",
      "Model saved after episode 30\n",
      "Episode 31/200, Highest Score: 107, Episode Score: 64, Episode Reward: 15.1000, Episode Epsilon: 0.8561, Episode Loss: 0.0719, Mean Score: 57.2258, Mean Reward 7.1516\n",
      "Episode 32/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.8518, Episode Loss: 6.6869, Mean Score: 57.0625, Mean Reward 6.9500\n",
      "Episode 33/200, Highest Score: 107, Episode Score: 54, Episode Reward: 4.6000, Episode Epsilon: 0.8475, Episode Loss: 54.3699, Mean Score: 56.9697, Mean Reward 6.8788\n",
      "Episode 34/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.8433, Episode Loss: 10151.1592, Mean Score: 56.8235, Mean Reward 6.6941\n",
      "Episode 35/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.8391, Episode Loss: 48.6221, Mean Score: 56.6857, Mean Reward 6.5286\n",
      "Episode 36/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.2000, Episode Epsilon: 0.8349, Episode Loss: 12.2669, Mean Score: 56.5278, Mean Reward 6.3806\n",
      "Episode 37/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.8307, Episode Loss: 1.3879, Mean Score: 56.4054, Mean Reward 6.2243\n",
      "Episode 38/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.8266, Episode Loss: 190.8391, Mean Score: 56.2895, Mean Reward 6.0816\n",
      "Episode 39/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.8224, Episode Loss: 0.0000, Mean Score: 56.1795, Mean Reward 5.9513\n",
      "Episode 40/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.8183, Episode Loss: 3.3555, Mean Score: 56.0750, Mean Reward 5.8325\n",
      "Model saved after episode 40\n",
      "Episode 41/200, Highest Score: 107, Episode Score: 71, Episode Reward: 4.6000, Episode Epsilon: 0.8142, Episode Loss: 0.0797, Mean Score: 56.4390, Mean Reward 5.8024\n",
      "Episode 42/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.6000, Episode Epsilon: 0.8102, Episode Loss: 19.2836, Mean Score: 56.3095, Mean Reward 5.7024\n",
      "Episode 43/200, Highest Score: 107, Episode Score: 65, Episode Reward: 3.4000, Episode Epsilon: 0.8061, Episode Loss: 0.5337, Mean Score: 56.5116, Mean Reward 5.6488\n",
      "Episode 44/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.8021, Episode Loss: 36.2273, Mean Score: 56.4091, Mean Reward 5.5614\n",
      "Episode 45/200, Highest Score: 107, Episode Score: 73, Episode Reward: 5.9000, Episode Epsilon: 0.7981, Episode Loss: 1.3809, Mean Score: 56.7778, Mean Reward 5.5689\n",
      "Episode 46/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7941, Episode Loss: 1.4089, Mean Score: 56.6739, Mean Reward 5.4674\n",
      "Episode 47/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.7901, Episode Loss: 4.5656, Mean Score: 56.5745, Mean Reward 5.3851\n",
      "Episode 48/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.7862, Episode Loss: 1.9580, Mean Score: 56.4792, Mean Reward 5.3042\n",
      "Episode 49/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.7822, Episode Loss: 48.9907, Mean Score: 56.3878, Mean Reward 5.2265\n",
      "Episode 50/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.7783, Episode Loss: 0.9510, Mean Score: 56.3000, Mean Reward 5.1620\n",
      "Model saved after episode 50\n",
      "Episode 51/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7744, Episode Loss: 0.5005, Mean Score: 56.2157, Mean Reward 5.0843\n",
      "Episode 52/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.7705, Episode Loss: 0.3931, Mean Score: 56.1346, Mean Reward 5.0250\n",
      "Episode 53/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7667, Episode Loss: 0.4724, Mean Score: 56.0566, Mean Reward 4.9453\n",
      "Episode 54/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7629, Episode Loss: 2.6204, Mean Score: 55.9815, Mean Reward 4.8722\n",
      "Episode 55/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7590, Episode Loss: 153.3139, Mean Score: 55.9091, Mean Reward 4.8000\n",
      "Episode 56/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.7553, Episode Loss: 1.7403, Mean Score: 55.8393, Mean Reward 4.7339\n",
      "Episode 57/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.9000, Episode Epsilon: 0.7515, Episode Loss: 2.1220, Mean Score: 55.7544, Mean Reward 4.6667\n",
      "Episode 58/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7477, Episode Loss: 28.7264, Mean Score: 55.6897, Mean Reward 4.6034\n",
      "Episode 59/200, Highest Score: 107, Episode Score: 52, Episode Reward: -0.3000, Episode Epsilon: 0.7440, Episode Loss: 0.9971, Mean Score: 55.6271, Mean Reward 4.5203\n",
      "Episode 60/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7403, Episode Loss: 0.6906, Mean Score: 55.5667, Mean Reward 4.4650\n",
      "Model saved after episode 60\n",
      "Episode 61/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7366, Episode Loss: 0.0223, Mean Score: 55.5082, Mean Reward 4.4082\n",
      "Episode 62/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7329, Episode Loss: 0.1377, Mean Score: 55.4516, Mean Reward 4.3516\n",
      "Episode 63/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.5000, Episode Epsilon: 0.7292, Episode Loss: 2.1757, Mean Score: 55.3968, Mean Reward 4.2905\n",
      "Episode 64/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7256, Episode Loss: 1.8199, Mean Score: 55.3438, Mean Reward 4.2359\n",
      "Episode 65/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.4000, Episode Epsilon: 0.7219, Episode Loss: 10.8715, Mean Score: 55.2923, Mean Reward 4.1769\n",
      "Episode 66/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.7183, Episode Loss: 3.8364, Mean Score: 55.2424, Mean Reward 4.1303\n",
      "Episode 67/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7147, Episode Loss: 1.2343, Mean Score: 55.1940, Mean Reward 4.0836\n",
      "Episode 68/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.1000, Episode Epsilon: 0.7112, Episode Loss: 107.9281, Mean Score: 55.1471, Mean Reward 4.0250\n",
      "Episode 69/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7076, Episode Loss: 14.4213, Mean Score: 55.1014, Mean Reward 3.9841\n",
      "Episode 70/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.7000, Episode Epsilon: 0.7041, Episode Loss: 65.9295, Mean Score: 55.0429, Mean Reward 3.9371\n",
      "Model saved after episode 70\n",
      "Episode 71/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7005, Episode Loss: 49.1321, Mean Score: 55.0000, Mean Reward 3.8930\n",
      "Episode 72/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.6970, Episode Loss: 33.8774, Mean Score: 54.9583, Mean Reward 3.8514\n",
      "Episode 73/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.6936, Episode Loss: 24.5962, Mean Score: 54.9178, Mean Reward 3.8068\n",
      "Episode 74/200, Highest Score: 107, Episode Score: 54, Episode Reward: 1.5000, Episode Epsilon: 0.6901, Episode Loss: 8.5556, Mean Score: 54.9054, Mean Reward 3.7757\n",
      "Episode 75/200, Highest Score: 107, Episode Score: 54, Episode Reward: 1.4000, Episode Epsilon: 0.6866, Episode Loss: 22.9704, Mean Score: 54.8933, Mean Reward 3.7440\n",
      "Episode 76/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.2000, Episode Epsilon: 0.6832, Episode Loss: 1.0469, Mean Score: 54.8553, Mean Reward 3.6974\n",
      "Episode 77/200, Highest Score: 107, Episode Score: 58, Episode Reward: 5.4000, Episode Epsilon: 0.6798, Episode Loss: 0.0365, Mean Score: 54.8961, Mean Reward 3.7195\n",
      "Episode 78/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.6764, Episode Loss: 0.0982, Mean Score: 54.8590, Mean Reward 3.7115\n",
      "Episode 79/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.2000, Episode Epsilon: 0.6730, Episode Loss: 4.5786, Mean Score: 54.8228, Mean Reward 3.6671\n",
      "Episode 80/200, Highest Score: 107, Episode Score: 73, Episode Reward: 9.8000, Episode Epsilon: 0.6696, Episode Loss: 0.5954, Mean Score: 55.0500, Mean Reward 3.7437\n",
      "Model saved after episode 80\n",
      "Episode 81/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.6663, Episode Loss: 0.2715, Mean Score: 55.0000, Mean Reward 3.7321\n",
      "Episode 82/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.6630, Episode Loss: 0.6947, Mean Score: 54.9634, Mean Reward 3.7159\n",
      "Episode 83/200, Highest Score: 107, Episode Score: 57, Episode Reward: 4.0000, Episode Epsilon: 0.6597, Episode Loss: 0.0337, Mean Score: 54.9880, Mean Reward 3.7193\n",
      "Episode 84/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.6564, Episode Loss: 0.8716, Mean Score: 54.9524, Mean Reward 3.6833\n",
      "Episode 85/200, Highest Score: 107, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.6531, Episode Loss: 0.0562, Mean Score: 54.9059, Mean Reward 3.7000\n",
      "Episode 86/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.6498, Episode Loss: 0.8472, Mean Score: 54.8721, Mean Reward 3.6686\n",
      "Episode 87/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.6466, Episode Loss: 3.2860, Mean Score: 54.8391, Mean Reward 3.6356\n",
      "Episode 88/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.6433, Episode Loss: 0.3698, Mean Score: 54.8068, Mean Reward 3.6068\n",
      "Episode 89/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.3000, Episode Epsilon: 0.6401, Episode Loss: 0.0724, Mean Score: 54.7865, Mean Reward 3.5809\n",
      "Episode 90/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.6369, Episode Loss: 0.0005, Mean Score: 54.7556, Mean Reward 3.5489\n",
      "Model saved after episode 90\n",
      "Episode 91/200, Highest Score: 107, Episode Score: 80, Episode Reward: 11.2000, Episode Epsilon: 0.6337, Episode Loss: 0.4506, Mean Score: 55.0330, Mean Reward 3.6330\n",
      "Episode 92/200, Highest Score: 107, Episode Score: 59, Episode Reward: 2.2000, Episode Epsilon: 0.6306, Episode Loss: 40.8070, Mean Score: 55.0761, Mean Reward 3.6174\n",
      "Episode 93/200, Highest Score: 107, Episode Score: 64, Episode Reward: 5.2000, Episode Epsilon: 0.6274, Episode Loss: 0.7932, Mean Score: 55.1720, Mean Reward 3.6344\n",
      "Episode 94/200, Highest Score: 107, Episode Score: 59, Episode Reward: 4.6000, Episode Epsilon: 0.6243, Episode Loss: 3.4156, Mean Score: 55.2128, Mean Reward 3.6447\n",
      "Episode 95/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.6211, Episode Loss: 3.1900, Mean Score: 55.1789, Mean Reward 3.6189\n",
      "Episode 96/200, Highest Score: 107, Episode Score: 87, Episode Reward: 8.4000, Episode Epsilon: 0.6180, Episode Loss: 0.0785, Mean Score: 55.5104, Mean Reward 3.6687\n",
      "Episode 97/200, Highest Score: 107, Episode Score: 64, Episode Reward: 7.2000, Episode Epsilon: 0.6149, Episode Loss: 1.0329, Mean Score: 55.5979, Mean Reward 3.7052\n",
      "Episode 98/200, Highest Score: 107, Episode Score: 68, Episode Reward: 6.7000, Episode Epsilon: 0.6119, Episode Loss: 0.3804, Mean Score: 55.7245, Mean Reward 3.7357\n",
      "Episode 99/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.9000, Episode Epsilon: 0.6088, Episode Loss: 0.0000, Mean Score: 55.6768, Mean Reward 3.7172\n",
      "Episode 100/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.6058, Episode Loss: 3.5528, Mean Score: 55.6400, Mean Reward 3.6960\n",
      "Model saved after episode 100\n",
      "Episode 101/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.5000, Episode Epsilon: 0.6027, Episode Loss: 3.0433, Mean Score: 55.6139, Mean Reward 3.6941\n",
      "Episode 102/200, Highest Score: 107, Episode Score: 72, Episode Reward: 7.4000, Episode Epsilon: 0.5997, Episode Loss: 0.1833, Mean Score: 55.7745, Mean Reward 3.7304\n",
      "Episode 103/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5967, Episode Loss: 7.4107, Mean Score: 55.7379, Mean Reward 3.7039\n",
      "Episode 104/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5937, Episode Loss: 0.0599, Mean Score: 55.7019, Mean Reward 3.6779\n",
      "Episode 105/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.0000, Episode Epsilon: 0.5908, Episode Loss: 0.0004, Mean Score: 55.6762, Mean Reward 3.6619\n",
      "Episode 106/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.5000, Episode Epsilon: 0.5878, Episode Loss: 0.2960, Mean Score: 55.6415, Mean Reward 3.6321\n",
      "Episode 107/200, Highest Score: 107, Episode Score: 62, Episode Reward: 6.8000, Episode Epsilon: 0.5849, Episode Loss: 2.5164, Mean Score: 55.7009, Mean Reward 3.6617\n",
      "Episode 108/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.7000, Episode Epsilon: 0.5820, Episode Loss: 8.9337, Mean Score: 55.6667, Mean Reward 3.6435\n",
      "Episode 109/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.3000, Episode Epsilon: 0.5790, Episode Loss: 17.4534, Mean Score: 55.6239, Mean Reward 3.6220\n",
      "Episode 110/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.4000, Episode Epsilon: 0.5762, Episode Loss: 11.2138, Mean Score: 55.5818, Mean Reward 3.6018\n",
      "Model saved after episode 110\n",
      "Episode 111/200, Highest Score: 107, Episode Score: 75, Episode Reward: 5.0000, Episode Epsilon: 0.5733, Episode Loss: 0.1608, Mean Score: 55.7568, Mean Reward 3.6144\n",
      "Episode 112/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.3000, Episode Epsilon: 0.5704, Episode Loss: 0.2990, Mean Score: 55.7321, Mean Reward 3.6027\n",
      "Episode 113/200, Highest Score: 107, Episode Score: 64, Episode Reward: 7.1000, Episode Epsilon: 0.5676, Episode Loss: 1.0937, Mean Score: 55.8053, Mean Reward 3.6336\n",
      "Episode 114/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.7000, Episode Epsilon: 0.5647, Episode Loss: 0.3800, Mean Score: 55.7807, Mean Reward 3.6167\n",
      "Episode 115/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.5619, Episode Loss: 1.3537, Mean Score: 55.7478, Mean Reward 3.6052\n",
      "Episode 116/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.5591, Episode Loss: 0.3103, Mean Score: 55.7155, Mean Reward 3.5957\n",
      "Episode 117/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.5563, Episode Loss: 0.4667, Mean Score: 55.6838, Mean Reward 3.5701\n",
      "Episode 118/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.5535, Episode Loss: 3.3259, Mean Score: 55.6525, Mean Reward 3.5551\n",
      "Episode 119/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.5507, Episode Loss: 8.9104, Mean Score: 55.6218, Mean Reward 3.5345\n",
      "Episode 120/200, Highest Score: 107, Episode Score: 55, Episode Reward: 2.6000, Episode Epsilon: 0.5480, Episode Loss: 1.7029, Mean Score: 55.6167, Mean Reward 3.5267\n",
      "Model saved after episode 120\n",
      "Episode 121/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.5452, Episode Loss: 8.9010, Mean Score: 55.5868, Mean Reward 3.5033\n",
      "Episode 122/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.6000, Episode Epsilon: 0.5425, Episode Loss: 0.1592, Mean Score: 55.5492, Mean Reward 3.4795\n",
      "Episode 123/200, Highest Score: 107, Episode Score: 69, Episode Reward: 16.5000, Episode Epsilon: 0.5398, Episode Loss: 1.2421, Mean Score: 55.6585, Mean Reward 3.5854\n",
      "Episode 124/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5371, Episode Loss: 3.6546, Mean Score: 55.6290, Mean Reward 3.5645\n",
      "Episode 125/200, Highest Score: 107, Episode Score: 53, Episode Reward: 0.7000, Episode Epsilon: 0.5344, Episode Loss: 0.6453, Mean Score: 55.6080, Mean Reward 3.5416\n",
      "Episode 126/200, Highest Score: 107, Episode Score: 65, Episode Reward: 6.9000, Episode Epsilon: 0.5318, Episode Loss: 0.0496, Mean Score: 55.6825, Mean Reward 3.5683\n",
      "Episode 127/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.5291, Episode Loss: 0.7174, Mean Score: 55.6535, Mean Reward 3.5457\n",
      "Episode 128/200, Highest Score: 107, Episode Score: 62, Episode Reward: 8.8000, Episode Epsilon: 0.5264, Episode Loss: 0.0052, Mean Score: 55.7031, Mean Reward 3.5867\n",
      "Episode 129/200, Highest Score: 107, Episode Score: 58, Episode Reward: 4.7000, Episode Epsilon: 0.5238, Episode Loss: 0.6841, Mean Score: 55.7209, Mean Reward 3.5953\n",
      "Episode 130/200, Highest Score: 107, Episode Score: 51, Episode Reward: -0.1000, Episode Epsilon: 0.5212, Episode Loss: 0.0960, Mean Score: 55.6846, Mean Reward 3.5669\n",
      "Model saved after episode 130\n",
      "Episode 131/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.5186, Episode Loss: 0.2749, Mean Score: 55.6565, Mean Reward 3.5534\n",
      "Episode 132/200, Highest Score: 107, Episode Score: 56, Episode Reward: 7.2000, Episode Epsilon: 0.5160, Episode Loss: 0.0727, Mean Score: 55.6591, Mean Reward 3.5811\n",
      "Episode 133/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.5134, Episode Loss: 0.0260, Mean Score: 55.6316, Mean Reward 3.5752\n",
      "Episode 134/200, Highest Score: 107, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.5108, Episode Loss: 0.5561, Mean Score: 55.6045, Mean Reward 3.5813\n",
      "Episode 135/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.5083, Episode Loss: 4.7097, Mean Score: 55.5778, Mean Reward 3.5719\n",
      "Episode 136/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.5058, Episode Loss: 4.4444, Mean Score: 55.5515, Mean Reward 3.5669\n",
      "Episode 137/200, Highest Score: 107, Episode Score: 55, Episode Reward: 7.9000, Episode Epsilon: 0.5032, Episode Loss: 0.9956, Mean Score: 55.5474, Mean Reward 3.5985\n",
      "Episode 138/200, Highest Score: 107, Episode Score: 66, Episode Reward: 18.2000, Episode Epsilon: 0.5007, Episode Loss: 31.1139, Mean Score: 55.6232, Mean Reward 3.7043\n",
      "Episode 139/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.4982, Episode Loss: 1.7099, Mean Score: 55.5971, Mean Reward 3.7014\n",
      "Episode 140/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.3000, Episode Epsilon: 0.4957, Episode Loss: 1.6931, Mean Score: 55.5643, Mean Reward 3.6986\n",
      "Model saved after episode 140\n",
      "Episode 141/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.4932, Episode Loss: 0.0301, Mean Score: 55.5390, Mean Reward 3.6943\n",
      "Episode 142/200, Highest Score: 107, Episode Score: 59, Episode Reward: 9.6000, Episode Epsilon: 0.4908, Episode Loss: 0.0639, Mean Score: 55.5634, Mean Reward 3.7359\n",
      "Episode 143/200, Highest Score: 107, Episode Score: 56, Episode Reward: 7.7000, Episode Epsilon: 0.4883, Episode Loss: 0.0147, Mean Score: 55.5664, Mean Reward 3.7636\n",
      "Episode 144/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.4859, Episode Loss: 31.5657, Mean Score: 55.5417, Mean Reward 3.7604\n",
      "Episode 145/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.4000, Episode Epsilon: 0.4834, Episode Loss: 3.4315, Mean Score: 55.5241, Mean Reward 3.7579\n",
      "Episode 146/200, Highest Score: 107, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.4810, Episode Loss: 281.8986, Mean Score: 55.5000, Mean Reward 3.7603\n",
      "Episode 147/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4786, Episode Loss: 9.0104, Mean Score: 55.4694, Mean Reward 3.7639\n",
      "Episode 148/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.4762, Episode Loss: 1.4367, Mean Score: 55.4392, Mean Reward 3.7682\n",
      "Episode 149/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4738, Episode Loss: 0.5401, Mean Score: 55.4094, Mean Reward 3.7698\n",
      "Episode 150/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4715, Episode Loss: 1.2505, Mean Score: 55.3800, Mean Reward 3.7713\n",
      "Model saved after episode 150\n",
      "Episode 151/200, Highest Score: 107, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.4691, Episode Loss: 0.6032, Mean Score: 55.3510, Mean Reward 3.7795\n",
      "Episode 152/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4668, Episode Loss: 0.1157, Mean Score: 55.3289, Mean Reward 3.7803\n",
      "Episode 153/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.3000, Episode Epsilon: 0.4644, Episode Loss: 0.0260, Mean Score: 55.3137, Mean Reward 3.7771\n",
      "Episode 154/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.4621, Episode Loss: 1.3185, Mean Score: 55.2922, Mean Reward 3.7727\n",
      "Episode 155/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4598, Episode Loss: 0.0984, Mean Score: 55.2710, Mean Reward 3.7671\n",
      "Episode 156/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.3000, Episode Epsilon: 0.4575, Episode Loss: 0.0113, Mean Score: 55.2564, Mean Reward 3.7577\n",
      "Episode 157/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.9000, Episode Epsilon: 0.4552, Episode Loss: 0.3035, Mean Score: 55.2293, Mean Reward 3.7522\n",
      "Episode 158/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4529, Episode Loss: 1.5886, Mean Score: 55.2089, Mean Reward 3.7487\n",
      "Episode 159/200, Highest Score: 107, Episode Score: 81, Episode Reward: 15.3000, Episode Epsilon: 0.4507, Episode Loss: 0.5645, Mean Score: 55.3711, Mean Reward 3.8214\n",
      "Episode 160/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.4484, Episode Loss: 0.0000, Mean Score: 55.3500, Mean Reward 3.8119\n",
      "Model saved after episode 160\n",
      "Episode 161/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.5000, Episode Epsilon: 0.4462, Episode Loss: 0.3420, Mean Score: 55.3354, Mean Reward 3.8037\n",
      "Episode 162/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.4000, Episode Epsilon: 0.4440, Episode Loss: 0.1420, Mean Score: 55.3086, Mean Reward 3.8012\n",
      "Episode 163/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4417, Episode Loss: 0.0057, Mean Score: 55.2883, Mean Reward 3.7975\n",
      "Episode 164/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4395, Episode Loss: 0.1781, Mean Score: 55.2622, Mean Reward 3.7988\n",
      "Episode 165/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.4373, Episode Loss: 0.0252, Mean Score: 55.2364, Mean Reward 3.7982\n",
      "Episode 166/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4351, Episode Loss: 0.4278, Mean Score: 55.2169, Mean Reward 3.7928\n",
      "Episode 167/200, Highest Score: 107, Episode Score: 52, Episode Reward: 6.1000, Episode Epsilon: 0.4330, Episode Loss: 1.0723, Mean Score: 55.1976, Mean Reward 3.8066\n",
      "Episode 168/200, Highest Score: 107, Episode Score: 54, Episode Reward: 3.7000, Episode Epsilon: 0.4308, Episode Loss: 6.9759, Mean Score: 55.1905, Mean Reward 3.8060\n",
      "Episode 169/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4286, Episode Loss: 5.2515, Mean Score: 55.1716, Mean Reward 3.8024\n",
      "Episode 170/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.4265, Episode Loss: 0.3292, Mean Score: 55.1529, Mean Reward 3.7918\n",
      "Model saved after episode 170\n",
      "Episode 171/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.4244, Episode Loss: 0.4206, Mean Score: 55.1345, Mean Reward 3.7836\n",
      "Episode 172/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.4223, Episode Loss: 1.5237, Mean Score: 55.1163, Mean Reward 3.7756\n",
      "Episode 173/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4201, Episode Loss: 1.0142, Mean Score: 55.0983, Mean Reward 3.7665\n",
      "Episode 174/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.4180, Episode Loss: 0.2204, Mean Score: 55.0805, Mean Reward 3.7552\n",
      "Episode 175/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.4159, Episode Loss: 13.9334, Mean Score: 55.0629, Mean Reward 3.7497\n",
      "Episode 176/200, Highest Score: 107, Episode Score: 70, Episode Reward: 6.7000, Episode Epsilon: 0.4139, Episode Loss: 0.5093, Mean Score: 55.1477, Mean Reward 3.7665\n",
      "Episode 177/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4118, Episode Loss: 0.0824, Mean Score: 55.1299, Mean Reward 3.7616\n",
      "Episode 178/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4097, Episode Loss: 29.8914, Mean Score: 55.1124, Mean Reward 3.7528\n",
      "Episode 179/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.4077, Episode Loss: 74.7894, Mean Score: 55.0950, Mean Reward 3.7464\n",
      "Episode 180/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.9000, Episode Epsilon: 0.4057, Episode Loss: 1.7137, Mean Score: 55.0722, Mean Reward 3.7417\n",
      "Model saved after episode 180\n",
      "Episode 181/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4036, Episode Loss: 64.7228, Mean Score: 55.0552, Mean Reward 3.7331\n",
      "Episode 182/200, Highest Score: 107, Episode Score: 66, Episode Reward: 5.1000, Episode Epsilon: 0.4016, Episode Loss: 1.3572, Mean Score: 55.1154, Mean Reward 3.7407\n",
      "Episode 183/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.0000, Episode Epsilon: 0.3996, Episode Loss: 13.2291, Mean Score: 55.0929, Mean Reward 3.7366\n",
      "Episode 184/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.3976, Episode Loss: 301.9651, Mean Score: 55.0761, Mean Reward 3.7266\n",
      "Episode 185/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.3956, Episode Loss: 2.0377, Mean Score: 55.0595, Mean Reward 3.7184\n",
      "Episode 186/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3936, Episode Loss: 15.9504, Mean Score: 55.0430, Mean Reward 3.7151\n",
      "Episode 187/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.6000, Episode Epsilon: 0.3917, Episode Loss: 2.0032, Mean Score: 55.0214, Mean Reward 3.7091\n",
      "Episode 188/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.3897, Episode Loss: 32.3000, Mean Score: 55.0053, Mean Reward 3.6989\n",
      "Episode 189/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.0000, Episode Epsilon: 0.3878, Episode Loss: 1.7364, Mean Score: 54.9841, Mean Reward 3.6899\n",
      "Episode 190/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.3858, Episode Loss: 171.5635, Mean Score: 54.9684, Mean Reward 3.6837\n",
      "Model saved after episode 190\n",
      "Episode 191/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3839, Episode Loss: 3.4848, Mean Score: 54.9529, Mean Reward 3.6806\n",
      "Episode 192/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.3820, Episode Loss: 0.4712, Mean Score: 54.9323, Mean Reward 3.6760\n",
      "Episode 193/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.3801, Episode Loss: 21.8032, Mean Score: 54.9171, Mean Reward 3.6663\n",
      "Episode 194/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.3782, Episode Loss: 4.0838, Mean Score: 54.9021, Mean Reward 3.6588\n",
      "Episode 195/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3763, Episode Loss: 0.2070, Mean Score: 54.8872, Mean Reward 3.6508\n",
      "Episode 196/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3744, Episode Loss: 2512.4690, Mean Score: 54.8724, Mean Reward 3.6429\n",
      "Episode 197/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3725, Episode Loss: 1.1048, Mean Score: 54.8579, Mean Reward 3.6350\n",
      "Episode 198/200, Highest Score: 107, Episode Score: 71, Episode Reward: 6.8000, Episode Epsilon: 0.3707, Episode Loss: 0.5458, Mean Score: 54.9394, Mean Reward 3.6510\n",
      "Episode 199/200, Highest Score: 107, Episode Score: 63, Episode Reward: 4.0000, Episode Epsilon: 0.3688, Episode Loss: 0.2220, Mean Score: 54.9799, Mean Reward 3.6528\n",
      "Episode 200/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.3670, Episode Loss: 1.2379, Mean Score: 54.9650, Mean Reward 3.6465\n",
      "Model saved after episode 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td></td></tr><tr><td>episode_epsilon</td><td></td></tr><tr><td>episode_loss</td><td></td></tr><tr><td>episode_reward</td><td></td></tr><tr><td>episode_score</td><td></td></tr><tr><td>highest_score</td><td></td></tr><tr><td>mean_current_score</td><td></td></tr><tr><td>mean_loss</td><td></td></tr><tr><td>mean_reward</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0.36696</td></tr><tr><td>episode_loss</td><td>1.23786</td></tr><tr><td>episode_reward</td><td>2.4</td></tr><tr><td>episode_score</td><td>52</td></tr><tr><td>highest_score</td><td>107</td></tr><tr><td>mean_current_score</td><td>54.965</td></tr><tr><td>mean_loss</td><td>1.23786</td></tr><tr><td>mean_reward</td><td>3.6465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14</a><br/>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_214120-cwainy14\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3e9e986-e49e-4259-942d-2e462aa02fbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### `train_6`\n",
    "Original Reward Strategy as in (train_1)\n",
    "\n",
    "- Trained for 400 episodes\n",
    "- Replay after every epoch\n",
    "- Epsilon decay = 0.995 with exploration breaks - set epsilon to 0 every 10 intervals to test the models knowledge while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df9d7892-0bef-4d66-8131-b53ac8d85dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230508_190645-9u2gsi4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_dqn_agent/runs/9u2gsi4t' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_dqn_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_dqn_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_dqn_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_dqn_agent/runs/9u2gsi4t' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_dqn_agent/runs/9u2gsi4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/400, Highest Score: 51, Episode Score: 51, Episode Reward: 125.5000, Episode Epsilon: 0.0000, Episode Loss: 0.0015, Mean Score: 51.0000, Mean Reward 125.5000\n",
      "Episode 2/400, Highest Score: 51, Episode Score: 51, Episode Reward: 3.2000, Episode Epsilon: 0.9950, Episode Loss: 145.9340, Mean Score: 51.0000, Mean Reward 64.3500\n",
      "Episode 3/400, Highest Score: 52, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.9900, Episode Loss: 0.8471, Mean Score: 51.3333, Mean Reward 44.3000\n",
      "Episode 4/400, Highest Score: 58, Episode Score: 58, Episode Reward: 21.1000, Episode Epsilon: 0.9851, Episode Loss: 0.0053, Mean Score: 53.0000, Mean Reward 38.5000\n",
      "Episode 5/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.9801, Episode Loss: 25535.4844, Mean Score: 52.8000, Mean Reward 31.5400\n",
      "Episode 6/400, Highest Score: 58, Episode Score: 55, Episode Reward: 8.8000, Episode Epsilon: 0.9752, Episode Loss: 52.1755, Mean Score: 53.1667, Mean Reward 27.7500\n",
      "Episode 7/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.9704, Episode Loss: 6.0395, Mean Score: 53.0000, Mean Reward 24.2571\n",
      "Episode 8/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.9655, Episode Loss: 1.4368, Mean Score: 52.8750, Mean Reward 21.6875\n",
      "Episode 9/400, Highest Score: 58, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.9607, Episode Loss: 4.0029, Mean Score: 52.7778, Mean Reward 19.6000\n",
      "Episode 10/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.9559, Episode Loss: 69.7204, Mean Score: 52.7000, Mean Reward 17.9900\n",
      "Model saved after episode 10\n",
      "Episode 11/400, Highest Score: 58, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.0000, Episode Loss: 1.3827, Mean Score: 52.5455, Mean Reward 16.7364\n",
      "Episode 12/400, Highest Score: 58, Episode Score: 52, Episode Reward: 6.1000, Episode Epsilon: 0.9511, Episode Loss: 3.9260, Mean Score: 52.5000, Mean Reward 15.8500\n",
      "Episode 13/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.9464, Episode Loss: 1.8053, Mean Score: 52.4615, Mean Reward 14.9000\n",
      "Episode 14/400, Highest Score: 58, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.9416, Episode Loss: 0.0928, Mean Score: 52.4286, Mean Reward 14.1143\n",
      "Episode 15/400, Highest Score: 58, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.9369, Episode Loss: 1.7404, Mean Score: 52.3333, Mean Reward 13.4333\n",
      "Episode 16/400, Highest Score: 58, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.9322, Episode Loss: 2.0117, Mean Score: 52.2500, Mean Reward 12.8812\n",
      "Episode 17/400, Highest Score: 58, Episode Score: 54, Episode Reward: 4.1000, Episode Epsilon: 0.9276, Episode Loss: 3.7744, Mean Score: 52.3529, Mean Reward 12.3647\n",
      "Episode 18/400, Highest Score: 61, Episode Score: 61, Episode Reward: 13.4000, Episode Epsilon: 0.9229, Episode Loss: 3.1015, Mean Score: 52.8333, Mean Reward 12.4222\n",
      "Episode 19/400, Highest Score: 61, Episode Score: 51, Episode Reward: 1.8000, Episode Epsilon: 0.9183, Episode Loss: 2.2703, Mean Score: 52.7368, Mean Reward 11.8632\n",
      "Episode 20/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.9137, Episode Loss: 4.8070, Mean Score: 52.7000, Mean Reward 11.4650\n",
      "Model saved after episode 20\n",
      "Episode 21/400, Highest Score: 61, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.0000, Episode Loss: 6.7380, Mean Score: 52.6190, Mean Reward 11.1238\n",
      "Episode 22/400, Highest Score: 61, Episode Score: 60, Episode Reward: 5.4000, Episode Epsilon: 0.9092, Episode Loss: 4.3811, Mean Score: 52.9545, Mean Reward 10.8636\n",
      "Episode 23/400, Highest Score: 61, Episode Score: 61, Episode Reward: 11.0000, Episode Epsilon: 0.9046, Episode Loss: 1.6609, Mean Score: 53.3043, Mean Reward 10.8696\n",
      "Episode 24/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.9001, Episode Loss: 1.2353, Mean Score: 53.2500, Mean Reward 10.5667\n",
      "Episode 25/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.8956, Episode Loss: 2.7881, Mean Score: 53.2000, Mean Reward 10.2760\n",
      "Episode 26/400, Highest Score: 61, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.8911, Episode Loss: 1.9841, Mean Score: 53.1538, Mean Reward 9.9846\n",
      "Episode 27/400, Highest Score: 61, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.8867, Episode Loss: 0.7273, Mean Score: 53.0741, Mean Reward 9.7741\n",
      "Episode 28/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.8822, Episode Loss: 50.6901, Mean Score: 53.0357, Mean Reward 9.5571\n",
      "Episode 29/400, Highest Score: 61, Episode Score: 53, Episode Reward: 3.7000, Episode Epsilon: 0.8778, Episode Loss: 1.8117, Mean Score: 53.0345, Mean Reward 9.3552\n",
      "Episode 30/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.8734, Episode Loss: 0.6575, Mean Score: 53.0000, Mean Reward 9.1600\n",
      "Model saved after episode 30\n",
      "Episode 31/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.0000, Episode Loss: 1.4341, Mean Score: 52.9677, Mean Reward 8.9903\n",
      "Episode 32/400, Highest Score: 61, Episode Score: 53, Episode Reward: 2.6000, Episode Epsilon: 0.8691, Episode Loss: 0.0080, Mean Score: 52.9688, Mean Reward 8.7906\n",
      "Episode 33/400, Highest Score: 61, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.8647, Episode Loss: 365.8468, Mean Score: 52.9091, Mean Reward 8.6455\n",
      "Episode 34/400, Highest Score: 61, Episode Score: 56, Episode Reward: 7.9000, Episode Epsilon: 0.8604, Episode Loss: 0.7528, Mean Score: 53.0000, Mean Reward 8.6235\n",
      "Episode 35/400, Highest Score: 61, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.8561, Episode Loss: 15.1169, Mean Score: 52.9714, Mean Reward 8.4857\n",
      "Episode 36/400, Highest Score: 61, Episode Score: 52, Episode Reward: 5.9000, Episode Epsilon: 0.8518, Episode Loss: 0.4652, Mean Score: 52.9444, Mean Reward 8.4139\n",
      "Episode 37/400, Highest Score: 61, Episode Score: 54, Episode Reward: 4.9000, Episode Epsilon: 0.8475, Episode Loss: 0.0148, Mean Score: 52.9730, Mean Reward 8.3189\n",
      "Episode 38/400, Highest Score: 66, Episode Score: 66, Episode Reward: 17.8000, Episode Epsilon: 0.8433, Episode Loss: 0.1095, Mean Score: 53.3158, Mean Reward 8.5684\n",
      "Episode 39/400, Highest Score: 66, Episode Score: 53, Episode Reward: 3.8000, Episode Epsilon: 0.8391, Episode Loss: 2.7439, Mean Score: 53.3077, Mean Reward 8.4462\n",
      "Episode 40/400, Highest Score: 66, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.8349, Episode Loss: 6.2430, Mean Score: 53.2750, Mean Reward 8.3275\n",
      "Model saved after episode 40\n",
      "Episode 41/400, Highest Score: 66, Episode Score: 53, Episode Reward: 4.8000, Episode Epsilon: 0.0000, Episode Loss: 200.5891, Mean Score: 53.2683, Mean Reward 8.2415\n",
      "Episode 42/400, Highest Score: 66, Episode Score: 53, Episode Reward: 3.2000, Episode Epsilon: 0.8307, Episode Loss: 0.6988, Mean Score: 53.2619, Mean Reward 8.1214\n",
      "Episode 43/400, Highest Score: 71, Episode Score: 71, Episode Reward: 20.0000, Episode Epsilon: 0.8266, Episode Loss: 0.1227, Mean Score: 53.6744, Mean Reward 8.3977\n",
      "Episode 44/400, Highest Score: 71, Episode Score: 53, Episode Reward: 5.5000, Episode Epsilon: 0.8224, Episode Loss: 0.1772, Mean Score: 53.6591, Mean Reward 8.3318\n",
      "Episode 45/400, Highest Score: 71, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.8183, Episode Loss: 2.2049, Mean Score: 53.6222, Mean Reward 8.1978\n",
      "Episode 46/400, Highest Score: 71, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.8142, Episode Loss: 123.8253, Mean Score: 53.5870, Mean Reward 8.0630\n",
      "Episode 47/400, Highest Score: 71, Episode Score: 51, Episode Reward: 3.6000, Episode Epsilon: 0.8102, Episode Loss: 0.7178, Mean Score: 53.5319, Mean Reward 7.9681\n",
      "Episode 48/400, Highest Score: 71, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.8061, Episode Loss: 0.2658, Mean Score: 53.5000, Mean Reward 7.8812\n",
      "Episode 49/400, Highest Score: 71, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.8021, Episode Loss: 0.4584, Mean Score: 53.4694, Mean Reward 7.8020\n",
      "Episode 50/400, Highest Score: 71, Episode Score: 53, Episode Reward: 2.9000, Episode Epsilon: 0.7981, Episode Loss: 5.3895, Mean Score: 53.4600, Mean Reward 7.7040\n",
      "Model saved after episode 50\n",
      "Episode 51/400, Highest Score: 72, Episode Score: 72, Episode Reward: 8.6000, Episode Epsilon: 0.0000, Episode Loss: 0.5596, Mean Score: 53.8235, Mean Reward 7.7216\n",
      "Episode 52/400, Highest Score: 72, Episode Score: 55, Episode Reward: 7.6000, Episode Epsilon: 0.7941, Episode Loss: 0.2773, Mean Score: 53.8462, Mean Reward 7.7192\n",
      "Episode 53/400, Highest Score: 72, Episode Score: 54, Episode Reward: 2.8000, Episode Epsilon: 0.7901, Episode Loss: 0.2616, Mean Score: 53.8491, Mean Reward 7.6264\n",
      "Episode 54/400, Highest Score: 72, Episode Score: 56, Episode Reward: 6.6000, Episode Epsilon: 0.7862, Episode Loss: 0.1655, Mean Score: 53.8889, Mean Reward 7.6074\n",
      "Episode 55/400, Highest Score: 72, Episode Score: 56, Episode Reward: 6.3000, Episode Epsilon: 0.7822, Episode Loss: 0.1189, Mean Score: 53.9273, Mean Reward 7.5836\n",
      "Episode 56/400, Highest Score: 72, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.7783, Episode Loss: 2.6645, Mean Score: 53.9107, Mean Reward 7.5196\n",
      "Episode 57/400, Highest Score: 72, Episode Score: 64, Episode Reward: 7.4000, Episode Epsilon: 0.7744, Episode Loss: 2.6819, Mean Score: 54.0877, Mean Reward 7.5175\n",
      "Episode 58/400, Highest Score: 72, Episode Score: 53, Episode Reward: 2.8000, Episode Epsilon: 0.7705, Episode Loss: 0.1658, Mean Score: 54.0690, Mean Reward 7.4362\n",
      "Episode 59/400, Highest Score: 72, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.7667, Episode Loss: 0.0652, Mean Score: 54.0339, Mean Reward 7.3559\n",
      "Episode 60/400, Highest Score: 72, Episode Score: 54, Episode Reward: 2.0000, Episode Epsilon: 0.7629, Episode Loss: 0.1220, Mean Score: 54.0333, Mean Reward 7.2667\n",
      "Model saved after episode 60\n",
      "Episode 61/400, Highest Score: 72, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.0000, Episode Loss: 0.1346, Mean Score: 54.0000, Mean Reward 7.1951\n",
      "Episode 62/400, Highest Score: 72, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.7590, Episode Loss: 0.1849, Mean Score: 53.9677, Mean Reward 7.1258\n",
      "Episode 63/400, Highest Score: 72, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.7553, Episode Loss: 0.0012, Mean Score: 53.9206, Mean Reward 7.0730\n",
      "Episode 64/400, Highest Score: 72, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.7515, Episode Loss: 0.0363, Mean Score: 53.8750, Mean Reward 7.0234\n",
      "Episode 65/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.7477, Episode Loss: 1.1833, Mean Score: 53.8462, Mean Reward 6.9877\n",
      "Episode 66/400, Highest Score: 72, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.7440, Episode Loss: 0.0254, Mean Score: 53.8182, Mean Reward 6.9394\n",
      "Episode 67/400, Highest Score: 72, Episode Score: 71, Episode Reward: 15.3000, Episode Epsilon: 0.7403, Episode Loss: 0.0704, Mean Score: 54.0746, Mean Reward 7.0642\n",
      "Episode 68/400, Highest Score: 72, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.7366, Episode Loss: 1.8563, Mean Score: 54.0294, Mean Reward 7.0279\n",
      "Episode 69/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.7329, Episode Loss: 0.0000, Mean Score: 54.0000, Mean Reward 6.9899\n",
      "Episode 70/400, Highest Score: 72, Episode Score: 52, Episode Reward: 3.0000, Episode Epsilon: 0.7292, Episode Loss: 0.0031, Mean Score: 53.9714, Mean Reward 6.9329\n",
      "Model saved after episode 70\n",
      "Episode 71/400, Highest Score: 72, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.0000, Episode Loss: 0.0781, Mean Score: 53.9437, Mean Reward 6.8873\n",
      "Episode 72/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.7256, Episode Loss: 0.0827, Mean Score: 53.9167, Mean Reward 6.8500\n",
      "Episode 73/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.7219, Episode Loss: 0.1936, Mean Score: 53.8904, Mean Reward 6.8164\n",
      "Episode 74/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.7183, Episode Loss: 0.0069, Mean Score: 53.8649, Mean Reward 6.7838\n",
      "Episode 75/400, Highest Score: 72, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.7147, Episode Loss: 0.0002, Mean Score: 53.8533, Mean Reward 6.7480\n",
      "Episode 76/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.7112, Episode Loss: 0.0765, Mean Score: 53.8289, Mean Reward 6.7184\n",
      "Episode 77/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.7076, Episode Loss: 0.0129, Mean Score: 53.8052, Mean Reward 6.6831\n",
      "Episode 78/400, Highest Score: 72, Episode Score: 63, Episode Reward: 8.2000, Episode Epsilon: 0.7041, Episode Loss: 0.0025, Mean Score: 53.9231, Mean Reward 6.7026\n",
      "Episode 79/400, Highest Score: 72, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.7005, Episode Loss: 0.0130, Mean Score: 53.8987, Mean Reward 6.6646\n",
      "Episode 80/400, Highest Score: 72, Episode Score: 52, Episode Reward: 5.9000, Episode Epsilon: 0.6970, Episode Loss: 0.0062, Mean Score: 53.8750, Mean Reward 6.6550\n",
      "Model saved after episode 80\n",
      "Episode 81/400, Highest Score: 72, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.0000, Episode Loss: 0.0864, Mean Score: 53.8519, Mean Reward 6.6272\n",
      "Episode 82/400, Highest Score: 72, Episode Score: 71, Episode Reward: 40.0000, Episode Epsilon: 0.6936, Episode Loss: 0.4555, Mean Score: 54.0610, Mean Reward 7.0341\n",
      "Episode 83/400, Highest Score: 84, Episode Score: 84, Episode Reward: 57.3000, Episode Epsilon: 0.6901, Episode Loss: 0.2004, Mean Score: 54.4217, Mean Reward 7.6398\n",
      "Episode 84/400, Highest Score: 84, Episode Score: 54, Episode Reward: 4.7000, Episode Epsilon: 0.6866, Episode Loss: 0.0405, Mean Score: 54.4167, Mean Reward 7.6048\n",
      "Episode 85/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.6832, Episode Loss: 0.0113, Mean Score: 54.3882, Mean Reward 7.5529\n",
      "Episode 86/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.6798, Episode Loss: 0.0243, Mean Score: 54.3721, Mean Reward 7.5128\n",
      "Episode 87/400, Highest Score: 84, Episode Score: 51, Episode Reward: 8.5000, Episode Epsilon: 0.6764, Episode Loss: 0.0011, Mean Score: 54.3333, Mean Reward 7.5241\n",
      "Episode 88/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.6730, Episode Loss: 69.7873, Mean Score: 54.3068, Mean Reward 7.4920\n",
      "Episode 89/400, Highest Score: 84, Episode Score: 63, Episode Reward: 7.5000, Episode Epsilon: 0.6696, Episode Loss: 0.1830, Mean Score: 54.4045, Mean Reward 7.4921\n",
      "Episode 90/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.6000, Episode Epsilon: 0.6663, Episode Loss: 0.5020, Mean Score: 54.3889, Mean Reward 7.4600\n",
      "Model saved after episode 90\n",
      "Episode 91/400, Highest Score: 84, Episode Score: 81, Episode Reward: 12.7000, Episode Epsilon: 0.0000, Episode Loss: 12.1399, Mean Score: 54.6813, Mean Reward 7.5176\n",
      "Episode 92/400, Highest Score: 84, Episode Score: 51, Episode Reward: 3.6000, Episode Epsilon: 0.6630, Episode Loss: 91.9847, Mean Score: 54.6413, Mean Reward 7.4750\n",
      "Episode 93/400, Highest Score: 84, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.6597, Episode Loss: 107.7028, Mean Score: 54.6022, Mean Reward 7.4409\n",
      "Episode 94/400, Highest Score: 84, Episode Score: 52, Episode Reward: 5.4000, Episode Epsilon: 0.6564, Episode Loss: 368.6475, Mean Score: 54.5745, Mean Reward 7.4191\n",
      "Episode 95/400, Highest Score: 84, Episode Score: 51, Episode Reward: 6.1000, Episode Epsilon: 0.6531, Episode Loss: 665.8787, Mean Score: 54.5368, Mean Reward 7.4053\n",
      "Episode 96/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.6498, Episode Loss: 186.9853, Mean Score: 54.5208, Mean Reward 7.3750\n",
      "Episode 97/400, Highest Score: 84, Episode Score: 64, Episode Reward: 7.9000, Episode Epsilon: 0.6466, Episode Loss: 4074.5964, Mean Score: 54.6186, Mean Reward 7.3804\n",
      "Episode 98/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.6433, Episode Loss: 3684.6785, Mean Score: 54.5918, Mean Reward 7.3429\n",
      "Episode 99/400, Highest Score: 84, Episode Score: 55, Episode Reward: 11.3000, Episode Epsilon: 0.6401, Episode Loss: 2215.5239, Mean Score: 54.5960, Mean Reward 7.3828\n",
      "Episode 100/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.6369, Episode Loss: 125.5986, Mean Score: 54.5700, Mean Reward 7.3540\n",
      "Model saved after episode 100\n",
      "Episode 101/400, Highest Score: 84, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.0000, Episode Loss: 31.2871, Mean Score: 54.5347, Mean Reward 7.3218\n",
      "Episode 102/400, Highest Score: 84, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.6337, Episode Loss: 51.9893, Mean Score: 54.5098, Mean Reward 7.2784\n",
      "Episode 103/400, Highest Score: 84, Episode Score: 52, Episode Reward: 5.6000, Episode Epsilon: 0.6306, Episode Loss: 0.0721, Mean Score: 54.4854, Mean Reward 7.2621\n",
      "Episode 104/400, Highest Score: 84, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.6274, Episode Loss: 44.6982, Mean Score: 54.4519, Mean Reward 7.2337\n",
      "Episode 105/400, Highest Score: 84, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.6243, Episode Loss: 56.3095, Mean Score: 54.4190, Mean Reward 7.2010\n",
      "Episode 106/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.6211, Episode Loss: 1.8969, Mean Score: 54.3962, Mean Reward 7.1679\n",
      "Episode 107/400, Highest Score: 84, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.6180, Episode Loss: 67.0164, Mean Score: 54.3645, Mean Reward 7.1458\n",
      "Episode 108/400, Highest Score: 84, Episode Score: 55, Episode Reward: 7.9000, Episode Epsilon: 0.6149, Episode Loss: 9.0642, Mean Score: 54.3704, Mean Reward 7.1528\n",
      "Episode 109/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.6119, Episode Loss: 25.5203, Mean Score: 54.3578, Mean Reward 7.1248\n",
      "Episode 110/400, Highest Score: 84, Episode Score: 65, Episode Reward: 7.8000, Episode Epsilon: 0.6088, Episode Loss: 1.8695, Mean Score: 54.4545, Mean Reward 7.1309\n",
      "Model saved after episode 110\n",
      "Episode 111/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.0000, Episode Loss: 2.6010, Mean Score: 54.4324, Mean Reward 7.1054\n",
      "Episode 112/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.6058, Episode Loss: 6.3355, Mean Score: 54.4107, Mean Reward 7.0741\n",
      "Episode 113/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.6027, Episode Loss: 253.6267, Mean Score: 54.3894, Mean Reward 7.0451\n",
      "Episode 114/400, Highest Score: 84, Episode Score: 52, Episode Reward: 18.9000, Episode Epsilon: 0.5997, Episode Loss: 4.0200, Mean Score: 54.3684, Mean Reward 7.1491\n",
      "Episode 115/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.5967, Episode Loss: 32.5344, Mean Score: 54.3478, Mean Reward 7.1252\n",
      "Episode 116/400, Highest Score: 84, Episode Score: 53, Episode Reward: 3.9000, Episode Epsilon: 0.5937, Episode Loss: 1.1093, Mean Score: 54.3362, Mean Reward 7.0974\n",
      "Episode 117/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.5908, Episode Loss: 2.4565, Mean Score: 54.3162, Mean Reward 7.0632\n",
      "Episode 118/400, Highest Score: 84, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.5878, Episode Loss: 28.7468, Mean Score: 54.2881, Mean Reward 7.0381\n",
      "Episode 119/400, Highest Score: 84, Episode Score: 53, Episode Reward: 3.7000, Episode Epsilon: 0.5849, Episode Loss: 4.1261, Mean Score: 54.2773, Mean Reward 7.0101\n",
      "Episode 120/400, Highest Score: 84, Episode Score: 52, Episode Reward: 1.3000, Episode Epsilon: 0.5820, Episode Loss: 11.3540, Mean Score: 54.2583, Mean Reward 6.9625\n",
      "Model saved after episode 120\n",
      "Episode 121/400, Highest Score: 84, Episode Score: 63, Episode Reward: 6.5000, Episode Epsilon: 0.0000, Episode Loss: 7.9704, Mean Score: 54.3306, Mean Reward 6.9587\n",
      "Episode 122/400, Highest Score: 84, Episode Score: 81, Episode Reward: 20.9000, Episode Epsilon: 0.5790, Episode Loss: 0.0691, Mean Score: 54.5492, Mean Reward 7.0730\n",
      "Episode 123/400, Highest Score: 84, Episode Score: 53, Episode Reward: 0.8000, Episode Epsilon: 0.5762, Episode Loss: 10.8495, Mean Score: 54.5366, Mean Reward 7.0220\n",
      "Episode 124/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.5733, Episode Loss: 17.9691, Mean Score: 54.5161, Mean Reward 6.9903\n",
      "Episode 125/400, Highest Score: 84, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.5704, Episode Loss: 2.8990, Mean Score: 54.4960, Mean Reward 6.9560\n",
      "Episode 126/400, Highest Score: 84, Episode Score: 53, Episode Reward: 1.9000, Episode Epsilon: 0.5676, Episode Loss: 10.4043, Mean Score: 54.4841, Mean Reward 6.9159\n",
      "Episode 127/400, Highest Score: 84, Episode Score: 53, Episode Reward: 1.1000, Episode Epsilon: 0.5647, Episode Loss: 36.2784, Mean Score: 54.4724, Mean Reward 6.8701\n",
      "Episode 128/400, Highest Score: 84, Episode Score: 67, Episode Reward: 5.2000, Episode Epsilon: 0.5619, Episode Loss: 3.7550, Mean Score: 54.5703, Mean Reward 6.8570\n",
      "Episode 129/400, Highest Score: 84, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.5591, Episode Loss: 1.7246, Mean Score: 54.5504, Mean Reward 6.8248\n",
      "Episode 130/400, Highest Score: 84, Episode Score: 70, Episode Reward: 17.5000, Episode Epsilon: 0.5563, Episode Loss: 2.8218, Mean Score: 54.6692, Mean Reward 6.9069\n",
      "Model saved after episode 130\n",
      "Episode 131/400, Highest Score: 84, Episode Score: 60, Episode Reward: 7.1000, Episode Epsilon: 0.0000, Episode Loss: 16.8360, Mean Score: 54.7099, Mean Reward 6.9084\n",
      "Episode 132/400, Highest Score: 84, Episode Score: 51, Episode Reward: 18.4000, Episode Epsilon: 0.5535, Episode Loss: 1.7074, Mean Score: 54.6818, Mean Reward 6.9955\n",
      "Episode 133/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.5507, Episode Loss: 5.5111, Mean Score: 54.6617, Mean Reward 6.9707\n",
      "Episode 134/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.5480, Episode Loss: 1.0973, Mean Score: 54.6418, Mean Reward 6.9470\n",
      "Episode 135/400, Highest Score: 84, Episode Score: 67, Episode Reward: 8.7000, Episode Epsilon: 0.5452, Episode Loss: 0.0723, Mean Score: 54.7333, Mean Reward 6.9600\n",
      "Episode 136/400, Highest Score: 84, Episode Score: 52, Episode Reward: 8.1000, Episode Epsilon: 0.5425, Episode Loss: 1.7381, Mean Score: 54.7132, Mean Reward 6.9684\n",
      "Episode 137/400, Highest Score: 84, Episode Score: 52, Episode Reward: 6.5000, Episode Epsilon: 0.5398, Episode Loss: 1.7162, Mean Score: 54.6934, Mean Reward 6.9650\n",
      "Episode 138/400, Highest Score: 84, Episode Score: 51, Episode Reward: 8.9000, Episode Epsilon: 0.5371, Episode Loss: 8.9780, Mean Score: 54.6667, Mean Reward 6.9790\n",
      "Episode 139/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.5344, Episode Loss: 4.6467, Mean Score: 54.6475, Mean Reward 6.9547\n",
      "Episode 140/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.5318, Episode Loss: 34.9458, Mean Score: 54.6286, Mean Reward 6.9300\n",
      "Model saved after episode 140\n",
      "Episode 141/400, Highest Score: 84, Episode Score: 51, Episode Reward: 5.2000, Episode Epsilon: 0.0000, Episode Loss: 0.8432, Mean Score: 54.6028, Mean Reward 6.9177\n",
      "Episode 142/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.6000, Episode Epsilon: 0.5291, Episode Loss: 25.4903, Mean Score: 54.5915, Mean Reward 6.9014\n",
      "Episode 143/400, Highest Score: 84, Episode Score: 53, Episode Reward: 7.7000, Episode Epsilon: 0.5264, Episode Loss: 0.3881, Mean Score: 54.5804, Mean Reward 6.9070\n",
      "Episode 144/400, Highest Score: 84, Episode Score: 54, Episode Reward: 8.7000, Episode Epsilon: 0.5238, Episode Loss: 4.8903, Mean Score: 54.5764, Mean Reward 6.9194\n",
      "Episode 145/400, Highest Score: 84, Episode Score: 60, Episode Reward: 7.0000, Episode Epsilon: 0.5212, Episode Loss: 0.6982, Mean Score: 54.6138, Mean Reward 6.9200\n",
      "Episode 146/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.5186, Episode Loss: 0.1282, Mean Score: 54.5959, Mean Reward 6.8993\n",
      "Episode 147/400, Highest Score: 84, Episode Score: 68, Episode Reward: 9.1000, Episode Epsilon: 0.5160, Episode Loss: 456.8526, Mean Score: 54.6871, Mean Reward 6.9143\n",
      "Episode 148/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.5134, Episode Loss: 10.1284, Mean Score: 54.6689, Mean Reward 6.8932\n",
      "Episode 149/400, Highest Score: 84, Episode Score: 51, Episode Reward: 6.4000, Episode Epsilon: 0.5108, Episode Loss: 1.1823, Mean Score: 54.6443, Mean Reward 6.8899\n",
      "Episode 150/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.3000, Episode Epsilon: 0.5083, Episode Loss: 1.2593, Mean Score: 54.6333, Mean Reward 6.8727\n",
      "Model saved after episode 150\n",
      "Episode 151/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.0000, Episode Loss: 8.9877, Mean Score: 54.6159, Mean Reward 6.8523\n",
      "Episode 152/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.5058, Episode Loss: 0.8788, Mean Score: 54.5987, Mean Reward 6.8349\n",
      "Episode 153/400, Highest Score: 84, Episode Score: 72, Episode Reward: 15.6000, Episode Epsilon: 0.5032, Episode Loss: 2.4408, Mean Score: 54.7124, Mean Reward 6.8922\n",
      "Episode 154/400, Highest Score: 84, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.5007, Episode Loss: 0.0451, Mean Score: 54.7013, Mean Reward 6.8740\n",
      "Episode 155/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4982, Episode Loss: 1.0364, Mean Score: 54.6839, Mean Reward 6.8555\n",
      "Episode 156/400, Highest Score: 84, Episode Score: 61, Episode Reward: 6.4000, Episode Epsilon: 0.4957, Episode Loss: 10.7891, Mean Score: 54.7244, Mean Reward 6.8526\n",
      "Episode 157/400, Highest Score: 84, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4932, Episode Loss: 0.0769, Mean Score: 54.7070, Mean Reward 6.8338\n",
      "Episode 158/400, Highest Score: 84, Episode Score: 70, Episode Reward: 13.1000, Episode Epsilon: 0.4908, Episode Loss: 0.0270, Mean Score: 54.8038, Mean Reward 6.8734\n",
      "Episode 159/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4883, Episode Loss: 0.7970, Mean Score: 54.7862, Mean Reward 6.8553\n",
      "Episode 160/400, Highest Score: 84, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.4859, Episode Loss: 10.6959, Mean Score: 54.7687, Mean Reward 6.8419\n",
      "Model saved after episode 160\n",
      "Episode 161/400, Highest Score: 91, Episode Score: 91, Episode Reward: 31.4000, Episode Epsilon: 0.0000, Episode Loss: 5.0609, Mean Score: 54.9938, Mean Reward 6.9944\n",
      "Episode 162/400, Highest Score: 91, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.4834, Episode Loss: 0.4358, Mean Score: 54.9753, Mean Reward 6.9716\n",
      "Episode 163/400, Highest Score: 91, Episode Score: 50, Episode Reward: 4.1000, Episode Epsilon: 0.4810, Episode Loss: 0.4009, Mean Score: 54.9448, Mean Reward 6.9540\n",
      "Episode 164/400, Highest Score: 91, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.4786, Episode Loss: 0.1166, Mean Score: 54.9268, Mean Reward 6.9396\n",
      "Episode 165/400, Highest Score: 91, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.4762, Episode Loss: 0.0240, Mean Score: 54.9091, Mean Reward 6.9133\n",
      "Episode 166/400, Highest Score: 91, Episode Score: 56, Episode Reward: 9.5000, Episode Epsilon: 0.4738, Episode Loss: 0.7542, Mean Score: 54.9157, Mean Reward 6.9289\n",
      "Episode 167/400, Highest Score: 91, Episode Score: 51, Episode Reward: 2.6000, Episode Epsilon: 0.4715, Episode Loss: 4.9748, Mean Score: 54.8922, Mean Reward 6.9030\n",
      "Episode 168/400, Highest Score: 91, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.4691, Episode Loss: 0.0200, Mean Score: 54.8750, Mean Reward 6.8863\n",
      "Episode 169/400, Highest Score: 91, Episode Score: 55, Episode Reward: 6.2000, Episode Epsilon: 0.4668, Episode Loss: 1.5983, Mean Score: 54.8757, Mean Reward 6.8822\n",
      "Episode 170/400, Highest Score: 91, Episode Score: 52, Episode Reward: 3.0000, Episode Epsilon: 0.4644, Episode Loss: 0.0457, Mean Score: 54.8588, Mean Reward 6.8594\n",
      "Model saved after episode 170\n",
      "Episode 171/400, Highest Score: 91, Episode Score: 58, Episode Reward: 8.0000, Episode Epsilon: 0.0000, Episode Loss: 0.6902, Mean Score: 54.8772, Mean Reward 6.8661\n",
      "Episode 172/400, Highest Score: 91, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.4621, Episode Loss: 0.3141, Mean Score: 54.8605, Mean Reward 6.8413\n",
      "Episode 173/400, Highest Score: 91, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4598, Episode Loss: 1.6974, Mean Score: 54.8439, Mean Reward 6.8249\n",
      "Episode 174/400, Highest Score: 91, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4575, Episode Loss: 1.3943, Mean Score: 54.8276, Mean Reward 6.8086\n",
      "Episode 175/400, Highest Score: 91, Episode Score: 51, Episode Reward: 6.2000, Episode Epsilon: 0.4552, Episode Loss: 0.1927, Mean Score: 54.8057, Mean Reward 6.8051\n",
      "Episode 176/400, Highest Score: 91, Episode Score: 80, Episode Reward: 11.8000, Episode Epsilon: 0.4529, Episode Loss: 0.4117, Mean Score: 54.9489, Mean Reward 6.8335\n",
      "Episode 177/400, Highest Score: 91, Episode Score: 53, Episode Reward: 4.6000, Episode Epsilon: 0.4507, Episode Loss: 0.3441, Mean Score: 54.9379, Mean Reward 6.8209\n",
      "Episode 178/400, Highest Score: 91, Episode Score: 69, Episode Reward: 8.2000, Episode Epsilon: 0.4484, Episode Loss: 2.1933, Mean Score: 55.0169, Mean Reward 6.8287\n",
      "Episode 179/400, Highest Score: 91, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.4462, Episode Loss: 8.0473, Mean Score: 55.0000, Mean Reward 6.8151\n",
      "Episode 180/400, Highest Score: 91, Episode Score: 53, Episode Reward: 5.3000, Episode Epsilon: 0.4440, Episode Loss: 0.5516, Mean Score: 54.9889, Mean Reward 6.8067\n",
      "Model saved after episode 180\n",
      "Episode 181/400, Highest Score: 142, Episode Score: 142, Episode Reward: 107.4000, Episode Epsilon: 0.0000, Episode Loss: 0.0242, Mean Score: 55.4696, Mean Reward 7.3624\n",
      "Episode 182/400, Highest Score: 142, Episode Score: 52, Episode Reward: 9.7000, Episode Epsilon: 0.4417, Episode Loss: 2.4493, Mean Score: 55.4505, Mean Reward 7.3753\n",
      "Episode 183/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.4395, Episode Loss: 2.3646, Mean Score: 55.4317, Mean Reward 7.3536\n",
      "Episode 184/400, Highest Score: 142, Episode Score: 53, Episode Reward: 5.1000, Episode Epsilon: 0.4373, Episode Loss: 5.8507, Mean Score: 55.4185, Mean Reward 7.3413\n",
      "Episode 185/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.4351, Episode Loss: 0.2106, Mean Score: 55.4054, Mean Reward 7.3259\n",
      "Episode 186/400, Highest Score: 142, Episode Score: 52, Episode Reward: 8.6000, Episode Epsilon: 0.4330, Episode Loss: 1.1186, Mean Score: 55.3871, Mean Reward 7.3328\n",
      "Episode 187/400, Highest Score: 142, Episode Score: 54, Episode Reward: 5.3000, Episode Epsilon: 0.4308, Episode Loss: 0.0958, Mean Score: 55.3797, Mean Reward 7.3219\n",
      "Episode 188/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4286, Episode Loss: 1.4891, Mean Score: 55.3617, Mean Reward 7.3037\n",
      "Episode 189/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.4265, Episode Loss: 0.2214, Mean Score: 55.3439, Mean Reward 7.2873\n",
      "Episode 190/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.4244, Episode Loss: 2.0618, Mean Score: 55.3263, Mean Reward 7.2726\n",
      "Model saved after episode 190\n",
      "Episode 191/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.0000, Episode Loss: 4.2015, Mean Score: 55.3089, Mean Reward 7.2581\n",
      "Episode 192/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.4223, Episode Loss: 0.0035, Mean Score: 55.2969, Mean Reward 7.2437\n",
      "Episode 193/400, Highest Score: 142, Episode Score: 50, Episode Reward: 4.1000, Episode Epsilon: 0.4201, Episode Loss: 51.1840, Mean Score: 55.2694, Mean Reward 7.2275\n",
      "Episode 194/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4180, Episode Loss: 0.0576, Mean Score: 55.2526, Mean Reward 7.2103\n",
      "Episode 195/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.4159, Episode Loss: 4.4109, Mean Score: 55.2359, Mean Reward 7.1969\n",
      "Episode 196/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.4139, Episode Loss: 18.0899, Mean Score: 55.2143, Mean Reward 7.1862\n",
      "Episode 197/400, Highest Score: 142, Episode Score: 69, Episode Reward: 11.8000, Episode Epsilon: 0.4118, Episode Loss: 55.3576, Mean Score: 55.2843, Mean Reward 7.2096\n",
      "Episode 198/400, Highest Score: 142, Episode Score: 55, Episode Reward: 5.1000, Episode Epsilon: 0.4097, Episode Loss: 8.5102, Mean Score: 55.2828, Mean Reward 7.1990\n",
      "Episode 199/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.4077, Episode Loss: 0.0706, Mean Score: 55.2663, Mean Reward 7.1764\n",
      "Episode 200/400, Highest Score: 142, Episode Score: 65, Episode Reward: 8.3000, Episode Epsilon: 0.4057, Episode Loss: 5.1136, Mean Score: 55.3150, Mean Reward 7.1820\n",
      "Model saved after episode 200\n",
      "Episode 201/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.0000, Episode Loss: 7.8146, Mean Score: 55.2985, Mean Reward 7.1677\n",
      "Episode 202/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.7000, Episode Epsilon: 0.4036, Episode Loss: 445.4257, Mean Score: 55.2871, Mean Reward 7.1554\n",
      "Episode 203/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.4016, Episode Loss: 2.4465, Mean Score: 55.2709, Mean Reward 7.1443\n",
      "Episode 204/400, Highest Score: 142, Episode Score: 53, Episode Reward: 5.0000, Episode Epsilon: 0.3996, Episode Loss: 4.8189, Mean Score: 55.2598, Mean Reward 7.1338\n",
      "Episode 205/400, Highest Score: 142, Episode Score: 52, Episode Reward: 5.1000, Episode Epsilon: 0.3976, Episode Loss: 4.2092, Mean Score: 55.2439, Mean Reward 7.1239\n",
      "Episode 206/400, Highest Score: 142, Episode Score: 52, Episode Reward: 5.1000, Episode Epsilon: 0.3956, Episode Loss: 1.8593, Mean Score: 55.2282, Mean Reward 7.1141\n",
      "Episode 207/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.3936, Episode Loss: 4.2634, Mean Score: 55.2126, Mean Reward 7.0995\n",
      "Episode 208/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.3917, Episode Loss: 4.5167, Mean Score: 55.1971, Mean Reward 7.0865\n",
      "Episode 209/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.3897, Episode Loss: 0.0108, Mean Score: 55.1818, Mean Reward 7.0761\n",
      "Episode 210/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3878, Episode Loss: 0.7510, Mean Score: 55.1667, Mean Reward 7.0595\n",
      "Model saved after episode 210\n",
      "Episode 211/400, Highest Score: 142, Episode Score: 62, Episode Reward: 7.3000, Episode Epsilon: 0.0000, Episode Loss: 1.9916, Mean Score: 55.1991, Mean Reward 7.0607\n",
      "Episode 212/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3858, Episode Loss: 0.0204, Mean Score: 55.1840, Mean Reward 7.0448\n",
      "Episode 213/400, Highest Score: 142, Episode Score: 58, Episode Reward: 8.5000, Episode Epsilon: 0.3839, Episode Loss: 7.0770, Mean Score: 55.1972, Mean Reward 7.0516\n",
      "Episode 214/400, Highest Score: 142, Episode Score: 71, Episode Reward: 10.0000, Episode Epsilon: 0.3820, Episode Loss: 3.2629, Mean Score: 55.2710, Mean Reward 7.0654\n",
      "Episode 215/400, Highest Score: 142, Episode Score: 54, Episode Reward: 5.0000, Episode Epsilon: 0.3801, Episode Loss: 0.1148, Mean Score: 55.2651, Mean Reward 7.0558\n",
      "Episode 216/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.3782, Episode Loss: 0.6651, Mean Score: 55.2500, Mean Reward 7.0389\n",
      "Episode 217/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.3763, Episode Loss: 0.1175, Mean Score: 55.2304, Mean Reward 7.0258\n",
      "Episode 218/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.3744, Episode Loss: 6.9568, Mean Score: 55.2110, Mean Reward 7.0147\n",
      "Episode 219/400, Highest Score: 142, Episode Score: 51, Episode Reward: 2.4000, Episode Epsilon: 0.3725, Episode Loss: 0.6190, Mean Score: 55.1918, Mean Reward 6.9936\n",
      "Episode 220/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.2000, Episode Epsilon: 0.3707, Episode Loss: 0.3084, Mean Score: 55.1818, Mean Reward 6.9809\n",
      "Model saved after episode 220\n",
      "Episode 221/400, Highest Score: 142, Episode Score: 52, Episode Reward: 5.4000, Episode Epsilon: 0.0000, Episode Loss: 0.2185, Mean Score: 55.1674, Mean Reward 6.9738\n",
      "Episode 222/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.7000, Episode Epsilon: 0.3688, Episode Loss: 4.4242, Mean Score: 55.1486, Mean Reward 6.9680\n",
      "Episode 223/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3670, Episode Loss: 3.8912, Mean Score: 55.1345, Mean Reward 6.9534\n",
      "Episode 224/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.6000, Episode Epsilon: 0.3651, Episode Loss: 1.1788, Mean Score: 55.1250, Mean Reward 6.9429\n",
      "Episode 225/400, Highest Score: 142, Episode Score: 52, Episode Reward: 8.0000, Episode Epsilon: 0.3633, Episode Loss: 3.8104, Mean Score: 55.1111, Mean Reward 6.9476\n",
      "Episode 226/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3615, Episode Loss: 0.6543, Mean Score: 55.0973, Mean Reward 6.9305\n",
      "Episode 227/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.9000, Episode Epsilon: 0.3597, Episode Loss: 0.1111, Mean Score: 55.0881, Mean Reward 6.9216\n",
      "Episode 228/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.3579, Episode Loss: 0.3270, Mean Score: 55.0746, Mean Reward 6.9066\n",
      "Episode 229/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3561, Episode Loss: 1.2262, Mean Score: 55.0611, Mean Reward 6.8921\n",
      "Episode 230/400, Highest Score: 142, Episode Score: 53, Episode Reward: 7.5000, Episode Epsilon: 0.3543, Episode Loss: 0.3141, Mean Score: 55.0522, Mean Reward 6.8948\n",
      "Model saved after episode 230\n",
      "Episode 231/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.0000, Episode Loss: 1.9576, Mean Score: 55.0390, Mean Reward 6.8788\n",
      "Episode 232/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3525, Episode Loss: 3.9961, Mean Score: 55.0259, Mean Reward 6.8625\n",
      "Episode 233/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.3508, Episode Loss: 1.3563, Mean Score: 55.0129, Mean Reward 6.8502\n",
      "Episode 234/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3490, Episode Loss: 1.8034, Mean Score: 55.0000, Mean Reward 6.8368\n",
      "Episode 235/400, Highest Score: 142, Episode Score: 63, Episode Reward: 17.4000, Episode Epsilon: 0.3473, Episode Loss: 1.1820, Mean Score: 55.0340, Mean Reward 6.8817\n",
      "Episode 236/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.3455, Episode Loss: 0.1077, Mean Score: 55.0212, Mean Reward 6.8669\n",
      "Episode 237/400, Highest Score: 142, Episode Score: 66, Episode Reward: 8.1000, Episode Epsilon: 0.3438, Episode Loss: 0.6735, Mean Score: 55.0675, Mean Reward 6.8722\n",
      "Episode 238/400, Highest Score: 142, Episode Score: 89, Episode Reward: 18.5000, Episode Epsilon: 0.3421, Episode Loss: 0.2571, Mean Score: 55.2101, Mean Reward 6.9210\n",
      "Episode 239/400, Highest Score: 142, Episode Score: 53, Episode Reward: 8.0000, Episode Epsilon: 0.3404, Episode Loss: 0.1288, Mean Score: 55.2008, Mean Reward 6.9255\n",
      "Episode 240/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.3387, Episode Loss: 0.9523, Mean Score: 55.1833, Mean Reward 6.9142\n",
      "Model saved after episode 240\n",
      "Episode 241/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.0000, Episode Loss: 0.5698, Mean Score: 55.1743, Mean Reward 6.9025\n",
      "Episode 242/400, Highest Score: 142, Episode Score: 58, Episode Reward: 12.6000, Episode Epsilon: 0.3370, Episode Loss: 3.8853, Mean Score: 55.1860, Mean Reward 6.9260\n",
      "Episode 243/400, Highest Score: 142, Episode Score: 94, Episode Reward: 31.3000, Episode Epsilon: 0.3353, Episode Loss: 0.9449, Mean Score: 55.3457, Mean Reward 7.0263\n",
      "Episode 244/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.3336, Episode Loss: 0.0261, Mean Score: 55.3320, Mean Reward 7.0090\n",
      "Episode 245/400, Highest Score: 142, Episode Score: 69, Episode Reward: 9.0000, Episode Epsilon: 0.3320, Episode Loss: 82.0558, Mean Score: 55.3878, Mean Reward 7.0171\n",
      "Episode 246/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.3303, Episode Loss: 1.4970, Mean Score: 55.3780, Mean Reward 7.0049\n",
      "Episode 247/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.3286, Episode Loss: 0.3350, Mean Score: 55.3603, Mean Reward 6.9947\n",
      "Episode 248/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3270, Episode Loss: 0.1618, Mean Score: 55.3468, Mean Reward 6.9810\n",
      "Episode 249/400, Highest Score: 142, Episode Score: 52, Episode Reward: 6.5000, Episode Epsilon: 0.3254, Episode Loss: 0.9180, Mean Score: 55.3333, Mean Reward 6.9791\n",
      "Episode 250/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3237, Episode Loss: 0.3069, Mean Score: 55.3200, Mean Reward 6.9636\n",
      "Model saved after episode 250\n",
      "Episode 251/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.0000, Episode Loss: 0.0425, Mean Score: 55.3028, Mean Reward 6.9514\n",
      "Episode 252/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.3221, Episode Loss: 23.5350, Mean Score: 55.2897, Mean Reward 6.9373\n",
      "Episode 253/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3205, Episode Loss: 8.5753, Mean Score: 55.2767, Mean Reward 6.9245\n",
      "Episode 254/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.3189, Episode Loss: 0.3172, Mean Score: 55.2598, Mean Reward 6.9165\n",
      "Episode 255/400, Highest Score: 142, Episode Score: 60, Episode Reward: 5.9000, Episode Epsilon: 0.3173, Episode Loss: 0.9865, Mean Score: 55.2784, Mean Reward 6.9125\n",
      "Episode 256/400, Highest Score: 142, Episode Score: 71, Episode Reward: 8.2000, Episode Epsilon: 0.3157, Episode Loss: 1.6341, Mean Score: 55.3398, Mean Reward 6.9176\n",
      "Episode 257/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.3141, Episode Loss: 57.5272, Mean Score: 55.3230, Mean Reward 6.9074\n",
      "Episode 258/400, Highest Score: 142, Episode Score: 64, Episode Reward: 15.7000, Episode Epsilon: 0.3126, Episode Loss: 53.9696, Mean Score: 55.3566, Mean Reward 6.9415\n",
      "Episode 259/400, Highest Score: 142, Episode Score: 72, Episode Reward: 12.1000, Episode Epsilon: 0.3110, Episode Loss: 0.5819, Mean Score: 55.4208, Mean Reward 6.9614\n",
      "Episode 260/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.8000, Episode Epsilon: 0.3095, Episode Loss: 0.0730, Mean Score: 55.4077, Mean Reward 6.9531\n",
      "Model saved after episode 260\n",
      "Episode 261/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.0000, Episode Loss: 0.5448, Mean Score: 55.3908, Mean Reward 6.9460\n",
      "Episode 262/400, Highest Score: 142, Episode Score: 85, Episode Reward: 15.2000, Episode Epsilon: 0.3079, Episode Loss: 0.9949, Mean Score: 55.5038, Mean Reward 6.9775\n",
      "Episode 263/400, Highest Score: 142, Episode Score: 53, Episode Reward: 3.7000, Episode Epsilon: 0.3064, Episode Loss: 1.2261, Mean Score: 55.4943, Mean Reward 6.9650\n",
      "Episode 264/400, Highest Score: 142, Episode Score: 54, Episode Reward: 7.1000, Episode Epsilon: 0.3048, Episode Loss: 0.2059, Mean Score: 55.4886, Mean Reward 6.9655\n",
      "Episode 265/400, Highest Score: 142, Episode Score: 54, Episode Reward: 5.4000, Episode Epsilon: 0.3033, Episode Loss: 1.8825, Mean Score: 55.4830, Mean Reward 6.9596\n",
      "Episode 266/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.3018, Episode Loss: 0.6294, Mean Score: 55.4699, Mean Reward 6.9466\n",
      "Episode 267/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.3003, Episode Loss: 0.0998, Mean Score: 55.4569, Mean Reward 6.9281\n",
      "Episode 268/400, Highest Score: 142, Episode Score: 53, Episode Reward: 0.3000, Episode Epsilon: 0.2988, Episode Loss: 0.4436, Mean Score: 55.4478, Mean Reward 6.9034\n",
      "Episode 269/400, Highest Score: 142, Episode Score: 69, Episode Reward: 5.8000, Episode Epsilon: 0.2973, Episode Loss: 0.0241, Mean Score: 55.4981, Mean Reward 6.8993\n",
      "Episode 270/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.2958, Episode Loss: 0.0001, Mean Score: 55.4852, Mean Reward 6.8837\n",
      "Model saved after episode 270\n",
      "Episode 271/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.0000, Episode Loss: 0.5865, Mean Score: 55.4723, Mean Reward 6.8672\n",
      "Episode 272/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.2943, Episode Loss: 4.1844, Mean Score: 55.4632, Mean Reward 6.8566\n",
      "Episode 273/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.2929, Episode Loss: 31.3793, Mean Score: 55.4505, Mean Reward 6.8473\n",
      "Episode 274/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.2914, Episode Loss: 0.9612, Mean Score: 55.4380, Mean Reward 6.8380\n",
      "Episode 275/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.2000, Episode Epsilon: 0.2899, Episode Loss: 0.0459, Mean Score: 55.4218, Mean Reward 6.8320\n",
      "Episode 276/400, Highest Score: 142, Episode Score: 53, Episode Reward: 8.8000, Episode Epsilon: 0.2885, Episode Loss: 0.9055, Mean Score: 55.4130, Mean Reward 6.8391\n",
      "Episode 277/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.2870, Episode Loss: 0.8650, Mean Score: 55.4007, Mean Reward 6.8321\n",
      "Episode 278/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.2856, Episode Loss: 0.2364, Mean Score: 55.3849, Mean Reward 6.8227\n",
      "Episode 279/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.2842, Episode Loss: 0.0676, Mean Score: 55.3692, Mean Reward 6.8118\n",
      "Episode 280/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.2828, Episode Loss: 1.9724, Mean Score: 55.3536, Mean Reward 6.8054\n",
      "Model saved after episode 280\n",
      "Episode 281/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.6000, Episode Epsilon: 0.0000, Episode Loss: 4.8464, Mean Score: 55.3452, Mean Reward 6.7975\n",
      "Episode 282/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.2813, Episode Loss: 1.7425, Mean Score: 55.3298, Mean Reward 6.7876\n",
      "Episode 283/400, Highest Score: 142, Episode Score: 54, Episode Reward: 4.5000, Episode Epsilon: 0.2799, Episode Loss: 0.0100, Mean Score: 55.3251, Mean Reward 6.7795\n",
      "Episode 284/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.2785, Episode Loss: 0.0231, Mean Score: 55.3134, Mean Reward 6.7694\n",
      "Episode 285/400, Highest Score: 142, Episode Score: 58, Episode Reward: 6.5000, Episode Epsilon: 0.2771, Episode Loss: 0.0052, Mean Score: 55.3228, Mean Reward 6.7684\n",
      "Episode 286/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.2758, Episode Loss: 0.4754, Mean Score: 55.3112, Mean Reward 6.7594\n",
      "Episode 287/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.2744, Episode Loss: 0.0023, Mean Score: 55.2962, Mean Reward 6.7523\n",
      "Episode 288/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2730, Episode Loss: 3.2444, Mean Score: 55.2847, Mean Reward 6.7417\n",
      "Episode 289/400, Highest Score: 142, Episode Score: 56, Episode Reward: 12.3000, Episode Epsilon: 0.2716, Episode Loss: 2.8259, Mean Score: 55.2872, Mean Reward 6.7609\n",
      "Episode 290/400, Highest Score: 142, Episode Score: 53, Episode Reward: 5.0000, Episode Epsilon: 0.2703, Episode Loss: 0.7818, Mean Score: 55.2793, Mean Reward 6.7548\n",
      "Model saved after episode 290\n",
      "Episode 291/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.0000, Episode Loss: 1.0502, Mean Score: 55.2646, Mean Reward 6.7460\n",
      "Episode 292/400, Highest Score: 142, Episode Score: 53, Episode Reward: 3.2000, Episode Epsilon: 0.2689, Episode Loss: 1.1288, Mean Score: 55.2568, Mean Reward 6.7339\n",
      "Episode 293/400, Highest Score: 142, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.2676, Episode Loss: 0.4569, Mean Score: 55.2423, Mean Reward 6.7205\n",
      "Episode 294/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.2663, Episode Loss: 0.1805, Mean Score: 55.2313, Mean Reward 6.7061\n",
      "Episode 295/400, Highest Score: 142, Episode Score: 63, Episode Reward: 4.1000, Episode Epsilon: 0.2649, Episode Loss: 0.3533, Mean Score: 55.2576, Mean Reward 6.6973\n",
      "Episode 296/400, Highest Score: 142, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.2636, Episode Loss: 10.0354, Mean Score: 55.2466, Mean Reward 6.6807\n",
      "Episode 297/400, Highest Score: 142, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.2623, Episode Loss: 2.2052, Mean Score: 55.2357, Mean Reward 6.6646\n",
      "Episode 298/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.2610, Episode Loss: 0.0849, Mean Score: 55.2248, Mean Reward 6.6513\n",
      "Episode 299/400, Highest Score: 142, Episode Score: 54, Episode Reward: 4.5000, Episode Epsilon: 0.2597, Episode Loss: 1.5738, Mean Score: 55.2207, Mean Reward 6.6441\n",
      "Episode 300/400, Highest Score: 142, Episode Score: 51, Episode Reward: 6.2000, Episode Epsilon: 0.2584, Episode Loss: 0.0000, Mean Score: 55.2067, Mean Reward 6.6427\n",
      "Model saved after episode 300\n",
      "Episode 301/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.7000, Episode Epsilon: 0.0000, Episode Loss: 0.0265, Mean Score: 55.1927, Mean Reward 6.6395\n",
      "Episode 302/400, Highest Score: 142, Episode Score: 56, Episode Reward: 13.2000, Episode Epsilon: 0.2571, Episode Loss: 0.3093, Mean Score: 55.1954, Mean Reward 6.6613\n",
      "Episode 303/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.8000, Episode Epsilon: 0.2558, Episode Loss: 1400.7394, Mean Score: 55.1848, Mean Reward 6.6551\n",
      "Episode 304/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.2545, Episode Loss: 48.7885, Mean Score: 55.1743, Mean Reward 6.6467\n",
      "Episode 305/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.2532, Episode Loss: 51.0635, Mean Score: 55.1607, Mean Reward 6.6400\n",
      "Episode 306/400, Highest Score: 142, Episode Score: 74, Episode Reward: 9.6000, Episode Epsilon: 0.2520, Episode Loss: 40.5312, Mean Score: 55.2222, Mean Reward 6.6497\n",
      "Episode 307/400, Highest Score: 142, Episode Score: 62, Episode Reward: 7.5000, Episode Epsilon: 0.2507, Episode Loss: 0.5156, Mean Score: 55.2443, Mean Reward 6.6524\n",
      "Episode 308/400, Highest Score: 142, Episode Score: 55, Episode Reward: 5.7000, Episode Epsilon: 0.2495, Episode Loss: 14.8952, Mean Score: 55.2435, Mean Reward 6.6494\n",
      "Episode 309/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.2482, Episode Loss: 3.2495, Mean Score: 55.2330, Mean Reward 6.6427\n",
      "Episode 310/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.2470, Episode Loss: 3.6210, Mean Score: 55.2226, Mean Reward 6.6345\n",
      "Model saved after episode 310\n",
      "Episode 311/400, Highest Score: 142, Episode Score: 63, Episode Reward: 6.6000, Episode Epsilon: 0.0000, Episode Loss: 2.9061, Mean Score: 55.2476, Mean Reward 6.6344\n",
      "Episode 312/400, Highest Score: 142, Episode Score: 54, Episode Reward: 8.9000, Episode Epsilon: 0.2457, Episode Loss: 0.0265, Mean Score: 55.2436, Mean Reward 6.6417\n",
      "Episode 313/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.2445, Episode Loss: 0.0142, Mean Score: 55.2332, Mean Reward 6.6332\n",
      "Episode 314/400, Highest Score: 142, Episode Score: 52, Episode Reward: 5.2000, Episode Epsilon: 0.2433, Episode Loss: 1.8843, Mean Score: 55.2229, Mean Reward 6.6287\n",
      "Episode 315/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.8000, Episode Epsilon: 0.2421, Episode Loss: 0.0019, Mean Score: 55.2127, Mean Reward 6.6229\n",
      "Episode 316/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.2409, Episode Loss: 0.0026, Mean Score: 55.2025, Mean Reward 6.6165\n",
      "Episode 317/400, Highest Score: 142, Episode Score: 70, Episode Reward: 9.6000, Episode Epsilon: 0.2397, Episode Loss: 2.5154, Mean Score: 55.2492, Mean Reward 6.6259\n",
      "Episode 318/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.2385, Episode Loss: 1.7950, Mean Score: 55.2358, Mean Reward 6.6208\n",
      "Episode 319/400, Highest Score: 142, Episode Score: 70, Episode Reward: 9.6000, Episode Epsilon: 0.2373, Episode Loss: 0.1950, Mean Score: 55.2821, Mean Reward 6.6301\n",
      "Episode 320/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.2361, Episode Loss: 0.5300, Mean Score: 55.2687, Mean Reward 6.6250\n",
      "Model saved after episode 320\n",
      "Episode 321/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.0000, Episode Loss: 3.7304, Mean Score: 55.2555, Mean Reward 6.6196\n",
      "Episode 322/400, Highest Score: 142, Episode Score: 58, Episode Reward: 5.3000, Episode Epsilon: 0.2349, Episode Loss: 1.7413, Mean Score: 55.2640, Mean Reward 6.6155\n",
      "Episode 323/400, Highest Score: 142, Episode Score: 70, Episode Reward: 12.2000, Episode Epsilon: 0.2337, Episode Loss: 0.2818, Mean Score: 55.3096, Mean Reward 6.6328\n",
      "Episode 324/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.2326, Episode Loss: 1.4774, Mean Score: 55.2994, Mean Reward 6.6269\n",
      "Episode 325/400, Highest Score: 142, Episode Score: 67, Episode Reward: 9.4000, Episode Epsilon: 0.2314, Episode Loss: 77.3010, Mean Score: 55.3354, Mean Reward 6.6354\n",
      "Episode 326/400, Highest Score: 142, Episode Score: 53, Episode Reward: 3.1000, Episode Epsilon: 0.2302, Episode Loss: 15.7681, Mean Score: 55.3282, Mean Reward 6.6245\n",
      "Episode 327/400, Highest Score: 142, Episode Score: 59, Episode Reward: 11.7000, Episode Epsilon: 0.2291, Episode Loss: 174.5246, Mean Score: 55.3394, Mean Reward 6.6401\n",
      "Episode 328/400, Highest Score: 142, Episode Score: 54, Episode Reward: 6.8000, Episode Epsilon: 0.2279, Episode Loss: 2.3913, Mean Score: 55.3354, Mean Reward 6.6405\n",
      "Episode 329/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.3000, Episode Epsilon: 0.2268, Episode Loss: 3.2360, Mean Score: 55.3283, Mean Reward 6.6334\n",
      "Episode 330/400, Highest Score: 142, Episode Score: 51, Episode Reward: 6.1000, Episode Epsilon: 0.2257, Episode Loss: 10.9833, Mean Score: 55.3152, Mean Reward 6.6318\n",
      "Model saved after episode 330\n",
      "Episode 331/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.0000, Episode Loss: 2.5830, Mean Score: 55.3051, Mean Reward 6.6236\n",
      "Episode 332/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.2245, Episode Loss: 0.8657, Mean Score: 55.2952, Mean Reward 6.6151\n",
      "Episode 333/400, Highest Score: 142, Episode Score: 52, Episode Reward: 5.0000, Episode Epsilon: 0.2234, Episode Loss: 3.2483, Mean Score: 55.2853, Mean Reward 6.6102\n",
      "Episode 334/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.2223, Episode Loss: 2.5301, Mean Score: 55.2725, Mean Reward 6.6045\n",
      "Episode 335/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.2212, Episode Loss: 0.1915, Mean Score: 55.2597, Mean Reward 6.5985\n",
      "Episode 336/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.2201, Episode Loss: 0.0732, Mean Score: 55.2500, Mean Reward 6.5896\n",
      "Episode 337/400, Highest Score: 142, Episode Score: 62, Episode Reward: 8.1000, Episode Epsilon: 0.2190, Episode Loss: 1.5619, Mean Score: 55.2700, Mean Reward 6.5941\n",
      "Episode 338/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.2179, Episode Loss: 19.8497, Mean Score: 55.2604, Mean Reward 6.5882\n",
      "Episode 339/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.2168, Episode Loss: 0.2717, Mean Score: 55.2478, Mean Reward 6.5823\n",
      "Episode 340/400, Highest Score: 142, Episode Score: 67, Episode Reward: 5.9000, Episode Epsilon: 0.2157, Episode Loss: 0.2382, Mean Score: 55.2824, Mean Reward 6.5803\n",
      "Model saved after episode 340\n",
      "Episode 341/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.0000, Episode Loss: 2.0356, Mean Score: 55.2698, Mean Reward 6.5721\n",
      "Episode 342/400, Highest Score: 142, Episode Score: 58, Episode Reward: 5.0000, Episode Epsilon: 0.2146, Episode Loss: 0.0684, Mean Score: 55.2778, Mean Reward 6.5675\n",
      "Episode 343/400, Highest Score: 142, Episode Score: 75, Episode Reward: 13.5000, Episode Epsilon: 0.2136, Episode Loss: 0.0476, Mean Score: 55.3353, Mean Reward 6.5878\n",
      "Episode 344/400, Highest Score: 142, Episode Score: 53, Episode Reward: 3.8000, Episode Epsilon: 0.2125, Episode Loss: 24.0614, Mean Score: 55.3285, Mean Reward 6.5797\n",
      "Episode 345/400, Highest Score: 142, Episode Score: 70, Episode Reward: 10.3000, Episode Epsilon: 0.2114, Episode Loss: 7.2368, Mean Score: 55.3710, Mean Reward 6.5904\n",
      "Episode 346/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.2104, Episode Loss: 2.2611, Mean Score: 55.3613, Mean Reward 6.5806\n",
      "Episode 347/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.2093, Episode Loss: 3.6022, Mean Score: 55.3487, Mean Reward 6.5738\n",
      "Episode 348/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.2083, Episode Loss: 0.0188, Mean Score: 55.3391, Mean Reward 6.5661\n",
      "Episode 349/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.2072, Episode Loss: 5.6118, Mean Score: 55.3266, Mean Reward 6.5593\n",
      "Episode 350/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.2062, Episode Loss: 0.0004, Mean Score: 55.3171, Mean Reward 6.5520\n",
      "Model saved after episode 350\n",
      "Episode 351/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.0000, Episode Loss: 0.4453, Mean Score: 55.3077, Mean Reward 6.5407\n",
      "Episode 352/400, Highest Score: 142, Episode Score: 88, Episode Reward: 12.2000, Episode Epsilon: 0.2052, Episode Loss: 0.4555, Mean Score: 55.4006, Mean Reward 6.5568\n",
      "Episode 353/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.2041, Episode Loss: 0.1925, Mean Score: 55.3881, Mean Reward 6.5501\n",
      "Episode 354/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.2031, Episode Loss: 0.0350, Mean Score: 55.3757, Mean Reward 6.5446\n",
      "Episode 355/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.2021, Episode Loss: 11.0421, Mean Score: 55.3662, Mean Reward 6.5377\n",
      "Episode 356/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.2011, Episode Loss: 0.0536, Mean Score: 55.3567, Mean Reward 6.5270\n",
      "Episode 357/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.2001, Episode Loss: 0.0429, Mean Score: 55.3473, Mean Reward 6.5179\n",
      "Episode 358/400, Highest Score: 142, Episode Score: 73, Episode Reward: 20.2000, Episode Epsilon: 0.1991, Episode Loss: 0.0669, Mean Score: 55.3966, Mean Reward 6.5561\n",
      "Episode 359/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.1981, Episode Loss: 2.8550, Mean Score: 55.3872, Mean Reward 6.5468\n",
      "Episode 360/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.1971, Episode Loss: 2.7871, Mean Score: 55.3750, Mean Reward 6.5392\n",
      "Model saved after episode 360\n",
      "Episode 361/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.0000, Episode Loss: 4.5611, Mean Score: 55.3629, Mean Reward 6.5316\n",
      "Episode 362/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.1961, Episode Loss: 0.0404, Mean Score: 55.3536, Mean Reward 6.5213\n",
      "Episode 363/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.1951, Episode Loss: 0.2758, Mean Score: 55.3444, Mean Reward 6.5132\n",
      "Episode 364/400, Highest Score: 142, Episode Score: 52, Episode Reward: 6.8000, Episode Epsilon: 0.1942, Episode Loss: 0.0492, Mean Score: 55.3352, Mean Reward 6.5140\n",
      "Episode 365/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.1932, Episode Loss: 1.1906, Mean Score: 55.3260, Mean Reward 6.5052\n",
      "Episode 366/400, Highest Score: 142, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.1922, Episode Loss: 0.5845, Mean Score: 55.3197, Mean Reward 6.4984\n",
      "Episode 367/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.1913, Episode Loss: 5.5476, Mean Score: 55.3106, Mean Reward 6.4921\n",
      "Episode 368/400, Highest Score: 142, Episode Score: 84, Episode Reward: 18.4000, Episode Epsilon: 0.1903, Episode Loss: 0.0955, Mean Score: 55.3886, Mean Reward 6.5245\n",
      "Episode 369/400, Highest Score: 142, Episode Score: 53, Episode Reward: 3.6000, Episode Epsilon: 0.1893, Episode Loss: 0.0790, Mean Score: 55.3821, Mean Reward 6.5165\n",
      "Episode 370/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.1884, Episode Loss: 6.2319, Mean Score: 55.3730, Mean Reward 6.5108\n",
      "Model saved after episode 370\n",
      "Episode 371/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.5000, Episode Epsilon: 0.0000, Episode Loss: 0.0246, Mean Score: 55.3612, Mean Reward 6.5027\n",
      "Episode 372/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.1875, Episode Loss: 0.5773, Mean Score: 55.3495, Mean Reward 6.4957\n",
      "Episode 373/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.0000, Episode Epsilon: 0.1865, Episode Loss: 0.0675, Mean Score: 55.3405, Mean Reward 6.4863\n",
      "Episode 374/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.1856, Episode Loss: 0.0968, Mean Score: 55.3289, Mean Reward 6.4799\n",
      "Episode 375/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.1847, Episode Loss: 0.0353, Mean Score: 55.3200, Mean Reward 6.4715\n",
      "Episode 376/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.1837, Episode Loss: 0.1448, Mean Score: 55.3112, Mean Reward 6.4636\n",
      "Episode 377/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.0000, Episode Epsilon: 0.1828, Episode Loss: 0.0389, Mean Score: 55.3024, Mean Reward 6.4544\n",
      "Episode 378/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.1819, Episode Loss: 0.1961, Mean Score: 55.2910, Mean Reward 6.4481\n",
      "Episode 379/400, Highest Score: 142, Episode Score: 51, Episode Reward: 5.9000, Episode Epsilon: 0.1810, Episode Loss: 0.3824, Mean Score: 55.2797, Mean Reward 6.4467\n",
      "Episode 380/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.1801, Episode Loss: 2.0538, Mean Score: 55.2684, Mean Reward 6.4400\n",
      "Model saved after episode 380\n",
      "Episode 381/400, Highest Score: 142, Episode Score: 75, Episode Reward: 8.0000, Episode Epsilon: 0.0000, Episode Loss: 0.3198, Mean Score: 55.3202, Mean Reward 6.4441\n",
      "Episode 382/400, Highest Score: 142, Episode Score: 54, Episode Reward: 2.7000, Episode Epsilon: 0.1792, Episode Loss: 2.6905, Mean Score: 55.3168, Mean Reward 6.4343\n",
      "Episode 383/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.0000, Episode Epsilon: 0.1783, Episode Loss: 0.1815, Mean Score: 55.3055, Mean Reward 6.4253\n",
      "Episode 384/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1774, Episode Loss: 0.0126, Mean Score: 55.2943, Mean Reward 6.4195\n",
      "Episode 385/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1765, Episode Loss: 0.0312, Mean Score: 55.2831, Mean Reward 6.4138\n",
      "Episode 386/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.1756, Episode Loss: 0.2688, Mean Score: 55.2720, Mean Reward 6.4075\n",
      "Episode 387/400, Highest Score: 142, Episode Score: 51, Episode Reward: 3.5000, Episode Epsilon: 0.1748, Episode Loss: 0.4082, Mean Score: 55.2610, Mean Reward 6.4000\n",
      "Episode 388/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.1739, Episode Loss: 0.3173, Mean Score: 55.2526, Mean Reward 6.3918\n",
      "Episode 389/400, Highest Score: 142, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.1730, Episode Loss: 2.3172, Mean Score: 55.2442, Mean Reward 6.3815\n",
      "Episode 390/400, Highest Score: 142, Episode Score: 55, Episode Reward: 9.8000, Episode Epsilon: 0.1721, Episode Loss: 0.7259, Mean Score: 55.2436, Mean Reward 6.3903\n",
      "Model saved after episode 390\n",
      "Episode 391/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.0000, Episode Loss: 0.6357, Mean Score: 55.2353, Mean Reward 6.3826\n",
      "Episode 392/400, Highest Score: 142, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.1713, Episode Loss: 1.6301, Mean Score: 55.2270, Mean Reward 6.3773\n",
      "Episode 393/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.1704, Episode Loss: 0.0614, Mean Score: 55.2163, Mean Reward 6.3712\n",
      "Episode 394/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1696, Episode Loss: 0.1439, Mean Score: 55.2056, Mean Reward 6.3657\n",
      "Episode 395/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.1687, Episode Loss: 0.4536, Mean Score: 55.1975, Mean Reward 6.3585\n",
      "Episode 396/400, Highest Score: 142, Episode Score: 53, Episode Reward: 5.3000, Episode Epsilon: 0.1679, Episode Loss: 0.0063, Mean Score: 55.1919, Mean Reward 6.3558\n",
      "Episode 397/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.1670, Episode Loss: 0.3696, Mean Score: 55.1839, Mean Reward 6.3481\n",
      "Episode 398/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.1662, Episode Loss: 1.0846, Mean Score: 55.1759, Mean Reward 6.3410\n",
      "Episode 399/400, Highest Score: 142, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.1654, Episode Loss: 0.0725, Mean Score: 55.1679, Mean Reward 6.3331\n",
      "Episode 400/400, Highest Score: 142, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.1646, Episode Loss: 0.2257, Mean Score: 55.1575, Mean Reward 6.3272\n",
      "Model saved after episode 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td></td></tr><tr><td>episode_epsilon</td><td></td></tr><tr><td>episode_loss</td><td></td></tr><tr><td>episode_reward</td><td></td></tr><tr><td>episode_score</td><td></td></tr><tr><td>highest_score</td><td></td></tr><tr><td>mean_current_score</td><td></td></tr><tr><td>mean_loss</td><td></td></tr><tr><td>mean_reward</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0.16455</td></tr><tr><td>episode_loss</td><td>0.22567</td></tr><tr><td>episode_reward</td><td>4.0</td></tr><tr><td>episode_score</td><td>51</td></tr><tr><td>highest_score</td><td>142</td></tr><tr><td>mean_current_score</td><td>55.1575</td></tr><tr><td>mean_loss</td><td>0.22567</td></tr><tr><td>mean_reward</td><td>6.32725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_dqn_agent/runs/9u2gsi4t' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_dqn_agent/runs/9u2gsi4t</a><br/>Synced 5 W&B file(s), 0 media file(s), 40 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_190645-9u2gsi4t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "    \n",
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 400\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05144910-6083-4497-9db9-3f36dd8d2c1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e8afd46-99e3-41a9-82a2-49072b4e5db0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b3b016-4d07-4470-b19d-f68954e8cd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(agent, env, episodes, model_path, log_to_wandb=False, older_model=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_dqn_agent', name='test_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    agent.load_model(model_path, older_model, for_training=False)\n",
    "\n",
    "    # Set exploration rate (epsilon) to 0 to only choose actions based on the model's predictions (exploit its knowledge)\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "            \n",
    "    if log_to_wandb:\n",
    "        # Finish wandb logging        \n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fafc388-89d9-42a8-b9b1-ac8557b7d92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e3ffa30-dc2e-4c13-b57b-6e008152210c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6ebd52c-a8cd-46e9-9e29-7b5dbb66e9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3b3054a3-6522-48a1-aca0-f58a9eb64637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/dino_dqn_episode_200.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "28844155-561c-494c-bccd-685b6dde5758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 64, Episode Score: 64, Episode Reward: 110.6000, Episode Epsilon: 0.0000, Mean Score: 64.0000, Mean Reward 110.6000\n",
      "Episode 2/5, Highest Score: 64, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.0000, Mean Score: 58.0000, Mean Reward 56.0500\n",
      "Episode 3/5, Highest Score: 64, Episode Score: 52, Episode Reward: 1.4000, Episode Epsilon: 0.0000, Mean Score: 56.0000, Mean Reward 37.8333\n",
      "Episode 4/5, Highest Score: 64, Episode Score: 54, Episode Reward: 2.0000, Episode Epsilon: 0.0000, Mean Score: 55.5000, Mean Reward 28.8750\n",
      "Episode 5/5, Highest Score: 64, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.0000, Mean Score: 54.8000, Mean Reward 23.2600\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9105cb9-e495-4938-87d5-c02f86f4457b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d69f549f-b21d-404d-8731-0377490c6845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 53, Episode Score: 53, Episode Reward: 121.2000, Episode Epsilon: 0.0000, Mean Score: 53.0000, Mean Reward 121.2000\n",
      "Episode 2/5, Highest Score: 60, Episode Score: 60, Episode Reward: 18.6000, Episode Epsilon: 0.0000, Mean Score: 56.5000, Mean Reward 69.9000\n",
      "Episode 3/5, Highest Score: 68, Episode Score: 68, Episode Reward: 24.1000, Episode Epsilon: 0.0000, Mean Score: 60.3333, Mean Reward 54.6333\n",
      "Episode 4/5, Highest Score: 83, Episode Score: 83, Episode Reward: 36.4000, Episode Epsilon: 0.0000, Mean Score: 66.0000, Mean Reward 50.0750\n",
      "Episode 5/5, Highest Score: 83, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.0000, Mean Score: 63.4000, Mean Reward 40.9600\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_6/dino_dqn_episode_110.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6f3a07-c678-40cc-b515-f096f986d904",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# All Model Tests\n",
    "Testing a model from each training run to determine the best trained model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e6bbc61-0442-48fb-b618-2ee3efae4e13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63b36917-df2d-4fee-955e-8e51b8f61c19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 338, Episode Score: 338, Episode Reward: 581.2000, Episode Epsilon: 0.0000, Mean Score: 338.0000, Mean Reward 581.2000\n",
      "Episode 2/5, Highest Score: 338, Episode Score: 333, Episode Reward: 59.7000, Episode Epsilon: 0.0000, Mean Score: 335.5000, Mean Reward 320.4500\n",
      "Episode 3/5, Highest Score: 572, Episode Score: 572, Episode Reward: 361.5000, Episode Epsilon: 0.0000, Mean Score: 414.3333, Mean Reward 334.1333\n",
      "Episode 4/5, Highest Score: 572, Episode Score: 259, Episode Reward: 42.4000, Episode Epsilon: 0.0000, Mean Score: 375.5000, Mean Reward 261.2000\n",
      "Episode 5/5, Highest Score: 572, Episode Score: 519, Episode Reward: 82.3000, Episode Epsilon: 0.0000, Mean Score: 404.2000, Mean Reward 225.4200\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_1/episode_100.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False, older_model=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b795aa30-6dac-47f3-a4de-a78d22a49815",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7eacbd9e-c27a-4efa-97f2-da6d24a56c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 158, Episode Score: 158, Episode Reward: 322.1000, Episode Epsilon: 0.0000, Mean Score: 158.0000, Mean Reward 322.1000\n",
      "Episode 2/5, Highest Score: 158, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.0000, Mean Score: 105.0000, Mean Reward 162.9500\n",
      "Episode 3/5, Highest Score: 158, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.0000, Mean Score: 87.3333, Mean Reward 109.9333\n",
      "Episode 4/5, Highest Score: 158, Episode Score: 143, Episode Reward: 26.2000, Episode Epsilon: 0.0000, Mean Score: 101.2500, Mean Reward 89.0000\n",
      "Episode 5/5, Highest Score: 158, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.0000, Mean Score: 91.4000, Mean Reward 71.9000\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_2/dino_dqn_episode_400.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a1486ae-3ebe-4d80-a400-f4fee7f5f440",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_3\n",
    "Used the `ModifiedDinoEnv` class for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a5d0920-c19d-4940-b8e7-9faac10eace8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 51, Episode Score: 51, Episode Reward: 147.8000, Episode Epsilon: 0.0000, Mean Score: 51.0000, Mean Reward 147.8000\n",
      "Episode 2/5, Highest Score: 51, Episode Score: 51, Episode Reward: 136.1000, Episode Epsilon: 0.0000, Mean Score: 51.0000, Mean Reward 141.9500\n",
      "Episode 3/5, Highest Score: 51, Episode Score: 51, Episode Reward: 137.3000, Episode Epsilon: 0.0000, Mean Score: 51.0000, Mean Reward 140.4000\n",
      "Episode 4/5, Highest Score: 52, Episode Score: 52, Episode Reward: 144.2000, Episode Epsilon: 0.0000, Mean Score: 51.2500, Mean Reward 141.3500\n",
      "Episode 5/5, Highest Score: 52, Episode Score: 51, Episode Reward: 141.3000, Episode Epsilon: 0.0000, Mean Score: 51.2000, Mean Reward 141.3400\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_3/dino_dqn_episode_110.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = ModifiedDinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35cf82db-7484-49d5-af3f-ed073393a963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_4\n",
    "Used the `ModifiedDinoEnv` class with a different reward strategy for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca092d2f-8572-48f6-b51b-af2d0c3e0c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 52, Episode Score: 52, Episode Reward: 147.4000, Episode Epsilon: 0.0000, Mean Score: 52.0000, Mean Reward 147.4000\n",
      "Episode 2/5, Highest Score: 52, Episode Score: 52, Episode Reward: 151.0000, Episode Epsilon: 0.0000, Mean Score: 52.0000, Mean Reward 149.2000\n",
      "Episode 3/5, Highest Score: 52, Episode Score: 51, Episode Reward: 143.0000, Episode Epsilon: 0.0000, Mean Score: 51.6667, Mean Reward 147.1333\n",
      "Episode 4/5, Highest Score: 52, Episode Score: 52, Episode Reward: 148.5000, Episode Epsilon: 0.0000, Mean Score: 51.7500, Mean Reward 147.4750\n",
      "Episode 5/5, Highest Score: 52, Episode Score: 51, Episode Reward: 150.0000, Episode Epsilon: 0.0000, Mean Score: 51.6000, Mean Reward 147.9800\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_4/dino_dqn_episode_100.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = ModifiedDinoEnvironment() \n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75a69823-0ae1-42b6-9fab-4442eba1e9b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70cd82e-97e9-455e-8498-7fe562bd1a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 69, Episode Score: 69, Episode Reward: 167.1000, Episode Epsilon: 0.0000, Mean Score: 69.0000, Mean Reward 167.1000\n",
      "Episode 2/5, Highest Score: 69, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.0000, Mean Score: 60.0000, Mean Reward 85.5000\n",
      "Episode 3/5, Highest Score: 69, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.0000, Mean Score: 57.6667, Mean Reward 58.5000\n",
      "Episode 4/5, Highest Score: 69, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.0000, Mean Score: 56.0000, Mean Reward 44.9500\n",
      "Episode 5/5, Highest Score: 69, Episode Score: 53, Episode Reward: 4.4000, Episode Epsilon: 0.0000, Mean Score: 55.4000, Mean Reward 36.8400\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_5/dino_dqn_episode_110.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c7b67c7-1282-4632-9bc0-3a446ccfa10d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67b3a980-c189-4b51-b6b1-093cfb187685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 66, Episode Score: 66, Episode Reward: 161.7000, Episode Epsilon: 0.0000, Mean Score: 66.0000, Mean Reward 161.7000\n",
      "Episode 2/5, Highest Score: 66, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.0000, Mean Score: 59.0000, Mean Reward 83.0000\n",
      "Episode 3/5, Highest Score: 66, Episode Score: 53, Episode Reward: 4.7000, Episode Epsilon: 0.0000, Mean Score: 57.0000, Mean Reward 56.9000\n",
      "Episode 4/5, Highest Score: 66, Episode Score: 66, Episode Reward: 8.5000, Episode Epsilon: 0.0000, Mean Score: 59.2500, Mean Reward 44.8000\n",
      "Episode 5/5, Highest Score: 82, Episode Score: 82, Episode Reward: 43.3000, Episode Epsilon: 0.0000, Mean Score: 63.8000, Mean Reward 44.5000\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5\n",
    "\n",
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_6/dino_dqn_episode_110.pth\"\n",
    "\n",
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b96b0dc1-46c2-402e-b877-f1d5cc199b4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Best Model Test\n",
    "Testing the best model (train_1: `episode_100.pth`) for 50 runs to see the highest score it can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e422a356-b6b1-4f42-95e3-506d6752be7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"best_trained_models\\episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3fbfb8b5-b0b5-45ce-bd31-a5dc29db5212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0bbd7344-6506-40b2-b00d-e6f0a9691bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_222901-bsp2lmb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, Highest Score: 362, Episode Score: 362, Episode Reward: 508.0000, Episode Epsilon: 0.0000, Mean Score: 362.0000, Mean Reward 508.0000\n",
      "Episode 2/50, Highest Score: 403, Episode Score: 403, Episode Reward: 84.7000, Episode Epsilon: 0.0000, Mean Score: 382.5000, Mean Reward 296.3500\n",
      "Episode 3/50, Highest Score: 442, Episode Score: 442, Episode Reward: 54.4000, Episode Epsilon: 0.0000, Mean Score: 402.3333, Mean Reward 215.7000\n",
      "Episode 4/50, Highest Score: 663, Episode Score: 663, Episode Reward: 286.5000, Episode Epsilon: 0.0000, Mean Score: 467.5000, Mean Reward 233.4000\n",
      "Episode 5/50, Highest Score: 663, Episode Score: 260, Episode Reward: 31.4000, Episode Epsilon: 0.0000, Mean Score: 426.0000, Mean Reward 193.0000\n",
      "Episode 6/50, Highest Score: 663, Episode Score: 282, Episode Reward: 33.6000, Episode Epsilon: 0.0000, Mean Score: 402.0000, Mean Reward 166.4333\n",
      "Episode 7/50, Highest Score: 663, Episode Score: 287, Episode Reward: 33.3000, Episode Epsilon: 0.0000, Mean Score: 385.5714, Mean Reward 147.4143\n",
      "Episode 8/50, Highest Score: 663, Episode Score: 221, Episode Reward: 27.0000, Episode Epsilon: 0.0000, Mean Score: 365.0000, Mean Reward 132.3625\n",
      "Episode 9/50, Highest Score: 663, Episode Score: 284, Episode Reward: 35.6000, Episode Epsilon: 0.0000, Mean Score: 356.0000, Mean Reward 121.6111\n",
      "Episode 10/50, Highest Score: 711, Episode Score: 711, Episode Reward: 124.0000, Episode Epsilon: 0.0000, Mean Score: 391.5000, Mean Reward 121.8500\n",
      "Episode 11/50, Highest Score: 711, Episode Score: 271, Episode Reward: 31.9000, Episode Epsilon: 0.0000, Mean Score: 380.5455, Mean Reward 113.6727\n",
      "Episode 12/50, Highest Score: 711, Episode Score: 269, Episode Reward: 30.6000, Episode Epsilon: 0.0000, Mean Score: 371.2500, Mean Reward 106.7500\n",
      "Episode 13/50, Highest Score: 711, Episode Score: 588, Episode Reward: 63.5000, Episode Epsilon: 0.0000, Mean Score: 387.9231, Mean Reward 103.4231\n",
      "Episode 14/50, Highest Score: 711, Episode Score: 408, Episode Reward: 45.8000, Episode Epsilon: 0.0000, Mean Score: 389.3571, Mean Reward 99.3071\n",
      "Episode 15/50, Highest Score: 711, Episode Score: 551, Episode Reward: 61.2000, Episode Epsilon: 0.0000, Mean Score: 400.1333, Mean Reward 96.7667\n",
      "Episode 16/50, Highest Score: 711, Episode Score: 362, Episode Reward: 44.0000, Episode Epsilon: 0.0000, Mean Score: 397.7500, Mean Reward 93.4688\n",
      "Episode 17/50, Highest Score: 711, Episode Score: 488, Episode Reward: 55.8000, Episode Epsilon: 0.0000, Mean Score: 403.0588, Mean Reward 91.2529\n",
      "Episode 18/50, Highest Score: 711, Episode Score: 176, Episode Reward: 22.6000, Episode Epsilon: 0.0000, Mean Score: 390.4444, Mean Reward 87.4389\n",
      "Episode 19/50, Highest Score: 711, Episode Score: 389, Episode Reward: 45.0000, Episode Epsilon: 0.0000, Mean Score: 390.3684, Mean Reward 85.2053\n",
      "Episode 20/50, Highest Score: 711, Episode Score: 425, Episode Reward: 54.7000, Episode Epsilon: 0.0000, Mean Score: 392.1000, Mean Reward 83.6800\n",
      "Episode 21/50, Highest Score: 711, Episode Score: 55, Episode Reward: 29.0000, Episode Epsilon: 0.0000, Mean Score: 376.0476, Mean Reward 81.0762\n",
      "Episode 22/50, Highest Score: 711, Episode Score: 445, Episode Reward: 71.1000, Episode Epsilon: 0.0000, Mean Score: 379.1818, Mean Reward 80.6227\n",
      "Episode 23/50, Highest Score: 778, Episode Score: 778, Episode Reward: 160.8000, Episode Epsilon: 0.0000, Mean Score: 396.5217, Mean Reward 84.1087\n",
      "Episode 24/50, Highest Score: 778, Episode Score: 517, Episode Reward: 79.7000, Episode Epsilon: 0.0000, Mean Score: 401.5417, Mean Reward 83.9250\n",
      "Episode 25/50, Highest Score: 778, Episode Score: 286, Episode Reward: 47.7000, Episode Epsilon: 0.0000, Mean Score: 396.9200, Mean Reward 82.4760\n",
      "Episode 26/50, Highest Score: 1107, Episode Score: 1107, Episode Reward: 575.1000, Episode Epsilon: 0.0000, Mean Score: 424.2308, Mean Reward 101.4231\n",
      "Episode 27/50, Highest Score: 1107, Episode Score: 593, Episode Reward: 88.6000, Episode Epsilon: 0.0000, Mean Score: 430.4815, Mean Reward 100.9481\n",
      "Episode 28/50, Highest Score: 1107, Episode Score: 492, Episode Reward: 77.4000, Episode Epsilon: 0.0000, Mean Score: 432.6786, Mean Reward 100.1071\n",
      "Episode 29/50, Highest Score: 1107, Episode Score: 323, Episode Reward: 53.6000, Episode Epsilon: 0.0000, Mean Score: 428.8966, Mean Reward 98.5034\n",
      "Episode 30/50, Highest Score: 1107, Episode Score: 299, Episode Reward: 50.7000, Episode Epsilon: 0.0000, Mean Score: 424.5667, Mean Reward 96.9100\n",
      "Episode 31/50, Highest Score: 1107, Episode Score: 384, Episode Reward: 61.6000, Episode Epsilon: 0.0000, Mean Score: 423.2581, Mean Reward 95.7710\n",
      "Episode 32/50, Highest Score: 1107, Episode Score: 55, Episode Reward: 61.8000, Episode Epsilon: 0.0000, Mean Score: 411.7500, Mean Reward 94.7094\n",
      "Episode 33/50, Highest Score: 1107, Episode Score: 55, Episode Reward: 12.2000, Episode Epsilon: 0.0000, Mean Score: 400.9394, Mean Reward 92.2091\n",
      "Episode 34/50, Highest Score: 1467, Episode Score: 1467, Episode Reward: 626.9000, Episode Epsilon: 0.0000, Mean Score: 432.2941, Mean Reward 107.9353\n",
      "Episode 35/50, Highest Score: 1467, Episode Score: 539, Episode Reward: 80.8000, Episode Epsilon: 0.0000, Mean Score: 435.3429, Mean Reward 107.1600\n",
      "Episode 36/50, Highest Score: 1467, Episode Score: 780, Episode Reward: 121.0000, Episode Epsilon: 0.0000, Mean Score: 444.9167, Mean Reward 107.5444\n",
      "Episode 37/50, Highest Score: 1467, Episode Score: 1186, Episode Reward: 170.4000, Episode Epsilon: 0.0000, Mean Score: 464.9459, Mean Reward 109.2432\n",
      "Episode 38/50, Highest Score: 1467, Episode Score: 634, Episode Reward: 96.9000, Episode Epsilon: 0.0000, Mean Score: 469.3947, Mean Reward 108.9184\n",
      "Episode 39/50, Highest Score: 1467, Episode Score: 1333, Episode Reward: 189.4000, Episode Epsilon: 0.0000, Mean Score: 491.5385, Mean Reward 110.9821\n",
      "Episode 40/50, Highest Score: 1467, Episode Score: 432, Episode Reward: 70.5000, Episode Epsilon: 0.0000, Mean Score: 490.0500, Mean Reward 109.9700\n",
      "Episode 41/50, Highest Score: 1467, Episode Score: 313, Episode Reward: 51.7000, Episode Epsilon: 0.0000, Mean Score: 485.7317, Mean Reward 108.5488\n",
      "Episode 42/50, Highest Score: 1467, Episode Score: 242, Episode Reward: 43.4000, Episode Epsilon: 0.0000, Mean Score: 479.9286, Mean Reward 106.9976\n",
      "Episode 43/50, Highest Score: 1467, Episode Score: 934, Episode Reward: 137.5000, Episode Epsilon: 0.0000, Mean Score: 490.4884, Mean Reward 107.7070\n",
      "Episode 44/50, Highest Score: 1467, Episode Score: 368, Episode Reward: 59.1000, Episode Epsilon: 0.0000, Mean Score: 487.7045, Mean Reward 106.6023\n",
      "Episode 45/50, Highest Score: 1467, Episode Score: 520, Episode Reward: 77.4000, Episode Epsilon: 0.0000, Mean Score: 488.4222, Mean Reward 105.9533\n",
      "Episode 46/50, Highest Score: 1467, Episode Score: 265, Episode Reward: 45.5000, Episode Epsilon: 0.0000, Mean Score: 483.5652, Mean Reward 104.6391\n",
      "Episode 47/50, Highest Score: 1467, Episode Score: 1121, Episode Reward: 162.3000, Episode Epsilon: 0.0000, Mean Score: 497.1277, Mean Reward 105.8660\n",
      "Episode 48/50, Highest Score: 1467, Episode Score: 838, Episode Reward: 122.2000, Episode Epsilon: 0.0000, Mean Score: 504.2292, Mean Reward 106.2062\n",
      "Episode 49/50, Highest Score: 1467, Episode Score: 821, Episode Reward: 123.8000, Episode Epsilon: 0.0000, Mean Score: 510.6939, Mean Reward 106.5653\n",
      "Episode 50/50, Highest Score: 1467, Episode Score: 261, Episode Reward: 42.3000, Episode Epsilon: 0.0000, Mean Score: 505.7000, Mean Reward 105.2800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td></td></tr><tr><td>episode_epsilon</td><td></td></tr><tr><td>episode_reward</td><td></td></tr><tr><td>episode_score</td><td></td></tr><tr><td>highest_score</td><td></td></tr><tr><td>mean_current_score</td><td></td></tr><tr><td>mean_reward</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0</td></tr><tr><td>episode_reward</td><td>42.3</td></tr><tr><td>episode_score</td><td>261</td></tr><tr><td>highest_score</td><td>1467</td></tr><tr><td>mean_current_score</td><td>505.7</td></tr><tr><td>mean_reward</td><td>105.28</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_222901-bsp2lmb4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=True, older_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2133b91c-7f2e-4bd1-bb6a-449e193d470f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
