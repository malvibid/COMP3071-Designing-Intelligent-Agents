{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dec060-bf08-4a6e-a1d4-1839b4730f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Custom Dino Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94c8c1-c5e5-4b8f-8c22-242647e9b880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64147db4-ea80-49a1-b50e-1fce4f97af98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Environment Components\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "# Selenium for automatically loading and play the game\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02f78b-c77c-4ac9-9a35-7ab41ed4e123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoEnvironment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0937a3c-edcf-487a-af0d-a178a1f5b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dino Game Environment\n",
    "class DinoEnvironment(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "\n",
    "        self.driver = self._create_driver()\n",
    "\n",
    "        # Setup spaces\n",
    "        low_values = np.array(\n",
    "            [0, 0, 0, 6, -1, -1, -1, -1, -1, -1], dtype=np.float32)  # Initial speed is 6, while max speed is 13\n",
    "        high_values = np.array(\n",
    "            [150, 1, 1, 13, 600, 3, 600, 150, 50, 50], dtype=np.float32)  # Canvas dimensions are 600x150\n",
    "        self.observation_space = Box(\n",
    "            low=low_values, high=high_values, shape=(10,), dtype=np.float32)\n",
    "\n",
    "        # Start jumping, Start ducking, Stop ducking, Do nothing - Ducking has been divided into two actions because the agent should also learn the correct ducking duration\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.actions_map = [\n",
    "            (Keys.ARROW_UP, \"key_down\"),  # Start jumping\n",
    "            (Keys.ARROW_DOWN, \"key_down\"),  # Start ducking\n",
    "            (Keys.ARROW_DOWN, \"key_up\"),  # Stop ducking\n",
    "            (Keys.ARROW_RIGHT, \"key_down\")  # Do nothing\n",
    "        ]\n",
    "\n",
    "        # Keep track of number of obstacles the agent has passed\n",
    "        self.passed_obstacles = 0\n",
    "\n",
    "    # Create and return an instance of the Chrome Driver\n",
    "    def _create_driver(self):\n",
    "\n",
    "        # Set options for the WebDriver\n",
    "        options = Options()\n",
    "\n",
    "        # Turn off logging to keep terminal clean\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "        # Keep the browser running after the code finishes executing\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        # Create a Service instance for running the ChromeDriver executable\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "        # Create an instance of the Chrome WebDriver with the specified service and options - The driver object can be used to automate interactions with the Chrome browser\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Maximize the Chrome window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # Encode the obstacle type as an integer\n",
    "    def _encode_obstacle_type(self, obstacle_type):\n",
    "        if obstacle_type == 'CACTUS_SMALL':\n",
    "            return 0\n",
    "        elif obstacle_type == 'CACTUS_LARGE':\n",
    "            return 1\n",
    "        elif obstacle_type == 'PTERODACTYL':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown obstacle type: {obstacle_type}\")\n",
    "\n",
    "    # Get obstacles that are currently on the screen\n",
    "    def _get_obstacles(self):\n",
    "        obstacles = self.driver.execute_script(\n",
    "            \"return Runner.instance_.horizon.obstacles\")\n",
    "        obstacle_info = []\n",
    "        for obstacle in obstacles:\n",
    "            obstacle_type = obstacle['typeConfig']['type']\n",
    "            # Encode the obstacle type as an integer\n",
    "            encoded_obstacle_type = self._encode_obstacle_type(obstacle_type)\n",
    "            obstacle_x = obstacle['xPos']\n",
    "            obstacle_y = obstacle['yPos']\n",
    "            obstacle_width = obstacle['typeConfig']['width']\n",
    "            obstacle_height = obstacle['typeConfig']['height']\n",
    "            obstacle_info.append(\n",
    "                (encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height))\n",
    "        return obstacle_info\n",
    "\n",
    "    # Get Trex's state (Jumping, Ducking or Running/Do nothing)\n",
    "    def _get_trex_info(self):\n",
    "        trex = self.driver.execute_script(\"return Runner.instance_.tRex\")\n",
    "        # xpos remains the same throughout the game - don't need it\n",
    "        trex_y = trex['yPos']\n",
    "        trex_is_jumping = trex['jumping']\n",
    "        trex_is_ducking = trex['ducking']\n",
    "        return trex_y, trex_is_jumping, trex_is_ducking\n",
    "\n",
    "    # Get current game speed\n",
    "    def _get_game_speed(self):\n",
    "        game_speed = self.driver.execute_script(\n",
    "            \"return Runner.instance_.currentSpeed\")\n",
    "        return game_speed\n",
    "\n",
    "    # Get the distance between the Trex and the next obstacle\n",
    "    def _get_distance_to_next_obstacle(self):\n",
    "        trex_x = self.driver.execute_script(\n",
    "            \"return Runner.instance_.tRex.xPos\")  # xpos of trex\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            next_obstacle = obstacles[0]\n",
    "            obstacle_x = next_obstacle[1]  # xpos of next obstacle\n",
    "            distance_to_next_obstacle = obstacle_x - trex_x\n",
    "        else:\n",
    "            distance_to_next_obstacle = None\n",
    "        return distance_to_next_obstacle\n",
    "\n",
    "    # Check if the agent has passed an obstacle\n",
    "    def _passed_obstacle(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            # next_obstacle: [encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height]\n",
    "            next_obstacle = obstacles[0]\n",
    "            trex_x = self.driver.execute_script(\n",
    "                \"return Runner.instance_.tRex.xPos\")\n",
    "            obstacle_x = next_obstacle[1]  # Next obstacles xpos\n",
    "            obstacle_width = next_obstacle[3]  # Next obstacles width\n",
    "            return obstacle_x + obstacle_width < trex_x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Get and return the score for the last game played\n",
    "    def _get_current_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.digits\")))\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Get and return the high score for all games played in current browser session\n",
    "    def _get_high_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.highScore.slice(-5)\")))  # MaxScore=99999, MaxScoreUnits=5\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Capture screenshot of current game state and return the image captured for rendering\n",
    "    def _get_image(self):\n",
    "        # Capture a screenshot of the game canvas as a data URL - string that represents the image in base64-encoded format\n",
    "        data_url = self.driver.execute_script(\n",
    "            \"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "\n",
    "        # Remove the leading text from the data URL using string slicing and decode the remaining base64-encoded data\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        image_data = base64.b64decode(data_url[len(LEADING_TEXT):])\n",
    "\n",
    "        # Convert the binary data in 'image_data' to a 1D NumPy array\n",
    "        image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "\n",
    "        # Decode the image data and create an OpenCV image object - OpenCV Image Shape format (H, W, C) ( rows, columns, and channels )\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Load and Reset the game environment\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Navigate to the Chrome Dino website\n",
    "            self.driver.get(\"chrome://dino/\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            # Ignore \"ERR_INTERNET_DISCONNECTED\" error thrown because this game is available offline\n",
    "            if \"ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                pass  # Ignore the exception.\n",
    "            else:\n",
    "                raise e  # Handle other WebDriverExceptions\n",
    "\n",
    "        # Avoid errors that can arise due to the 'runner-canvas' element not being present - Using WebDriverWait and EC together ensures that the code does not proceed until the required element is present\n",
    "        timeout = 10\n",
    "        WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
    "\n",
    "        # Start game\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the current state of the game and return it as the observation\n",
    "    def get_observation(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        trex_y, trex_is_jumping, trex_is_ducking = self._get_trex_info()\n",
    "        game_speed = self._get_game_speed()\n",
    "        distance_to_next_obstacle = self._get_distance_to_next_obstacle()\n",
    "\n",
    "        state = (\n",
    "            trex_y,\n",
    "            trex_is_jumping,\n",
    "            trex_is_ducking,\n",
    "            game_speed,\n",
    "            distance_to_next_obstacle,\n",
    "            # Unpack the tuple of the first obstacle\n",
    "            *(obstacles[0] if obstacles else (None, None, None, None, None))\n",
    "        )\n",
    "\n",
    "        # Set dtype for state to float32 for consistency and compatibility with the RL algorithm\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        # Replace NaN values with -1\n",
    "        state[np.isnan(state)] = -1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Check if the game is over and return True or False\n",
    "    def is_game_over(self):\n",
    "        # Done if either Trex crashed into an obstacle or reached max score which is 99999\n",
    "        # Check if Trex crashed\n",
    "        crashed = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "        # Get the maximum score from the game\n",
    "        max_score = self.driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.maxScore\")\n",
    "        current_score = self._get_current_score()\n",
    "\n",
    "        return crashed or (current_score >= max_score)\n",
    "\n",
    "    # Calculate and return the reward for the current state of the game\n",
    "    def get_reward(self, done):\n",
    "        # Must maintain the relative importance of different rewards so that the agent can differentiate between the various outcomes and is encouraged to learn a good policy\n",
    "        reward = 0\n",
    "        if done:\n",
    "            # Penalize for crashing into an obstacle\n",
    "            reward -= 10\n",
    "        else:\n",
    "            if self._passed_obstacle():\n",
    "                # Reward for passing an obstacle\n",
    "                reward += 0.5\n",
    "                self.passed_obstacles += 1\n",
    "            else:\n",
    "                # Small reward for staying alive\n",
    "                reward += 0.1\n",
    "\n",
    "        current_score = self._get_current_score()\n",
    "        high_score = self._get_high_score()\n",
    "\n",
    "        if current_score > high_score:\n",
    "            # Bonus reward for surpassing the high score\n",
    "            reward += 1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Take a step in the game environment based on the given action\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take action\n",
    "        # Get key and action mapping\n",
    "        key, action_type = self.actions_map[action]\n",
    "\n",
    "        # Create a new ActionChains object\n",
    "        action_chains = ActionChains(self.driver)\n",
    "\n",
    "        # Perform the key press action\n",
    "        if action_type == \"key_down\":\n",
    "            action_chains.key_down(key).perform()\n",
    "        # Perform the key release action\n",
    "        elif action_type == \"key_up\":\n",
    "            action_chains.key_up(key).perform()\n",
    "\n",
    "        # Get next observation\n",
    "        obs = self.get_observation()\n",
    "\n",
    "        # Check whether game is over\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        # Get reward\n",
    "        reward = self.get_reward(done)\n",
    "\n",
    "        info = {\n",
    "            'current_score': self._get_current_score(),\n",
    "            'high_score': self._get_high_score()\n",
    "        }\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Visualise the game\n",
    "    def render(self, mode: str = 'human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb-array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            cv2.imshow('Dino Game', img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    # Close the game environment and the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42599e-123f-4932-bbe4-8e97663b4222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the Custom Game Environment\n",
    "\n",
    "This section is for testing the Game Environment to ensure it is defined correctly before using it with the Agent for RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc403f4-7400-4656-ba2a-bd79673640d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper class to format and print observations properly\n",
    "def print_formatted_obs(observations):\n",
    "    obs_titles = [\"trex_y\", \"trex_jumping\", \"trex_ducking\", \"game_speed\", \"obst_dist\",\n",
    "                  \"obst_type\", \"obst_x\", \"obst_y\", \"obst_width\", \"obst_height\"]\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(observations, columns=obs_titles)\n",
    "\n",
    "    # Set the pandas display options for better readability (optional)\n",
    "    pd.set_option(\"display.width\", 120)\n",
    "    # pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb1a4d7-bf2a-492e-bbea-96ff3ecda78c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = DinoEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc8c9b8-87a7-407e-bea4-bdbf7585fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0.  0.  0.  6. -1. -1. -1. -1. -1. -1.], [150.   1.   1.  13. 600.   3. 600. 150.  50.  50.], (10,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fba9fa-0135-4ca0-ac02-e2620fe05f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd240adb-926c-4f23-b692-1063bf2942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fedc098e-9ef4-4fde-9ec2-1ed964c28bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c10c2-7cbb-40e1-9289-aaa14a95c7b9",
   "metadata": {},
   "source": [
    "**Note:** Render function works better if using `.py` python files instead of the `.ipynb` notebook to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ab681d-89a2-41bc-9491-ab58d48e497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trex_y  trex_jumping  trex_ducking  game_speed  obst_dist  obst_type  obst_x  obst_y  obst_width  obst_height\n",
      "0      70.0           1.0           0.0       6.017       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "1      43.0           1.0           0.0       6.035       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "2      24.0           1.0           0.0       6.047       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "3      13.0           1.0           0.0       6.063       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "4       7.0           1.0           0.0       6.075       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "..      ...           ...           ...         ...        ...        ...     ...     ...         ...          ...\n",
      "126    11.0           1.0           0.0       7.563       91.0        1.0    96.0    90.0        25.0         50.0\n",
      "127    17.0           1.0           0.0       7.575       74.0        1.0    79.0    90.0        25.0         50.0\n",
      "128    34.0           1.0           0.0       7.589       53.0        1.0    59.0    90.0        25.0         50.0\n",
      "129    61.0           1.0           0.0       7.603       32.0        1.0    38.0    90.0        25.0         50.0\n",
      "130    70.0           1.0           0.0       7.609       23.0        1.0    23.0    90.0        25.0         50.0\n",
      "\n",
      "[131 rows x 10 columns]\n",
      "Episode: 0, Total Reward: 119.99999999999972, , Current Score: 52, High Score: 52\n"
     ]
    }
   ],
   "source": [
    "# Test loop - Play 1 game\n",
    "env = DinoEnvironment()\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = []\n",
    "    # images = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(obs)\n",
    "        all_observations.append(obs)  # Print obs formatted nicely in a table\n",
    "        total_reward += reward\n",
    "\n",
    "        # env.render(mode='human')\n",
    "        # img = env.render(mode='rgb-array')\n",
    "        # images.append(img) # Can use some image library to create a gif using collected images\n",
    "\n",
    "    print_formatted_obs(all_observations)\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, , Current Score: {info['current_score']}, High Score: {info['high_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf346759-391a-4a48-81c2-e5943ed23ad5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. DQN Dino Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc910af-22e2-45ed-8022-ace974d90ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4efbeb-851d-4a23-b8c6-0f33f2a27645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0862b2-3e86-4c79-ad98-078ce2e5ddcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoDQNAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc2b9a48-e081-4727-b071-807a90f131e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoDQNAgent():\n",
    "    def __init__(self, env,\n",
    "                 gamma=0.95,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=32,\n",
    "                 memory_size=100000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]  # 10\n",
    "        self.action_size = env.action_space.n  # 4\n",
    "        self.hidden_sizes = [64, 128]  # number of hidden neurons for the model\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma  # discounting factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min  # min exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # exploration decay per step\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the DQN model architecture - This model will be used to approximate the Q-values of the agent's actions given a state.\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], self.action_size)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Store agents experiences as a tuple\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Determine which action to take given a state\n",
    "    def act(self, state):\n",
    "        # Explore randomly or exploit given the current epsilon value\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            return action\n",
    "\n",
    "    # Update the DQN model using a batch of experiences sampled from the memory\n",
    "    def replay(self):\n",
    "        # Check if the number of experiences (state, action, reward, next_state, done) in the memory is less than the batch size\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # Don't do anything since there's not enough data to create a minibatch for training\n",
    "            return\n",
    "\n",
    "        # Create minibatch from a random sample of experiences from the memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Calculate the expected Q-value for the current state-action pair (q_target)\n",
    "            # If done, - Game has ended, don't need to make predictions about future rewards\n",
    "            q_target = reward\n",
    "            if not done:\n",
    "                # Calculate the Q-values for the next state using the DQN model, i.e., estimate future reward\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "                q_values_next = self.model(next_state)\n",
    "                # Update the target value by adding the discounted maximum Q-value of the next state to the current reward\n",
    "                q_target = reward + self.gamma * \\\n",
    "                    torch.max(q_values_next).item()\n",
    "\n",
    "            # Calculate the Q-values for the current state using the DQN model\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "\n",
    "            # Update/Map the expected Q-value of the chosen action with the calculated target value\n",
    "            q_values_expected = q_values.clone().detach()\n",
    "\n",
    "            q_values_expected[action] = q_target\n",
    "\n",
    "            # Note: q_values_expected is the ground truth for the action that the agent took in the current state vs q_values is the models prediction of what should happen\n",
    "\n",
    "            # Reset the gradients of the optimizer before performing backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss using the Mean Squared Error (MSE) between the current Q-values and the expected Q-values\n",
    "            loss = self.loss_fn(q_values, q_values_expected)\n",
    "\n",
    "            # Perform backpropagation to calculate the gradients of the model's parameters with respect to the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's parameters using the calculated gradients and the optimizer's learning rate\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decrease episolon over time to reduce exploration and increase exploitation of the models learnt knowledge\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "    # Save the current state of the DQN model and optimizer to a file.\n",
    "    def save_model(self, model_name, model_output_dir, log_to_wandb):\n",
    "        # Create a dictionary to store the state of the model, optimizer and any other additional information\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            model_output_dir, model_name)\n",
    "\n",
    "        # Save the state dictionary to a file\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "        if log_to_wandb:\n",
    "            # Save model as a wandb artifact\n",
    "            artifact = wandb.Artifact(model_name, type='model')\n",
    "            artifact.add_file(save_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "    # Load the DQN model and optimizer state from a file.\n",
    "    def load_model(self, file_path, for_training=False):\n",
    "\n",
    "        # Load the state dictionary from the file using the torch.load() function\n",
    "        state = torch.load(file_path)\n",
    "\n",
    "        # Restore the state of the model and optimizer\n",
    "        self.model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "        # Set for_training to true if using the model to continue training from a previously saved state\n",
    "        if for_training:\n",
    "            self.optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1648970-0a2b-4fcd-b73f-e2c59c443464",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Train and Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742893d-09ed-4817-bc9e-2381bba3a6b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c883a5b-c4ac-4d5f-a20c-2817711396b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392289b8-6d05-438e-b037-60f46efe93b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578dbe60-f461-4c8a-8303-3fcd1fd4ba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d966df0-fd81-489d-b867-a35a62148678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, env, episodes, model_output_dir, save_interval=10, log_to_wandb=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='dino_rl_agent', name='train_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Remember agents experience after every step\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Train/Update the model every step\n",
    "        loss = agent.replay()\n",
    "        episode_loss.append(loss)\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_episode_loss = sum(episode_loss) / len(episode_loss)\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Episode Loss: {loss:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"episode_loss\": loss,\n",
    "                \"mean_loss\": mean_episode_loss,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "\n",
    "        # Save the model every save_interval episodes\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            model_name = f\"dino_dqn_episode_{episode + 1}.pth\"\n",
    "            agent.save_model(model_name, model_output_dir, log_to_wandb)\n",
    "            print(f\"Model saved after episode {episode + 1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de6bef-757a-49f3-bdb9-04dc2a6130db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f0fbcd-552c-4eb4-8c8a-23901ac790b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "389c61ea-2ec2-4df1-9e9a-d1f8ff4961ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4da4d70-a7c3-495b-8ab4-9468f6b4a5b5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbidmalvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230506_231309-qvx8u6om</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/dino_rl_agent/runs/qvx8u6om' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/dino_rl_agent/runs/qvx8u6om' target=\"_blank\">https://wandb.ai/bidmalvi/dino_rl_agent/runs/qvx8u6om</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Highest Score: 74, Episode Score: 74, Episode Reward: 160.3000, Episode Epsilon: 0.9950, Episode Loss: 25.6143, Mean Score: 74.0000, Mean Reward 160.3000\n",
      "Episode 2/500, Highest Score: 74, Episode Score: 72, Episode Reward: 7.7000, Episode Epsilon: 0.9900, Episode Loss: 1.9440, Mean Score: 73.0000, Mean Reward 84.0000\n",
      "Episode 3/500, Highest Score: 74, Episode Score: 64, Episode Reward: 4.2000, Episode Epsilon: 0.9851, Episode Loss: 0.8339, Mean Score: 70.0000, Mean Reward 57.4000\n",
      "Episode 4/500, Highest Score: 74, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.9801, Episode Loss: 3.6402, Mean Score: 65.5000, Mean Reward 43.8250\n",
      "Episode 5/500, Highest Score: 74, Episode Score: 63, Episode Reward: 17.2000, Episode Epsilon: 0.9752, Episode Loss: 2.1060, Mean Score: 65.0000, Mean Reward 38.5000\n",
      "Episode 6/500, Highest Score: 74, Episode Score: 60, Episode Reward: 14.4000, Episode Epsilon: 0.9704, Episode Loss: 1.3333, Mean Score: 64.1667, Mean Reward 34.4833\n",
      "Episode 7/500, Highest Score: 74, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.9655, Episode Loss: 1.2495, Mean Score: 62.4286, Mean Reward 30.0143\n",
      "Episode 8/500, Highest Score: 74, Episode Score: 58, Episode Reward: 3.2000, Episode Epsilon: 0.9607, Episode Loss: 209.4354, Mean Score: 61.8750, Mean Reward 26.6625\n",
      "Episode 9/500, Highest Score: 74, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.9559, Episode Loss: 4011.4749, Mean Score: 60.7778, Mean Reward 24.0111\n",
      "Episode 10/500, Highest Score: 74, Episode Score: 53, Episode Reward: 0.9000, Episode Epsilon: 0.9511, Episode Loss: 754.6027, Mean Score: 60.0000, Mean Reward 21.7000\n",
      "Model saved after episode 10\n",
      "Episode 11/500, Highest Score: 74, Episode Score: 64, Episode Reward: 12.6000, Episode Epsilon: 0.9464, Episode Loss: 40.0626, Mean Score: 60.3636, Mean Reward 20.8727\n",
      "Episode 12/500, Highest Score: 74, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.9416, Episode Loss: 31.8007, Mean Score: 59.6667, Mean Reward 19.3417\n",
      "Episode 13/500, Highest Score: 74, Episode Score: 53, Episode Reward: 0.1000, Episode Epsilon: 0.9369, Episode Loss: 38.4315, Mean Score: 59.1538, Mean Reward 17.8615\n",
      "Episode 14/500, Highest Score: 74, Episode Score: 64, Episode Reward: 37.9000, Episode Epsilon: 0.9322, Episode Loss: 1.5934, Mean Score: 59.5000, Mean Reward 19.2929\n",
      "Episode 15/500, Highest Score: 74, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.9276, Episode Loss: 21.1714, Mean Score: 58.9333, Mean Reward 18.2533\n",
      "Episode 16/500, Highest Score: 74, Episode Score: 62, Episode Reward: 11.3000, Episode Epsilon: 0.9229, Episode Loss: 18.7300, Mean Score: 59.1250, Mean Reward 17.8187\n",
      "Episode 17/500, Highest Score: 74, Episode Score: 51, Episode Reward: 2.4000, Episode Epsilon: 0.9183, Episode Loss: 5.6157, Mean Score: 58.6471, Mean Reward 16.9118\n",
      "Episode 18/500, Highest Score: 74, Episode Score: 51, Episode Reward: 2.3000, Episode Epsilon: 0.9137, Episode Loss: 2.6831, Mean Score: 58.2222, Mean Reward 16.1000\n",
      "Episode 19/500, Highest Score: 74, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.9092, Episode Loss: 0.3652, Mean Score: 57.8947, Mean Reward 15.4000\n",
      "Episode 20/500, Highest Score: 74, Episode Score: 53, Episode Reward: 108.7000, Episode Epsilon: 0.9046, Episode Loss: 1.5103, Mean Score: 57.6500, Mean Reward 20.0650\n",
      "Model saved after episode 20\n",
      "Episode 21/500, Highest Score: 74, Episode Score: 53, Episode Reward: 1.1000, Episode Epsilon: 0.9001, Episode Loss: 8.7285, Mean Score: 57.4286, Mean Reward 19.1619\n",
      "Episode 22/500, Highest Score: 74, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.8956, Episode Loss: 8.1898, Mean Score: 57.1364, Mean Reward 18.4182\n",
      "Episode 23/500, Highest Score: 74, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.8911, Episode Loss: 45.1206, Mean Score: 56.8696, Mean Reward 17.7783\n",
      "Episode 24/500, Highest Score: 74, Episode Score: 53, Episode Reward: 6.1000, Episode Epsilon: 0.8867, Episode Loss: 2.1844, Mean Score: 56.7083, Mean Reward 17.2917\n",
      "Episode 25/500, Highest Score: 74, Episode Score: 53, Episode Reward: 2.7000, Episode Epsilon: 0.8822, Episode Loss: 4.6195, Mean Score: 56.5600, Mean Reward 16.7080\n",
      "Episode 26/500, Highest Score: 74, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.8778, Episode Loss: 0.0015, Mean Score: 56.3846, Mean Reward 16.1962\n",
      "Episode 27/500, Highest Score: 74, Episode Score: 64, Episode Reward: 23.5000, Episode Epsilon: 0.8734, Episode Loss: 0.0912, Mean Score: 56.6667, Mean Reward 16.4667\n",
      "Episode 28/500, Highest Score: 74, Episode Score: 51, Episode Reward: 3.1000, Episode Epsilon: 0.8691, Episode Loss: 0.0515, Mean Score: 56.4643, Mean Reward 15.9893\n",
      "Episode 29/500, Highest Score: 74, Episode Score: 53, Episode Reward: 1.8000, Episode Epsilon: 0.8647, Episode Loss: 0.2051, Mean Score: 56.3448, Mean Reward 15.5000\n",
      "Episode 30/500, Highest Score: 74, Episode Score: 50, Episode Reward: 6.5000, Episode Epsilon: 0.8604, Episode Loss: 0.7320, Mean Score: 56.1333, Mean Reward 15.2000\n",
      "Model saved after episode 30\n",
      "Episode 31/500, Highest Score: 74, Episode Score: 63, Episode Reward: 11.2000, Episode Epsilon: 0.8561, Episode Loss: 28.6405, Mean Score: 56.3548, Mean Reward 15.0710\n",
      "Episode 32/500, Highest Score: 74, Episode Score: 73, Episode Reward: 6.6000, Episode Epsilon: 0.8518, Episode Loss: 1.8963, Mean Score: 56.8750, Mean Reward 14.8062\n",
      "Episode 33/500, Highest Score: 74, Episode Score: 52, Episode Reward: 49.4000, Episode Epsilon: 0.8475, Episode Loss: 0.2711, Mean Score: 56.7273, Mean Reward 15.8545\n",
      "Episode 34/500, Highest Score: 74, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.8433, Episode Loss: 0.1068, Mean Score: 56.5882, Mean Reward 15.5118\n",
      "Episode 35/500, Highest Score: 74, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.8391, Episode Loss: 0.1082, Mean Score: 56.4571, Mean Reward 15.1829\n",
      "Episode 36/500, Highest Score: 74, Episode Score: 52, Episode Reward: 8.1000, Episode Epsilon: 0.8349, Episode Loss: 1.5313, Mean Score: 56.3333, Mean Reward 14.9861\n",
      "Episode 37/500, Highest Score: 74, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.8307, Episode Loss: 0.0046, Mean Score: 56.2162, Mean Reward 14.6865\n",
      "Episode 38/500, Highest Score: 74, Episode Score: 73, Episode Reward: 29.4000, Episode Epsilon: 0.8266, Episode Loss: 0.9310, Mean Score: 56.6579, Mean Reward 15.0737\n",
      "Episode 39/500, Highest Score: 74, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.8224, Episode Loss: 54.3644, Mean Score: 56.5128, Mean Reward 14.8051\n",
      "Episode 40/500, Highest Score: 74, Episode Score: 56, Episode Reward: 13.3000, Episode Epsilon: 0.8183, Episode Loss: 0.4077, Mean Score: 56.5000, Mean Reward 14.7675\n",
      "Model saved after episode 40\n",
      "Episode 41/500, Highest Score: 77, Episode Score: 77, Episode Reward: 22.6000, Episode Epsilon: 0.8142, Episode Loss: 0.1343, Mean Score: 57.0000, Mean Reward 14.9585\n",
      "Episode 42/500, Highest Score: 77, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.8102, Episode Loss: 1.1755, Mean Score: 56.8810, Mean Reward 14.6476\n",
      "Episode 43/500, Highest Score: 77, Episode Score: 58, Episode Reward: 10.6000, Episode Epsilon: 0.8061, Episode Loss: 0.0330, Mean Score: 56.9070, Mean Reward 14.5535\n",
      "Episode 44/500, Highest Score: 77, Episode Score: 56, Episode Reward: 8.3000, Episode Epsilon: 0.8021, Episode Loss: 1.3587, Mean Score: 56.8864, Mean Reward 14.4114\n",
      "Episode 45/500, Highest Score: 77, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.7981, Episode Loss: 0.0052, Mean Score: 56.7778, Mean Reward 14.1378\n",
      "Episode 46/500, Highest Score: 77, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.7941, Episode Loss: 0.0006, Mean Score: 56.6739, Mean Reward 13.8870\n",
      "Episode 47/500, Highest Score: 77, Episode Score: 52, Episode Reward: 1.7000, Episode Epsilon: 0.7901, Episode Loss: 0.1686, Mean Score: 56.5745, Mean Reward 13.6277\n",
      "Episode 48/500, Highest Score: 77, Episode Score: 52, Episode Reward: 1.7000, Episode Epsilon: 0.7862, Episode Loss: 0.0139, Mean Score: 56.4792, Mean Reward 13.3792\n",
      "Episode 49/500, Highest Score: 77, Episode Score: 51, Episode Reward: 2.2000, Episode Epsilon: 0.7822, Episode Loss: 11.7927, Mean Score: 56.3673, Mean Reward 13.1510\n",
      "Episode 50/500, Highest Score: 77, Episode Score: 53, Episode Reward: 7.2000, Episode Epsilon: 0.7783, Episode Loss: 0.4717, Mean Score: 56.3000, Mean Reward 13.0320\n",
      "Model saved after episode 50\n",
      "Episode 51/500, Highest Score: 77, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.7744, Episode Loss: 3.1676, Mean Score: 56.2157, Mean Reward 12.8275\n",
      "Episode 52/500, Highest Score: 77, Episode Score: 70, Episode Reward: 21.2000, Episode Epsilon: 0.7705, Episode Loss: 2.5695, Mean Score: 56.4808, Mean Reward 12.9885\n",
      "Episode 53/500, Highest Score: 77, Episode Score: 69, Episode Reward: 18.5000, Episode Epsilon: 0.7667, Episode Loss: 1.2177, Mean Score: 56.7170, Mean Reward 13.0925\n",
      "Episode 54/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.7629, Episode Loss: 0.9184, Mean Score: 56.6111, Mean Reward 12.9296\n",
      "Episode 55/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.7590, Episode Loss: 0.1649, Mean Score: 56.5273, Mean Reward 12.7727\n",
      "Episode 56/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.7553, Episode Loss: 0.0073, Mean Score: 56.4464, Mean Reward 12.6125\n",
      "Episode 57/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.7515, Episode Loss: 3.6942, Mean Score: 56.3509, Mean Reward 12.4737\n",
      "Episode 58/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.7477, Episode Loss: 1.2206, Mean Score: 56.2759, Mean Reward 12.3241\n",
      "Episode 59/500, Highest Score: 77, Episode Score: 65, Episode Reward: 7.1000, Episode Epsilon: 0.7440, Episode Loss: 1.7491, Mean Score: 56.4237, Mean Reward 12.2356\n",
      "Episode 60/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.7403, Episode Loss: 20.2692, Mean Score: 56.3333, Mean Reward 12.1050\n",
      "Model saved after episode 60\n",
      "Episode 61/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.7366, Episode Loss: 0.7766, Mean Score: 56.2623, Mean Reward 11.9705\n",
      "Episode 62/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.7329, Episode Loss: 0.0159, Mean Score: 56.1774, Mean Reward 11.8419\n",
      "Episode 63/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.7292, Episode Loss: 0.3082, Mean Score: 56.1111, Mean Reward 11.7175\n",
      "Episode 64/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.7256, Episode Loss: 2.1455, Mean Score: 56.0469, Mean Reward 11.5937\n",
      "Episode 65/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.7219, Episode Loss: 37.1084, Mean Score: 55.9846, Mean Reward 11.4769\n",
      "Episode 66/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.7183, Episode Loss: 0.1238, Mean Score: 55.9242, Mean Reward 11.3712\n",
      "Episode 67/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.7147, Episode Loss: 0.4027, Mean Score: 55.8507, Mean Reward 11.2731\n",
      "Episode 68/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.7112, Episode Loss: 1.4259, Mean Score: 55.7941, Mean Reward 11.1765\n",
      "Episode 69/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.7076, Episode Loss: 17.5614, Mean Score: 55.7391, Mean Reward 11.0667\n",
      "Episode 70/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.7041, Episode Loss: 2.1443, Mean Score: 55.6857, Mean Reward 10.9643\n",
      "Model saved after episode 70\n",
      "Episode 71/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.7005, Episode Loss: 4.4299, Mean Score: 55.6338, Mean Reward 10.8676\n",
      "Episode 72/500, Highest Score: 77, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.6970, Episode Loss: 0.0897, Mean Score: 55.5694, Mean Reward 10.7722\n",
      "Episode 73/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.6936, Episode Loss: 2.0775, Mean Score: 55.5205, Mean Reward 10.6890\n",
      "Episode 74/500, Highest Score: 77, Episode Score: 51, Episode Reward: 6.4000, Episode Epsilon: 0.6901, Episode Loss: 0.0009, Mean Score: 55.4595, Mean Reward 10.6311\n",
      "Episode 75/500, Highest Score: 77, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.6866, Episode Loss: 4.4858, Mean Score: 55.4133, Mean Reward 10.5440\n",
      "Episode 76/500, Highest Score: 77, Episode Score: 68, Episode Reward: 34.1000, Episode Epsilon: 0.6832, Episode Loss: 0.9899, Mean Score: 55.5789, Mean Reward 10.8539\n",
      "Episode 77/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.6798, Episode Loss: 0.8759, Mean Score: 55.5325, Mean Reward 10.7636\n",
      "Episode 78/500, Highest Score: 77, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.6764, Episode Loss: 8.1145, Mean Score: 55.4744, Mean Reward 10.6756\n",
      "Episode 79/500, Highest Score: 77, Episode Score: 64, Episode Reward: 27.3000, Episode Epsilon: 0.6730, Episode Loss: 0.1227, Mean Score: 55.5823, Mean Reward 10.8861\n",
      "Episode 80/500, Highest Score: 77, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.6696, Episode Loss: 0.0003, Mean Score: 55.5375, Mean Reward 10.7975\n",
      "Model saved after episode 80\n",
      "Episode 81/500, Highest Score: 86, Episode Score: 86, Episode Reward: 26.2000, Episode Epsilon: 0.6663, Episode Loss: 4.5820, Mean Score: 55.9136, Mean Reward 10.9877\n",
      "Episode 82/500, Highest Score: 86, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.6630, Episode Loss: 2.3468, Mean Score: 55.8659, Mean Reward 10.9049\n",
      "Episode 83/500, Highest Score: 86, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.6597, Episode Loss: 0.7027, Mean Score: 55.8193, Mean Reward 10.8253\n",
      "Episode 84/500, Highest Score: 86, Episode Score: 53, Episode Reward: 4.5000, Episode Epsilon: 0.6564, Episode Loss: 1.7963, Mean Score: 55.7857, Mean Reward 10.7500\n",
      "Episode 85/500, Highest Score: 86, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.6531, Episode Loss: 0.0359, Mean Score: 55.7412, Mean Reward 10.6729\n",
      "Episode 86/500, Highest Score: 90, Episode Score: 90, Episode Reward: 109.2000, Episode Epsilon: 0.6498, Episode Loss: 0.0380, Mean Score: 56.1395, Mean Reward 11.8186\n",
      "Episode 87/500, Highest Score: 90, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.6466, Episode Loss: 0.7776, Mean Score: 56.0920, Mean Reward 11.7276\n",
      "Episode 88/500, Highest Score: 90, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.6433, Episode Loss: 0.0733, Mean Score: 56.0455, Mean Reward 11.6420\n",
      "Episode 89/500, Highest Score: 90, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.6401, Episode Loss: 0.0809, Mean Score: 55.9888, Mean Reward 11.5663\n",
      "Episode 90/500, Highest Score: 90, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.6369, Episode Loss: 0.2860, Mean Score: 55.9333, Mean Reward 11.4900\n",
      "Model saved after episode 90\n",
      "Episode 91/500, Highest Score: 90, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.6337, Episode Loss: 0.4416, Mean Score: 55.8791, Mean Reward 11.4088\n",
      "Episode 92/500, Highest Score: 90, Episode Score: 65, Episode Reward: 7.0000, Episode Epsilon: 0.6306, Episode Loss: 0.1792, Mean Score: 55.9783, Mean Reward 11.3609\n",
      "Episode 93/500, Highest Score: 90, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.6274, Episode Loss: 0.0069, Mean Score: 55.9247, Mean Reward 11.2882\n",
      "Episode 94/500, Highest Score: 90, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.6243, Episode Loss: 0.4384, Mean Score: 55.8830, Mean Reward 11.2160\n",
      "Episode 95/500, Highest Score: 92, Episode Score: 92, Episode Reward: 60.6000, Episode Epsilon: 0.6211, Episode Loss: 1.0485, Mean Score: 56.2632, Mean Reward 11.7358\n",
      "Episode 96/500, Highest Score: 92, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.6180, Episode Loss: 8.1826, Mean Score: 56.2188, Mean Reward 11.6542\n",
      "Episode 97/500, Highest Score: 92, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.6149, Episode Loss: 1.4012, Mean Score: 56.1649, Mean Reward 11.5753\n",
      "Episode 98/500, Highest Score: 92, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.6119, Episode Loss: 3.7224, Mean Score: 56.1224, Mean Reward 11.5000\n",
      "Episode 99/500, Highest Score: 92, Episode Score: 52, Episode Reward: 9.6000, Episode Epsilon: 0.6088, Episode Loss: 0.2626, Mean Score: 56.0808, Mean Reward 11.4808\n",
      "Episode 100/500, Highest Score: 92, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.6058, Episode Loss: 0.0388, Mean Score: 56.0400, Mean Reward 11.4130\n",
      "Model saved after episode 100\n",
      "Episode 101/500, Highest Score: 92, Episode Score: 67, Episode Reward: 30.5000, Episode Epsilon: 0.6027, Episode Loss: 0.2662, Mean Score: 56.1485, Mean Reward 11.6020\n",
      "Episode 102/500, Highest Score: 92, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.5997, Episode Loss: 37.7553, Mean Score: 56.1078, Mean Reward 11.5235\n",
      "Episode 103/500, Highest Score: 92, Episode Score: 63, Episode Reward: 24.6000, Episode Epsilon: 0.5967, Episode Loss: 1.5956, Mean Score: 56.1748, Mean Reward 11.6505\n",
      "Episode 104/500, Highest Score: 92, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.5937, Episode Loss: 0.3269, Mean Score: 56.1346, Mean Reward 11.5760\n",
      "Episode 105/500, Highest Score: 92, Episode Score: 71, Episode Reward: 8.4000, Episode Epsilon: 0.5908, Episode Loss: 513.4661, Mean Score: 56.2762, Mean Reward 11.5457\n",
      "Episode 106/500, Highest Score: 92, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.5878, Episode Loss: 0.1542, Mean Score: 56.2264, Mean Reward 11.4774\n",
      "Episode 107/500, Highest Score: 95, Episode Score: 95, Episode Reward: 78.8000, Episode Epsilon: 0.5849, Episode Loss: 22.6389, Mean Score: 56.5888, Mean Reward 12.1065\n",
      "Episode 108/500, Highest Score: 95, Episode Score: 54, Episode Reward: 4.8000, Episode Epsilon: 0.5820, Episode Loss: 182.3464, Mean Score: 56.5648, Mean Reward 12.0389\n",
      "Episode 109/500, Highest Score: 95, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.5790, Episode Loss: 382.1949, Mean Score: 56.5229, Mean Reward 11.9615\n",
      "Episode 110/500, Highest Score: 95, Episode Score: 52, Episode Reward: 1.3000, Episode Epsilon: 0.5762, Episode Loss: 42.0329, Mean Score: 56.4818, Mean Reward 11.8645\n",
      "Model saved after episode 110\n",
      "Episode 111/500, Highest Score: 95, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.5733, Episode Loss: 107.3231, Mean Score: 56.4414, Mean Reward 11.7793\n",
      "Episode 112/500, Highest Score: 95, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.5704, Episode Loss: 1.1154, Mean Score: 56.4018, Mean Reward 11.7000\n",
      "Episode 113/500, Highest Score: 95, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.5676, Episode Loss: 0.7735, Mean Score: 56.3628, Mean Reward 11.6248\n",
      "Episode 114/500, Highest Score: 95, Episode Score: 57, Episode Reward: 12.3000, Episode Epsilon: 0.5647, Episode Loss: 0.7122, Mean Score: 56.3684, Mean Reward 11.6307\n",
      "Episode 115/500, Highest Score: 95, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.5619, Episode Loss: 4.6020, Mean Score: 56.3304, Mean Reward 11.5591\n",
      "Episode 116/500, Highest Score: 95, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.5591, Episode Loss: 0.5272, Mean Score: 56.2931, Mean Reward 11.4888\n",
      "Episode 117/500, Highest Score: 95, Episode Score: 70, Episode Reward: 8.4000, Episode Epsilon: 0.5563, Episode Loss: 0.0012, Mean Score: 56.4103, Mean Reward 11.4624\n",
      "Episode 118/500, Highest Score: 95, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.5535, Episode Loss: 0.0194, Mean Score: 56.3729, Mean Reward 11.3881\n",
      "Episode 119/500, Highest Score: 95, Episode Score: 53, Episode Reward: 2.8000, Episode Epsilon: 0.5507, Episode Loss: 0.8295, Mean Score: 56.3445, Mean Reward 11.3160\n",
      "Episode 120/500, Highest Score: 95, Episode Score: 58, Episode Reward: 3.9000, Episode Epsilon: 0.5480, Episode Loss: 0.3578, Mean Score: 56.3583, Mean Reward 11.2542\n",
      "Model saved after episode 120\n",
      "Episode 121/500, Highest Score: 95, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.5452, Episode Loss: 1.7287, Mean Score: 56.3223, Mean Reward 11.1777\n",
      "Episode 122/500, Highest Score: 95, Episode Score: 56, Episode Reward: 8.3000, Episode Epsilon: 0.5425, Episode Loss: 1.6466, Mean Score: 56.3197, Mean Reward 11.1541\n",
      "Episode 123/500, Highest Score: 95, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.5398, Episode Loss: 0.1006, Mean Score: 56.2764, Mean Reward 11.0992\n",
      "Episode 124/500, Highest Score: 95, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.5371, Episode Loss: 1.4708, Mean Score: 56.2339, Mean Reward 11.0492\n",
      "Episode 125/500, Highest Score: 95, Episode Score: 63, Episode Reward: 5.6000, Episode Epsilon: 0.5344, Episode Loss: 0.1485, Mean Score: 56.2880, Mean Reward 11.0056\n",
      "Episode 126/500, Highest Score: 95, Episode Score: 61, Episode Reward: 6.5000, Episode Epsilon: 0.5318, Episode Loss: 3.4017, Mean Score: 56.3254, Mean Reward 10.9698\n",
      "Episode 127/500, Highest Score: 95, Episode Score: 93, Episode Reward: 58.0000, Episode Epsilon: 0.5291, Episode Loss: 4.6259, Mean Score: 56.6142, Mean Reward 11.3402\n",
      "Episode 128/500, Highest Score: 95, Episode Score: 59, Episode Reward: 5.6000, Episode Epsilon: 0.5264, Episode Loss: 0.6871, Mean Score: 56.6328, Mean Reward 11.2953\n",
      "Episode 129/500, Highest Score: 95, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.5238, Episode Loss: 0.9926, Mean Score: 56.5969, Mean Reward 11.2357\n",
      "Episode 130/500, Highest Score: 95, Episode Score: 64, Episode Reward: 20.7000, Episode Epsilon: 0.5212, Episode Loss: 0.0245, Mean Score: 56.6538, Mean Reward 11.3085\n",
      "Model saved after episode 130\n",
      "Episode 131/500, Highest Score: 95, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.5186, Episode Loss: 2.0774, Mean Score: 56.6183, Mean Reward 11.2527\n",
      "Episode 132/500, Highest Score: 95, Episode Score: 60, Episode Reward: 18.0000, Episode Epsilon: 0.5160, Episode Loss: 0.2732, Mean Score: 56.6439, Mean Reward 11.3038\n",
      "Episode 133/500, Highest Score: 95, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.5134, Episode Loss: 0.0001, Mean Score: 56.6090, Mean Reward 11.2511\n",
      "Episode 134/500, Highest Score: 106, Episode Score: 106, Episode Reward: 37.6000, Episode Epsilon: 0.5108, Episode Loss: 296.5520, Mean Score: 56.9776, Mean Reward 11.4478\n",
      "Episode 135/500, Highest Score: 106, Episode Score: 59, Episode Reward: 17.0000, Episode Epsilon: 0.5083, Episode Loss: 0.6243, Mean Score: 56.9926, Mean Reward 11.4889\n",
      "Episode 136/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.5058, Episode Loss: 0.1430, Mean Score: 56.9559, Mean Reward 11.4324\n",
      "Episode 137/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.5032, Episode Loss: 0.2479, Mean Score: 56.9197, Mean Reward 11.3752\n",
      "Episode 138/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.5007, Episode Loss: 0.0127, Mean Score: 56.8841, Mean Reward 11.3225\n",
      "Episode 139/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.4982, Episode Loss: 15.5450, Mean Score: 56.8417, Mean Reward 11.2712\n",
      "Episode 140/500, Highest Score: 106, Episode Score: 69, Episode Reward: 9.0000, Episode Epsilon: 0.4957, Episode Loss: 15.7208, Mean Score: 56.9286, Mean Reward 11.2550\n",
      "Model saved after episode 140\n",
      "Episode 141/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.4932, Episode Loss: 0.1294, Mean Score: 56.8865, Mean Reward 11.2106\n",
      "Episode 142/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.4908, Episode Loss: 0.0015, Mean Score: 56.8451, Mean Reward 11.1613\n",
      "Episode 143/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.4883, Episode Loss: 1.1679, Mean Score: 56.8042, Mean Reward 11.1161\n",
      "Episode 144/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.4859, Episode Loss: 0.2010, Mean Score: 56.7708, Mean Reward 11.0715\n",
      "Episode 145/500, Highest Score: 106, Episode Score: 93, Episode Reward: 62.6000, Episode Epsilon: 0.4834, Episode Loss: 0.5345, Mean Score: 57.0207, Mean Reward 11.4269\n",
      "Episode 146/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.4810, Episode Loss: 0.1574, Mean Score: 56.9795, Mean Reward 11.3788\n",
      "Episode 147/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.4786, Episode Loss: 3.2979, Mean Score: 56.9456, Mean Reward 11.3265\n",
      "Episode 148/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.4762, Episode Loss: 1.0228, Mean Score: 56.9122, Mean Reward 11.2730\n",
      "Episode 149/500, Highest Score: 106, Episode Score: 56, Episode Reward: 11.9000, Episode Epsilon: 0.4738, Episode Loss: 0.3332, Mean Score: 56.9060, Mean Reward 11.2772\n",
      "Episode 150/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4715, Episode Loss: 0.0678, Mean Score: 56.8667, Mean Reward 11.2307\n",
      "Model saved after episode 150\n",
      "Episode 151/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4691, Episode Loss: 0.6381, Mean Score: 56.8278, Mean Reward 11.1848\n",
      "Episode 152/500, Highest Score: 106, Episode Score: 70, Episode Reward: 11.3000, Episode Epsilon: 0.4668, Episode Loss: 0.1679, Mean Score: 56.9145, Mean Reward 11.1855\n",
      "Episode 153/500, Highest Score: 106, Episode Score: 54, Episode Reward: 10.6000, Episode Epsilon: 0.4644, Episode Loss: 2.7329, Mean Score: 56.8954, Mean Reward 11.1817\n",
      "Episode 154/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4621, Episode Loss: 0.0944, Mean Score: 56.8571, Mean Reward 11.1370\n",
      "Episode 155/500, Highest Score: 106, Episode Score: 60, Episode Reward: 5.1000, Episode Epsilon: 0.4598, Episode Loss: 83.2417, Mean Score: 56.8774, Mean Reward 11.0981\n",
      "Episode 156/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.4575, Episode Loss: 0.3349, Mean Score: 56.8397, Mean Reward 11.0577\n",
      "Episode 157/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.4552, Episode Loss: 0.7724, Mean Score: 56.8089, Mean Reward 11.0108\n",
      "Episode 158/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.4529, Episode Loss: 0.0000, Mean Score: 56.7785, Mean Reward 10.9671\n",
      "Episode 159/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.4507, Episode Loss: 5.9696, Mean Score: 56.7484, Mean Reward 10.9252\n",
      "Episode 160/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.4484, Episode Loss: 3.4735, Mean Score: 56.7125, Mean Reward 10.8875\n",
      "Model saved after episode 160\n",
      "Episode 161/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.4462, Episode Loss: 32.9163, Mean Score: 56.6770, Mean Reward 10.8435\n",
      "Episode 162/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.4440, Episode Loss: 0.7412, Mean Score: 56.6420, Mean Reward 10.8056\n",
      "Episode 163/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.4417, Episode Loss: 66.5777, Mean Score: 56.6135, Mean Reward 10.7693\n",
      "Episode 164/500, Highest Score: 106, Episode Score: 58, Episode Reward: 5.7000, Episode Epsilon: 0.4395, Episode Loss: 0.7366, Mean Score: 56.6220, Mean Reward 10.7384\n",
      "Episode 165/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.4373, Episode Loss: 0.0395, Mean Score: 56.5939, Mean Reward 10.6945\n",
      "Episode 166/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4351, Episode Loss: 6.4856, Mean Score: 56.5663, Mean Reward 10.6536\n",
      "Episode 167/500, Highest Score: 106, Episode Score: 62, Episode Reward: 7.8000, Episode Epsilon: 0.4330, Episode Loss: 1.7212, Mean Score: 56.5988, Mean Reward 10.6365\n",
      "Episode 168/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.4308, Episode Loss: 60.0222, Mean Score: 56.5714, Mean Reward 10.5946\n",
      "Episode 169/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.4286, Episode Loss: 2.9641, Mean Score: 56.5385, Mean Reward 10.5586\n",
      "Episode 170/500, Highest Score: 106, Episode Score: 89, Episode Reward: 78.3000, Episode Epsilon: 0.4265, Episode Loss: 0.0132, Mean Score: 56.7294, Mean Reward 10.9571\n",
      "Model saved after episode 170\n",
      "Episode 171/500, Highest Score: 106, Episode Score: 95, Episode Reward: 18.9000, Episode Epsilon: 0.4244, Episode Loss: 0.3688, Mean Score: 56.9532, Mean Reward 11.0035\n",
      "Episode 172/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.4223, Episode Loss: 0.7647, Mean Score: 56.9244, Mean Reward 10.9657\n",
      "Episode 173/500, Highest Score: 106, Episode Score: 55, Episode Reward: 11.0000, Episode Epsilon: 0.4201, Episode Loss: 1.8929, Mean Score: 56.9133, Mean Reward 10.9659\n",
      "Episode 174/500, Highest Score: 106, Episode Score: 57, Episode Reward: 14.0000, Episode Epsilon: 0.4180, Episode Loss: 2.4225, Mean Score: 56.9138, Mean Reward 10.9833\n",
      "Episode 175/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4159, Episode Loss: 0.7277, Mean Score: 56.8857, Mean Reward 10.9434\n",
      "Episode 176/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.4139, Episode Loss: 1788.5791, Mean Score: 56.8523, Mean Reward 10.9068\n",
      "Episode 177/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.4118, Episode Loss: 74.3006, Mean Score: 56.8192, Mean Reward 10.8701\n",
      "Episode 178/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4097, Episode Loss: 23.6776, Mean Score: 56.7865, Mean Reward 10.8331\n",
      "Episode 179/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.4077, Episode Loss: 61.1755, Mean Score: 56.7598, Mean Reward 10.7966\n",
      "Episode 180/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.4057, Episode Loss: 14.0860, Mean Score: 56.7333, Mean Reward 10.7589\n",
      "Model saved after episode 180\n",
      "Episode 181/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.4036, Episode Loss: 0.5741, Mean Score: 56.7072, Mean Reward 10.7182\n",
      "Episode 182/500, Highest Score: 106, Episode Score: 92, Episode Reward: 78.2000, Episode Epsilon: 0.4016, Episode Loss: 33.9903, Mean Score: 56.9011, Mean Reward 11.0890\n",
      "Episode 183/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.3996, Episode Loss: 2.1040, Mean Score: 56.8743, Mean Reward 11.0508\n",
      "Episode 184/500, Highest Score: 106, Episode Score: 64, Episode Reward: 6.0000, Episode Epsilon: 0.3976, Episode Loss: 2.4789, Mean Score: 56.9130, Mean Reward 11.0234\n",
      "Episode 185/500, Highest Score: 106, Episode Score: 65, Episode Reward: 7.4000, Episode Epsilon: 0.3956, Episode Loss: 0.5594, Mean Score: 56.9568, Mean Reward 11.0038\n",
      "Episode 186/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.3936, Episode Loss: 0.0743, Mean Score: 56.9301, Mean Reward 10.9656\n",
      "Episode 187/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.3917, Episode Loss: 0.6949, Mean Score: 56.8984, Mean Reward 10.9321\n",
      "Episode 188/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.3897, Episode Loss: 0.0077, Mean Score: 56.8670, Mean Reward 10.8973\n",
      "Episode 189/500, Highest Score: 106, Episode Score: 52, Episode Reward: 5.0000, Episode Epsilon: 0.3878, Episode Loss: 0.3703, Mean Score: 56.8413, Mean Reward 10.8661\n",
      "Episode 190/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.3858, Episode Loss: 0.5880, Mean Score: 56.8105, Mean Reward 10.8358\n",
      "Model saved after episode 190\n",
      "Episode 191/500, Highest Score: 106, Episode Score: 56, Episode Reward: 12.4000, Episode Epsilon: 0.3839, Episode Loss: 59.2423, Mean Score: 56.8063, Mean Reward 10.8440\n",
      "Episode 192/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.3820, Episode Loss: 36.5741, Mean Score: 56.7760, Mean Reward 10.8135\n",
      "Episode 193/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.3801, Episode Loss: 0.3363, Mean Score: 56.7461, Mean Reward 10.7839\n",
      "Episode 194/500, Highest Score: 106, Episode Score: 65, Episode Reward: 7.1000, Episode Epsilon: 0.3782, Episode Loss: 0.4397, Mean Score: 56.7887, Mean Reward 10.7649\n",
      "Episode 195/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.3763, Episode Loss: 7.3221, Mean Score: 56.7590, Mean Reward 10.7354\n",
      "Episode 196/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.3744, Episode Loss: 0.0406, Mean Score: 56.7347, Mean Reward 10.7036\n",
      "Episode 197/500, Highest Score: 106, Episode Score: 61, Episode Reward: 17.0000, Episode Epsilon: 0.3725, Episode Loss: 66.4326, Mean Score: 56.7563, Mean Reward 10.7355\n",
      "Episode 198/500, Highest Score: 106, Episode Score: 62, Episode Reward: 12.4000, Episode Epsilon: 0.3707, Episode Loss: 9.9294, Mean Score: 56.7828, Mean Reward 10.7439\n",
      "Episode 199/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.3688, Episode Loss: 2.5412, Mean Score: 56.7588, Mean Reward 10.7065\n",
      "Episode 200/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3670, Episode Loss: 57.1049, Mean Score: 56.7350, Mean Reward 10.6715\n",
      "Model saved after episode 200\n",
      "Episode 201/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.3651, Episode Loss: 7.6457, Mean Score: 56.7065, Mean Reward 10.6418\n",
      "Episode 202/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.3633, Episode Loss: 0.0965, Mean Score: 56.6782, Mean Reward 10.6144\n",
      "Episode 203/500, Highest Score: 106, Episode Score: 67, Episode Reward: 8.6000, Episode Epsilon: 0.3615, Episode Loss: 0.6977, Mean Score: 56.7291, Mean Reward 10.6044\n",
      "Episode 204/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.3597, Episode Loss: 5.2513, Mean Score: 56.7059, Mean Reward 10.5730\n",
      "Episode 205/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.3579, Episode Loss: 0.1956, Mean Score: 56.6829, Mean Reward 10.5385\n",
      "Episode 206/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.3000, Episode Epsilon: 0.3561, Episode Loss: 1.4283, Mean Score: 56.6650, Mean Reward 10.5083\n",
      "Episode 207/500, Highest Score: 106, Episode Score: 62, Episode Reward: 6.1000, Episode Epsilon: 0.3543, Episode Loss: 1.1705, Mean Score: 56.6908, Mean Reward 10.4870\n",
      "Episode 208/500, Highest Score: 106, Episode Score: 66, Episode Reward: 9.5000, Episode Epsilon: 0.3525, Episode Loss: 1.0548, Mean Score: 56.7356, Mean Reward 10.4822\n",
      "Episode 209/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.3508, Episode Loss: 1.0443, Mean Score: 56.7129, Mean Reward 10.4507\n",
      "Episode 210/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.3490, Episode Loss: 20.1804, Mean Score: 56.6905, Mean Reward 10.4200\n",
      "Model saved after episode 210\n",
      "Episode 211/500, Highest Score: 106, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.3473, Episode Loss: 0.0436, Mean Score: 56.6682, Mean Reward 10.3834\n",
      "Episode 212/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3455, Episode Loss: 0.4020, Mean Score: 56.6462, Mean Reward 10.3514\n",
      "Episode 213/500, Highest Score: 106, Episode Score: 58, Episode Reward: 13.9000, Episode Epsilon: 0.3438, Episode Loss: 0.0169, Mean Score: 56.6526, Mean Reward 10.3681\n",
      "Episode 214/500, Highest Score: 106, Episode Score: 51, Episode Reward: 6.4000, Episode Epsilon: 0.3421, Episode Loss: 0.4567, Mean Score: 56.6262, Mean Reward 10.3495\n",
      "Episode 215/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.3404, Episode Loss: 0.6474, Mean Score: 56.6047, Mean Reward 10.3191\n",
      "Episode 216/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.3387, Episode Loss: 0.7051, Mean Score: 56.5833, Mean Reward 10.2912\n",
      "Episode 217/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.3370, Episode Loss: 0.8044, Mean Score: 56.5622, Mean Reward 10.2622\n",
      "Episode 218/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.3353, Episode Loss: 0.0452, Mean Score: 56.5367, Mean Reward 10.2358\n",
      "Episode 219/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.3336, Episode Loss: 0.1320, Mean Score: 56.5160, Mean Reward 10.2082\n",
      "Episode 220/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.3320, Episode Loss: 4.5780, Mean Score: 56.4909, Mean Reward 10.1795\n",
      "Model saved after episode 220\n",
      "Episode 221/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.3303, Episode Loss: 5.9148, Mean Score: 56.4706, Mean Reward 10.1489\n",
      "Episode 222/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.3286, Episode Loss: 2.1660, Mean Score: 56.4459, Mean Reward 10.1230\n",
      "Episode 223/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.3270, Episode Loss: 4.8614, Mean Score: 56.4215, Mean Reward 10.0964\n",
      "Episode 224/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3254, Episode Loss: 0.0936, Mean Score: 56.4018, Mean Reward 10.0674\n",
      "Episode 225/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.4000, Episode Epsilon: 0.3237, Episode Loss: 0.0056, Mean Score: 56.3778, Mean Reward 10.0378\n",
      "Episode 226/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3221, Episode Loss: 82.3676, Mean Score: 56.3584, Mean Reward 10.0097\n",
      "Episode 227/500, Highest Score: 106, Episode Score: 51, Episode Reward: 6.7000, Episode Epsilon: 0.3205, Episode Loss: 5.3270, Mean Score: 56.3348, Mean Reward 9.9952\n",
      "Episode 228/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.3189, Episode Loss: 0.4737, Mean Score: 56.3114, Mean Reward 9.9697\n",
      "Episode 229/500, Highest Score: 106, Episode Score: 95, Episode Reward: 88.7000, Episode Epsilon: 0.3173, Episode Loss: 0.7023, Mean Score: 56.4803, Mean Reward 10.3135\n",
      "Episode 230/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.3157, Episode Loss: 0.4353, Mean Score: 56.4609, Mean Reward 10.2848\n",
      "Model saved after episode 230\n",
      "Episode 231/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.3141, Episode Loss: 6.8578, Mean Score: 56.4372, Mean Reward 10.2580\n",
      "Episode 232/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.8000, Episode Epsilon: 0.3126, Episode Loss: 6.3777, Mean Score: 56.4138, Mean Reward 10.2302\n",
      "Episode 233/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.3110, Episode Loss: 3.8294, Mean Score: 56.3948, Mean Reward 10.2017\n",
      "Episode 234/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.3095, Episode Loss: 0.0443, Mean Score: 56.3718, Mean Reward 10.1795\n",
      "Episode 235/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.3079, Episode Loss: 6.9961, Mean Score: 56.3489, Mean Reward 10.1536\n",
      "Episode 236/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.3064, Episode Loss: 0.2959, Mean Score: 56.3305, Mean Reward 10.1292\n",
      "Episode 237/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.3048, Episode Loss: 0.6553, Mean Score: 56.3080, Mean Reward 10.1055\n",
      "Episode 238/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.3033, Episode Loss: 0.8155, Mean Score: 56.2899, Mean Reward 10.0773\n",
      "Episode 239/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.3018, Episode Loss: 0.0554, Mean Score: 56.2720, Mean Reward 10.0527\n",
      "Episode 240/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.3003, Episode Loss: 0.6106, Mean Score: 56.2500, Mean Reward 10.0292\n",
      "Model saved after episode 240\n",
      "Episode 241/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.2988, Episode Loss: 1.4679, Mean Score: 56.2282, Mean Reward 10.0041\n",
      "Episode 242/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.2973, Episode Loss: 0.4498, Mean Score: 56.2107, Mean Reward 9.9793\n",
      "Episode 243/500, Highest Score: 106, Episode Score: 69, Episode Reward: 19.6000, Episode Epsilon: 0.2958, Episode Loss: 0.0519, Mean Score: 56.2634, Mean Reward 10.0189\n",
      "Episode 244/500, Highest Score: 106, Episode Score: 86, Episode Reward: 46.8000, Episode Epsilon: 0.2943, Episode Loss: 0.1675, Mean Score: 56.3852, Mean Reward 10.1697\n",
      "Episode 245/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2929, Episode Loss: 0.2547, Mean Score: 56.3673, Mean Reward 10.1433\n",
      "Episode 246/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.2914, Episode Loss: 0.5496, Mean Score: 56.3496, Mean Reward 10.1183\n",
      "Episode 247/500, Highest Score: 106, Episode Score: 63, Episode Reward: 6.8000, Episode Epsilon: 0.2899, Episode Loss: 11.5200, Mean Score: 56.3765, Mean Reward 10.1049\n",
      "Episode 248/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.2885, Episode Loss: 1.8433, Mean Score: 56.3548, Mean Reward 10.0823\n",
      "Episode 249/500, Highest Score: 106, Episode Score: 82, Episode Reward: 11.4000, Episode Epsilon: 0.2870, Episode Loss: 0.9281, Mean Score: 56.4578, Mean Reward 10.0876\n",
      "Episode 250/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.2856, Episode Loss: 0.6332, Mean Score: 56.4400, Mean Reward 10.0628\n",
      "Model saved after episode 250\n",
      "Episode 251/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.2842, Episode Loss: 0.0312, Mean Score: 56.4183, Mean Reward 10.0394\n",
      "Episode 252/500, Highest Score: 106, Episode Score: 92, Episode Reward: 105.0000, Episode Epsilon: 0.2828, Episode Loss: 0.0077, Mean Score: 56.5595, Mean Reward 10.4163\n",
      "Episode 253/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.2813, Episode Loss: 0.0133, Mean Score: 56.5415, Mean Reward 10.3913\n",
      "Episode 254/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.2799, Episode Loss: 0.3222, Mean Score: 56.5197, Mean Reward 10.3677\n",
      "Episode 255/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.2785, Episode Loss: 0.0366, Mean Score: 56.4980, Mean Reward 10.3459\n",
      "Episode 256/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.2771, Episode Loss: 0.0176, Mean Score: 56.4805, Mean Reward 10.3187\n",
      "Episode 257/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.2758, Episode Loss: 0.7757, Mean Score: 56.4630, Mean Reward 10.2934\n",
      "Episode 258/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.2744, Episode Loss: 22.8072, Mean Score: 56.4419, Mean Reward 10.2690\n",
      "Episode 259/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.8000, Episode Epsilon: 0.2730, Episode Loss: 0.5613, Mean Score: 56.4247, Mean Reward 10.2479\n",
      "Episode 260/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.2716, Episode Loss: 6.0597, Mean Score: 56.4038, Mean Reward 10.2254\n",
      "Model saved after episode 260\n",
      "Episode 261/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.2703, Episode Loss: 0.5693, Mean Score: 56.3831, Mean Reward 10.2042\n",
      "Episode 262/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.2689, Episode Loss: 0.8033, Mean Score: 56.3626, Mean Reward 10.1840\n",
      "Episode 263/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.2676, Episode Loss: 0.2287, Mean Score: 56.3422, Mean Reward 10.1616\n",
      "Episode 264/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.2663, Episode Loss: 3.6678, Mean Score: 56.3258, Mean Reward 10.1398\n",
      "Episode 265/500, Highest Score: 106, Episode Score: 75, Episode Reward: 34.5000, Episode Epsilon: 0.2649, Episode Loss: 0.7010, Mean Score: 56.3962, Mean Reward 10.2317\n",
      "Episode 266/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.2636, Episode Loss: 2.2075, Mean Score: 56.3797, Mean Reward 10.2060\n",
      "Episode 267/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.2623, Episode Loss: 15.1253, Mean Score: 56.3596, Mean Reward 10.1816\n",
      "Episode 268/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.2610, Episode Loss: 1.5120, Mean Score: 56.3396, Mean Reward 10.1604\n",
      "Episode 269/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.2597, Episode Loss: 0.6464, Mean Score: 56.3197, Mean Reward 10.1401\n",
      "Episode 270/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.2584, Episode Loss: 2.8175, Mean Score: 56.3000, Mean Reward 10.1204\n",
      "Model saved after episode 270\n",
      "Episode 271/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.2571, Episode Loss: 2.2969, Mean Score: 56.2804, Mean Reward 10.0974\n",
      "Episode 272/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2558, Episode Loss: 1.1019, Mean Score: 56.2647, Mean Reward 10.0739\n",
      "Episode 273/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.2545, Episode Loss: 0.3099, Mean Score: 56.2454, Mean Reward 10.0542\n",
      "Episode 274/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.2532, Episode Loss: 0.1854, Mean Score: 56.2299, Mean Reward 10.0307\n",
      "Episode 275/500, Highest Score: 106, Episode Score: 55, Episode Reward: 11.2000, Episode Epsilon: 0.2520, Episode Loss: 0.0545, Mean Score: 56.2255, Mean Reward 10.0349\n",
      "Episode 276/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.2507, Episode Loss: 0.0165, Mean Score: 56.2065, Mean Reward 10.0120\n",
      "Episode 277/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2495, Episode Loss: 0.1574, Mean Score: 56.1913, Mean Reward 9.9892\n",
      "Episode 278/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.2482, Episode Loss: 0.5568, Mean Score: 56.1763, Mean Reward 9.9687\n",
      "Episode 279/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.2470, Episode Loss: 0.7061, Mean Score: 56.1577, Mean Reward 9.9477\n",
      "Episode 280/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.2457, Episode Loss: 1.6038, Mean Score: 56.1429, Mean Reward 9.9275\n",
      "Model saved after episode 280\n",
      "Episode 281/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.2445, Episode Loss: 1.3739, Mean Score: 56.1246, Mean Reward 9.9085\n",
      "Episode 282/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.2433, Episode Loss: 36.1389, Mean Score: 56.1099, Mean Reward 9.8894\n",
      "Episode 283/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.2421, Episode Loss: 0.0052, Mean Score: 56.0954, Mean Reward 9.8703\n",
      "Episode 284/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.2409, Episode Loss: 0.7928, Mean Score: 56.0810, Mean Reward 9.8489\n",
      "Episode 285/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.2397, Episode Loss: 0.4210, Mean Score: 56.0632, Mean Reward 9.8302\n",
      "Episode 286/500, Highest Score: 106, Episode Score: 59, Episode Reward: 6.6000, Episode Epsilon: 0.2385, Episode Loss: 0.2463, Mean Score: 56.0734, Mean Reward 9.8189\n",
      "Episode 287/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.0000, Episode Epsilon: 0.2373, Episode Loss: 3.3549, Mean Score: 56.0592, Mean Reward 9.7986\n",
      "Episode 288/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.2361, Episode Loss: 3.5423, Mean Score: 56.0451, Mean Reward 9.7764\n",
      "Episode 289/500, Highest Score: 106, Episode Score: 66, Episode Reward: 7.9000, Episode Epsilon: 0.2349, Episode Loss: 0.0522, Mean Score: 56.0796, Mean Reward 9.7699\n",
      "Episode 290/500, Highest Score: 106, Episode Score: 52, Episode Reward: 5.0000, Episode Epsilon: 0.2337, Episode Loss: 0.2561, Mean Score: 56.0655, Mean Reward 9.7534\n",
      "Model saved after episode 290\n",
      "Episode 291/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.2326, Episode Loss: 0.0860, Mean Score: 56.0515, Mean Reward 9.7333\n",
      "Episode 292/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.2314, Episode Loss: 0.0265, Mean Score: 56.0377, Mean Reward 9.7130\n",
      "Episode 293/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.2302, Episode Loss: 0.6983, Mean Score: 56.0273, Mean Reward 9.6935\n",
      "Episode 294/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.2291, Episode Loss: 0.3407, Mean Score: 56.0102, Mean Reward 9.6769\n",
      "Episode 295/500, Highest Score: 106, Episode Score: 59, Episode Reward: 6.1000, Episode Epsilon: 0.2279, Episode Loss: 0.1818, Mean Score: 56.0203, Mean Reward 9.6647\n",
      "Episode 296/500, Highest Score: 106, Episode Score: 54, Episode Reward: 4.7000, Episode Epsilon: 0.2268, Episode Loss: 21.3199, Mean Score: 56.0135, Mean Reward 9.6480\n",
      "Episode 297/500, Highest Score: 106, Episode Score: 53, Episode Reward: 5.0000, Episode Epsilon: 0.2257, Episode Loss: 7.4946, Mean Score: 56.0034, Mean Reward 9.6323\n",
      "Episode 298/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2245, Episode Loss: 64.2832, Mean Score: 55.9899, Mean Reward 9.6124\n",
      "Episode 299/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.2234, Episode Loss: 0.6953, Mean Score: 55.9766, Mean Reward 9.5967\n",
      "Episode 300/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2223, Episode Loss: 2.0287, Mean Score: 55.9633, Mean Reward 9.5770\n",
      "Model saved after episode 300\n",
      "Episode 301/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.2212, Episode Loss: 0.4878, Mean Score: 55.9502, Mean Reward 9.5605\n",
      "Episode 302/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.2201, Episode Loss: 0.0243, Mean Score: 55.9371, Mean Reward 9.5404\n",
      "Episode 303/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.2190, Episode Loss: 0.2769, Mean Score: 55.9241, Mean Reward 9.5231\n",
      "Episode 304/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.2179, Episode Loss: 0.2127, Mean Score: 55.9112, Mean Reward 9.5066\n",
      "Episode 305/500, Highest Score: 106, Episode Score: 65, Episode Reward: 6.9000, Episode Epsilon: 0.2168, Episode Loss: 14.6928, Mean Score: 55.9410, Mean Reward 9.4980\n",
      "Episode 306/500, Highest Score: 106, Episode Score: 60, Episode Reward: 6.6000, Episode Epsilon: 0.2157, Episode Loss: 0.0802, Mean Score: 55.9542, Mean Reward 9.4886\n",
      "Episode 307/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.2146, Episode Loss: 6.1460, Mean Score: 55.9414, Mean Reward 9.4726\n",
      "Episode 308/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.2136, Episode Loss: 0.5877, Mean Score: 55.9253, Mean Reward 9.4575\n",
      "Episode 309/500, Highest Score: 106, Episode Score: 71, Episode Reward: 10.2000, Episode Epsilon: 0.2125, Episode Loss: 0.0000, Mean Score: 55.9741, Mean Reward 9.4599\n",
      "Episode 310/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.2114, Episode Loss: 0.2097, Mean Score: 55.9613, Mean Reward 9.4419\n",
      "Model saved after episode 310\n",
      "Episode 311/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.2104, Episode Loss: 1.2085, Mean Score: 55.9453, Mean Reward 9.4248\n",
      "Episode 312/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.2093, Episode Loss: 0.0380, Mean Score: 55.9295, Mean Reward 9.4106\n",
      "Episode 313/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.2083, Episode Loss: 0.0841, Mean Score: 55.9169, Mean Reward 9.3923\n",
      "Episode 314/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.9000, Episode Epsilon: 0.2072, Episode Loss: 0.1192, Mean Score: 55.9013, Mean Reward 9.3812\n",
      "Episode 315/500, Highest Score: 106, Episode Score: 58, Episode Reward: 5.2000, Episode Epsilon: 0.2062, Episode Loss: 0.0281, Mean Score: 55.9079, Mean Reward 9.3679\n",
      "Episode 316/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.4000, Episode Epsilon: 0.2052, Episode Loss: 0.0113, Mean Score: 55.8987, Mean Reward 9.3522\n",
      "Episode 317/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.2041, Episode Loss: 0.1299, Mean Score: 55.8864, Mean Reward 9.3366\n",
      "Episode 318/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.2031, Episode Loss: 0.1933, Mean Score: 55.8742, Mean Reward 9.3179\n",
      "Episode 319/500, Highest Score: 106, Episode Score: 82, Episode Reward: 9.6000, Episode Epsilon: 0.2021, Episode Loss: 0.9967, Mean Score: 55.9561, Mean Reward 9.3188\n",
      "Episode 320/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.5000, Episode Epsilon: 0.2011, Episode Loss: 0.0512, Mean Score: 55.9438, Mean Reward 9.3006\n",
      "Model saved after episode 320\n",
      "Episode 321/500, Highest Score: 106, Episode Score: 82, Episode Reward: 14.0000, Episode Epsilon: 0.2001, Episode Loss: 0.0126, Mean Score: 56.0249, Mean Reward 9.3153\n",
      "Episode 322/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.1991, Episode Loss: 5.7185, Mean Score: 56.0093, Mean Reward 9.3000\n",
      "Episode 323/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.1981, Episode Loss: 6.5482, Mean Score: 55.9969, Mean Reward 9.2851\n",
      "Episode 324/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.1971, Episode Loss: 2.7126, Mean Score: 55.9815, Mean Reward 9.2713\n",
      "Episode 325/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.1961, Episode Loss: 7.1249, Mean Score: 55.9662, Mean Reward 9.2548\n",
      "Episode 326/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.1951, Episode Loss: 0.1389, Mean Score: 55.9509, Mean Reward 9.2411\n",
      "Episode 327/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1942, Episode Loss: 0.0138, Mean Score: 55.9358, Mean Reward 9.2260\n",
      "Episode 328/500, Highest Score: 106, Episode Score: 52, Episode Reward: 58.2000, Episode Epsilon: 0.1932, Episode Loss: 4.8538, Mean Score: 55.9238, Mean Reward 9.3753\n",
      "Episode 329/500, Highest Score: 106, Episode Score: 54, Episode Reward: 3.5000, Episode Epsilon: 0.1922, Episode Loss: 0.0104, Mean Score: 55.9179, Mean Reward 9.3574\n",
      "Episode 330/500, Highest Score: 106, Episode Score: 60, Episode Reward: 4.8000, Episode Epsilon: 0.1913, Episode Loss: 1.2025, Mean Score: 55.9303, Mean Reward 9.3436\n",
      "Model saved after episode 330\n",
      "Episode 331/500, Highest Score: 106, Episode Score: 53, Episode Reward: 2.0000, Episode Epsilon: 0.1903, Episode Loss: 0.0147, Mean Score: 55.9215, Mean Reward 9.3215\n",
      "Episode 332/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1893, Episode Loss: 4.2444, Mean Score: 55.9066, Mean Reward 9.3069\n",
      "Episode 333/500, Highest Score: 106, Episode Score: 57, Episode Reward: 12.5000, Episode Epsilon: 0.1884, Episode Loss: 0.3219, Mean Score: 55.9099, Mean Reward 9.3165\n",
      "Episode 334/500, Highest Score: 106, Episode Score: 60, Episode Reward: 6.1000, Episode Epsilon: 0.1875, Episode Loss: 0.2446, Mean Score: 55.9222, Mean Reward 9.3069\n",
      "Episode 335/500, Highest Score: 106, Episode Score: 54, Episode Reward: 4.4000, Episode Epsilon: 0.1865, Episode Loss: 0.0190, Mean Score: 55.9164, Mean Reward 9.2922\n",
      "Episode 336/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.1856, Episode Loss: 0.2723, Mean Score: 55.9048, Mean Reward 9.2783\n",
      "Episode 337/500, Highest Score: 106, Episode Score: 106, Episode Reward: 21.3000, Episode Epsilon: 0.1847, Episode Loss: 0.0073, Mean Score: 56.0534, Mean Reward 9.3139\n",
      "Episode 338/500, Highest Score: 106, Episode Score: 51, Episode Reward: 12.7000, Episode Epsilon: 0.1837, Episode Loss: 2301.9087, Mean Score: 56.0385, Mean Reward 9.3240\n",
      "Episode 339/500, Highest Score: 106, Episode Score: 76, Episode Reward: 8.9000, Episode Epsilon: 0.1828, Episode Loss: 0.0775, Mean Score: 56.0973, Mean Reward 9.3227\n",
      "Episode 340/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.1819, Episode Loss: 0.0032, Mean Score: 56.0853, Mean Reward 9.3053\n",
      "Model saved after episode 340\n",
      "Episode 341/500, Highest Score: 106, Episode Score: 71, Episode Reward: 9.2000, Episode Epsilon: 0.1810, Episode Loss: 0.6274, Mean Score: 56.1290, Mean Reward 9.3050\n",
      "Episode 342/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.1801, Episode Loss: 0.0346, Mean Score: 56.1170, Mean Reward 9.2886\n",
      "Episode 343/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.1792, Episode Loss: 0.0390, Mean Score: 56.1050, Mean Reward 9.2714\n",
      "Episode 344/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.6000, Episode Epsilon: 0.1783, Episode Loss: 6.3585, Mean Score: 56.0901, Mean Reward 9.2608\n",
      "Episode 345/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.1774, Episode Loss: 0.0102, Mean Score: 56.0754, Mean Reward 9.2467\n",
      "Episode 346/500, Highest Score: 106, Episode Score: 51, Episode Reward: 7.6000, Episode Epsilon: 0.1765, Episode Loss: 0.0054, Mean Score: 56.0607, Mean Reward 9.2419\n",
      "Episode 347/500, Highest Score: 106, Episode Score: 63, Episode Reward: 6.5000, Episode Epsilon: 0.1756, Episode Loss: 1.0867, Mean Score: 56.0807, Mean Reward 9.2340\n",
      "Episode 348/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.1748, Episode Loss: 0.0376, Mean Score: 56.0690, Mean Reward 9.2178\n",
      "Episode 349/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.7000, Episode Epsilon: 0.1739, Episode Loss: 0.0055, Mean Score: 56.0573, Mean Reward 9.2049\n",
      "Episode 350/500, Highest Score: 106, Episode Score: 58, Episode Reward: 6.2000, Episode Epsilon: 0.1730, Episode Loss: 0.0589, Mean Score: 56.0629, Mean Reward 9.1963\n",
      "Model saved after episode 350\n",
      "Episode 351/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.6000, Episode Epsilon: 0.1721, Episode Loss: 0.3612, Mean Score: 56.0513, Mean Reward 9.1803\n",
      "Episode 352/500, Highest Score: 106, Episode Score: 71, Episode Reward: 10.6000, Episode Epsilon: 0.1713, Episode Loss: 0.0177, Mean Score: 56.0938, Mean Reward 9.1844\n",
      "Episode 353/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.0000, Episode Epsilon: 0.1704, Episode Loss: 0.0959, Mean Score: 56.0822, Mean Reward 9.1669\n",
      "Episode 354/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.1696, Episode Loss: 0.0195, Mean Score: 56.0706, Mean Reward 9.1537\n",
      "Episode 355/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.1687, Episode Loss: 0.3703, Mean Score: 56.0592, Mean Reward 9.1394\n",
      "Episode 356/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.1679, Episode Loss: 0.1211, Mean Score: 56.0478, Mean Reward 9.1242\n",
      "Episode 357/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.1670, Episode Loss: 0.0451, Mean Score: 56.0364, Mean Reward 9.1081\n",
      "Episode 358/500, Highest Score: 106, Episode Score: 70, Episode Reward: 8.9000, Episode Epsilon: 0.1662, Episode Loss: 0.0616, Mean Score: 56.0754, Mean Reward 9.1075\n",
      "Episode 359/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.1654, Episode Loss: 0.0359, Mean Score: 56.0613, Mean Reward 9.0955\n",
      "Episode 360/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.1646, Episode Loss: 0.0023, Mean Score: 56.0500, Mean Reward 9.0822\n",
      "Model saved after episode 360\n",
      "Episode 361/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.1637, Episode Loss: 0.2355, Mean Score: 56.0360, Mean Reward 9.0681\n",
      "Episode 362/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.1629, Episode Loss: 0.0266, Mean Score: 56.0249, Mean Reward 9.0539\n",
      "Episode 363/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.1621, Episode Loss: 0.0356, Mean Score: 56.0138, Mean Reward 9.0402\n",
      "Episode 364/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1613, Episode Loss: 0.4371, Mean Score: 56.0000, Mean Reward 9.0277\n",
      "Episode 365/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.1605, Episode Loss: 0.0674, Mean Score: 55.9863, Mean Reward 9.0156\n",
      "Episode 366/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.1597, Episode Loss: 1.0388, Mean Score: 55.9754, Mean Reward 9.0022\n",
      "Episode 367/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.1589, Episode Loss: 9.2744, Mean Score: 55.9619, Mean Reward 8.9907\n",
      "Episode 368/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.4000, Episode Epsilon: 0.1581, Episode Loss: 0.0683, Mean Score: 55.9538, Mean Reward 8.9783\n",
      "Episode 369/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.1573, Episode Loss: 0.0024, Mean Score: 55.9431, Mean Reward 8.9664\n",
      "Episode 370/500, Highest Score: 106, Episode Score: 79, Episode Reward: 9.8000, Episode Epsilon: 0.1565, Episode Loss: 0.1619, Mean Score: 56.0054, Mean Reward 8.9686\n",
      "Model saved after episode 370\n",
      "Episode 371/500, Highest Score: 106, Episode Score: 63, Episode Reward: 6.6000, Episode Epsilon: 0.1557, Episode Loss: 0.0027, Mean Score: 56.0243, Mean Reward 8.9623\n",
      "Episode 372/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.1549, Episode Loss: 0.1561, Mean Score: 56.0108, Mean Reward 8.9516\n",
      "Episode 373/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.8000, Episode Epsilon: 0.1542, Episode Loss: 0.0345, Mean Score: 56.0000, Mean Reward 8.9378\n",
      "Episode 374/500, Highest Score: 106, Episode Score: 54, Episode Reward: 9.1000, Episode Epsilon: 0.1534, Episode Loss: 0.7663, Mean Score: 55.9947, Mean Reward 8.9382\n",
      "Episode 375/500, Highest Score: 106, Episode Score: 70, Episode Reward: 9.4000, Episode Epsilon: 0.1526, Episode Loss: 0.0007, Mean Score: 56.0320, Mean Reward 8.9395\n",
      "Episode 376/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.1519, Episode Loss: 0.0636, Mean Score: 56.0213, Mean Reward 8.9271\n",
      "Episode 377/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.1511, Episode Loss: 1.9826, Mean Score: 56.0106, Mean Reward 8.9143\n",
      "Episode 378/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.8000, Episode Epsilon: 0.1504, Episode Loss: 0.0286, Mean Score: 56.0000, Mean Reward 8.9034\n",
      "Episode 379/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.1496, Episode Loss: 0.0009, Mean Score: 55.9868, Mean Reward 8.8929\n",
      "Episode 380/500, Highest Score: 106, Episode Score: 64, Episode Reward: 6.8000, Episode Epsilon: 0.1489, Episode Loss: 0.0086, Mean Score: 56.0079, Mean Reward 8.8874\n",
      "Model saved after episode 380\n",
      "Episode 381/500, Highest Score: 106, Episode Score: 53, Episode Reward: 3.5000, Episode Epsilon: 0.1481, Episode Loss: 0.1202, Mean Score: 56.0000, Mean Reward 8.8732\n",
      "Episode 382/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.1474, Episode Loss: 0.0394, Mean Score: 55.9895, Mean Reward 8.8597\n",
      "Episode 383/500, Highest Score: 106, Episode Score: 62, Episode Reward: 4.1000, Episode Epsilon: 0.1466, Episode Loss: 0.0319, Mean Score: 56.0052, Mean Reward 8.8473\n",
      "Episode 384/500, Highest Score: 106, Episode Score: 53, Episode Reward: 3.4000, Episode Epsilon: 0.1459, Episode Loss: 0.0000, Mean Score: 55.9974, Mean Reward 8.8331\n",
      "Episode 385/500, Highest Score: 106, Episode Score: 51, Episode Reward: 6.2000, Episode Epsilon: 0.1452, Episode Loss: 2.8313, Mean Score: 55.9844, Mean Reward 8.8262\n",
      "Episode 386/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.1444, Episode Loss: 0.0001, Mean Score: 55.9767, Mean Reward 8.8140\n",
      "Episode 387/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1437, Episode Loss: 0.1254, Mean Score: 55.9638, Mean Reward 8.8021\n",
      "Episode 388/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.1430, Episode Loss: 0.0009, Mean Score: 55.9510, Mean Reward 8.7920\n",
      "Episode 389/500, Highest Score: 106, Episode Score: 58, Episode Reward: 5.9000, Episode Epsilon: 0.1423, Episode Loss: 10.2206, Mean Score: 55.9563, Mean Reward 8.7846\n",
      "Episode 390/500, Highest Score: 106, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.1416, Episode Loss: 0.0251, Mean Score: 55.9436, Mean Reward 8.7751\n",
      "Model saved after episode 390\n",
      "Episode 391/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1409, Episode Loss: 0.0144, Mean Score: 55.9309, Mean Reward 8.7642\n",
      "Episode 392/500, Highest Score: 106, Episode Score: 60, Episode Reward: 12.6000, Episode Epsilon: 0.1402, Episode Loss: 20.2350, Mean Score: 55.9413, Mean Reward 8.7740\n",
      "Episode 393/500, Highest Score: 106, Episode Score: 61, Episode Reward: 11.1000, Episode Epsilon: 0.1395, Episode Loss: 0.0720, Mean Score: 55.9542, Mean Reward 8.7799\n",
      "Episode 394/500, Highest Score: 106, Episode Score: 58, Episode Reward: 10.1000, Episode Epsilon: 0.1388, Episode Loss: 0.1604, Mean Score: 55.9594, Mean Reward 8.7832\n",
      "Episode 395/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.1381, Episode Loss: 11.3937, Mean Score: 55.9494, Mean Reward 8.7704\n",
      "Episode 396/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.1374, Episode Loss: 0.1254, Mean Score: 55.9394, Mean Reward 8.7568\n",
      "Episode 397/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.1367, Episode Loss: 1.7964, Mean Score: 55.9320, Mean Reward 8.7448\n",
      "Episode 398/500, Highest Score: 106, Episode Score: 59, Episode Reward: 11.4000, Episode Epsilon: 0.1360, Episode Loss: 0.9302, Mean Score: 55.9397, Mean Reward 8.7515\n",
      "Episode 399/500, Highest Score: 106, Episode Score: 63, Episode Reward: 6.6000, Episode Epsilon: 0.1353, Episode Loss: 0.7881, Mean Score: 55.9574, Mean Reward 8.7461\n",
      "Episode 400/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.4000, Episode Epsilon: 0.1347, Episode Loss: 0.0256, Mean Score: 55.9500, Mean Reward 8.7352\n",
      "Model saved after episode 400\n",
      "Episode 401/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.1340, Episode Loss: 0.4039, Mean Score: 55.9377, Mean Reward 8.7257\n",
      "Episode 402/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.1333, Episode Loss: 0.4092, Mean Score: 55.9254, Mean Reward 8.7154\n",
      "Episode 403/500, Highest Score: 106, Episode Score: 56, Episode Reward: 9.0000, Episode Epsilon: 0.1326, Episode Loss: 0.0845, Mean Score: 55.9256, Mean Reward 8.7161\n",
      "Episode 404/500, Highest Score: 106, Episode Score: 59, Episode Reward: 8.2000, Episode Epsilon: 0.1320, Episode Loss: 0.0036, Mean Score: 55.9332, Mean Reward 8.7149\n",
      "Episode 405/500, Highest Score: 106, Episode Score: 69, Episode Reward: 14.4000, Episode Epsilon: 0.1313, Episode Loss: 0.0004, Mean Score: 55.9654, Mean Reward 8.7289\n",
      "Episode 406/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1307, Episode Loss: 0.1084, Mean Score: 55.9532, Mean Reward 8.7185\n",
      "Episode 407/500, Highest Score: 106, Episode Score: 70, Episode Reward: 8.9000, Episode Epsilon: 0.1300, Episode Loss: 0.1273, Mean Score: 55.9877, Mean Reward 8.7189\n",
      "Episode 408/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.1294, Episode Loss: 0.0093, Mean Score: 55.9779, Mean Reward 8.7056\n",
      "Episode 409/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.5000, Episode Epsilon: 0.1287, Episode Loss: 0.0000, Mean Score: 55.9682, Mean Reward 8.6954\n",
      "Episode 410/500, Highest Score: 106, Episode Score: 50, Episode Reward: 5.5000, Episode Epsilon: 0.1281, Episode Loss: 0.1566, Mean Score: 55.9537, Mean Reward 8.6876\n",
      "Model saved after episode 410\n",
      "Episode 411/500, Highest Score: 106, Episode Score: 62, Episode Reward: 7.4000, Episode Epsilon: 0.1274, Episode Loss: 0.0643, Mean Score: 55.9684, Mean Reward 8.6844\n",
      "Episode 412/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1268, Episode Loss: 0.0361, Mean Score: 55.9563, Mean Reward 8.6738\n",
      "Episode 413/500, Highest Score: 106, Episode Score: 52, Episode Reward: 3.4000, Episode Epsilon: 0.1262, Episode Loss: 0.0081, Mean Score: 55.9467, Mean Reward 8.6610\n",
      "Episode 414/500, Highest Score: 106, Episode Score: 52, Episode Reward: 4.3000, Episode Epsilon: 0.1255, Episode Loss: 0.0884, Mean Score: 55.9372, Mean Reward 8.6505\n",
      "Episode 415/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1249, Episode Loss: 0.0366, Mean Score: 55.9253, Mean Reward 8.6400\n",
      "Episode 416/500, Highest Score: 106, Episode Score: 53, Episode Reward: 5.1000, Episode Epsilon: 0.1243, Episode Loss: 0.0001, Mean Score: 55.9183, Mean Reward 8.6315\n",
      "Episode 417/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1237, Episode Loss: 0.0018, Mean Score: 55.9065, Mean Reward 8.6211\n",
      "Episode 418/500, Highest Score: 106, Episode Score: 53, Episode Reward: 4.0000, Episode Epsilon: 0.1230, Episode Loss: 0.0211, Mean Score: 55.8995, Mean Reward 8.6100\n",
      "Episode 419/500, Highest Score: 106, Episode Score: 51, Episode Reward: 3.9000, Episode Epsilon: 0.1224, Episode Loss: 0.0106, Mean Score: 55.8878, Mean Reward 8.5988\n",
      "Episode 420/500, Highest Score: 106, Episode Score: 93, Episode Reward: 12.2000, Episode Epsilon: 0.1218, Episode Loss: 0.0282, Mean Score: 55.9762, Mean Reward 8.6074\n",
      "Model saved after episode 420\n",
      "Episode 421/500, Highest Score: 106, Episode Score: 53, Episode Reward: 3.7000, Episode Epsilon: 0.1212, Episode Loss: 0.0259, Mean Score: 55.9691, Mean Reward 8.5957\n",
      "Episode 422/500, Highest Score: 106, Episode Score: 51, Episode Reward: 6.9000, Episode Epsilon: 0.1206, Episode Loss: 0.0926, Mean Score: 55.9573, Mean Reward 8.5917\n",
      "Episode 423/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.7000, Episode Epsilon: 0.1200, Episode Loss: 0.3836, Mean Score: 55.9456, Mean Reward 8.5825\n",
      "Episode 424/500, Highest Score: 106, Episode Score: 69, Episode Reward: 12.5000, Episode Epsilon: 0.1194, Episode Loss: 2.8229, Mean Score: 55.9764, Mean Reward 8.5917\n",
      "Episode 425/500, Highest Score: 106, Episode Score: 56, Episode Reward: 11.6000, Episode Epsilon: 0.1188, Episode Loss: 0.0017, Mean Score: 55.9765, Mean Reward 8.5988\n",
      "Episode 426/500, Highest Score: 106, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.1182, Episode Loss: 0.0022, Mean Score: 55.9648, Mean Reward 8.5894\n",
      "Episode 427/500, Highest Score: 130, Episode Score: 130, Episode Reward: 36.7000, Episode Epsilon: 0.1176, Episode Loss: 0.0002, Mean Score: 56.1382, Mean Reward 8.6553\n",
      "Episode 428/500, Highest Score: 130, Episode Score: 60, Episode Reward: 6.5000, Episode Epsilon: 0.1170, Episode Loss: 0.0434, Mean Score: 56.1472, Mean Reward 8.6502\n",
      "Episode 429/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1164, Episode Loss: 0.0000, Mean Score: 56.1352, Mean Reward 8.6401\n",
      "Episode 430/500, Highest Score: 130, Episode Score: 67, Episode Reward: 8.4000, Episode Epsilon: 0.1159, Episode Loss: 0.3490, Mean Score: 56.1605, Mean Reward 8.6395\n",
      "Model saved after episode 430\n",
      "Episode 431/500, Highest Score: 130, Episode Score: 53, Episode Reward: 5.9000, Episode Epsilon: 0.1153, Episode Loss: 0.0004, Mean Score: 56.1531, Mean Reward 8.6332\n",
      "Episode 432/500, Highest Score: 130, Episode Score: 52, Episode Reward: 5.1000, Episode Epsilon: 0.1147, Episode Loss: 0.5460, Mean Score: 56.1435, Mean Reward 8.6250\n",
      "Episode 433/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1141, Episode Loss: 0.0633, Mean Score: 56.1316, Mean Reward 8.6155\n",
      "Episode 434/500, Highest Score: 130, Episode Score: 56, Episode Reward: 12.2000, Episode Epsilon: 0.1136, Episode Loss: 0.0394, Mean Score: 56.1313, Mean Reward 8.6237\n",
      "Episode 435/500, Highest Score: 130, Episode Score: 56, Episode Reward: 9.6000, Episode Epsilon: 0.1130, Episode Loss: 0.3087, Mean Score: 56.1310, Mean Reward 8.6260\n",
      "Episode 436/500, Highest Score: 130, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.1124, Episode Loss: 1.2987, Mean Score: 56.1216, Mean Reward 8.6138\n",
      "Episode 437/500, Highest Score: 130, Episode Score: 53, Episode Reward: 3.0000, Episode Epsilon: 0.1119, Episode Loss: 0.0538, Mean Score: 56.1144, Mean Reward 8.6009\n",
      "Episode 438/500, Highest Score: 130, Episode Score: 51, Episode Reward: 3.3000, Episode Epsilon: 0.1113, Episode Loss: 0.0351, Mean Score: 56.1027, Mean Reward 8.5888\n",
      "Episode 439/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.1107, Episode Loss: 3.6369, Mean Score: 56.0911, Mean Reward 8.5786\n",
      "Episode 440/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1102, Episode Loss: 0.0769, Mean Score: 56.0795, Mean Reward 8.5693\n",
      "Model saved after episode 440\n",
      "Episode 441/500, Highest Score: 130, Episode Score: 70, Episode Reward: 7.8000, Episode Epsilon: 0.1096, Episode Loss: 0.1955, Mean Score: 56.1111, Mean Reward 8.5676\n",
      "Episode 442/500, Highest Score: 130, Episode Score: 57, Episode Reward: 8.7000, Episode Epsilon: 0.1091, Episode Loss: 0.2848, Mean Score: 56.1131, Mean Reward 8.5679\n",
      "Episode 443/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.1085, Episode Loss: 0.0991, Mean Score: 56.1016, Mean Reward 8.5582\n",
      "Episode 444/500, Highest Score: 130, Episode Score: 59, Episode Reward: 5.3000, Episode Epsilon: 0.1080, Episode Loss: 0.0198, Mean Score: 56.1081, Mean Reward 8.5509\n",
      "Episode 445/500, Highest Score: 130, Episode Score: 53, Episode Reward: 4.4000, Episode Epsilon: 0.1075, Episode Loss: 0.0207, Mean Score: 56.1011, Mean Reward 8.5416\n",
      "Episode 446/500, Highest Score: 130, Episode Score: 65, Episode Reward: 7.8000, Episode Epsilon: 0.1069, Episode Loss: 0.1687, Mean Score: 56.1211, Mean Reward 8.5399\n",
      "Episode 447/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1064, Episode Loss: 0.5016, Mean Score: 56.1096, Mean Reward 8.5302\n",
      "Episode 448/500, Highest Score: 130, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.1059, Episode Loss: 0.3536, Mean Score: 56.0982, Mean Reward 8.5225\n",
      "Episode 449/500, Highest Score: 130, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.1053, Episode Loss: 0.4561, Mean Score: 56.0891, Mean Reward 8.5134\n",
      "Episode 450/500, Highest Score: 130, Episode Score: 65, Episode Reward: 7.5000, Episode Epsilon: 0.1048, Episode Loss: 0.0100, Mean Score: 56.1089, Mean Reward 8.5111\n",
      "Model saved after episode 450\n",
      "Episode 451/500, Highest Score: 130, Episode Score: 66, Episode Reward: 11.5000, Episode Epsilon: 0.1043, Episode Loss: 0.3806, Mean Score: 56.1308, Mean Reward 8.5177\n",
      "Episode 452/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.1038, Episode Loss: 0.0017, Mean Score: 56.1195, Mean Reward 8.5082\n",
      "Episode 453/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.6000, Episode Epsilon: 0.1032, Episode Loss: 0.0407, Mean Score: 56.1082, Mean Reward 8.4996\n",
      "Episode 454/500, Highest Score: 130, Episode Score: 52, Episode Reward: 4.6000, Episode Epsilon: 0.1027, Episode Loss: 0.0836, Mean Score: 56.0991, Mean Reward 8.4910\n",
      "Episode 455/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.5000, Episode Epsilon: 0.1022, Episode Loss: 0.0001, Mean Score: 56.0879, Mean Reward 8.4822\n",
      "Episode 456/500, Highest Score: 130, Episode Score: 66, Episode Reward: 8.0000, Episode Epsilon: 0.1017, Episode Loss: 0.0655, Mean Score: 56.1096, Mean Reward 8.4811\n",
      "Episode 457/500, Highest Score: 130, Episode Score: 61, Episode Reward: 6.1000, Episode Epsilon: 0.1012, Episode Loss: 0.0064, Mean Score: 56.1204, Mean Reward 8.4759\n",
      "Episode 458/500, Highest Score: 130, Episode Score: 67, Episode Reward: 7.9000, Episode Epsilon: 0.1007, Episode Loss: 0.0105, Mean Score: 56.1441, Mean Reward 8.4747\n",
      "Episode 459/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.1000, Episode Epsilon: 0.1002, Episode Loss: 0.2602, Mean Score: 56.1329, Mean Reward 8.4651\n",
      "Episode 460/500, Highest Score: 130, Episode Score: 54, Episode Reward: 4.5000, Episode Epsilon: 0.0997, Episode Loss: 29.5285, Mean Score: 56.1283, Mean Reward 8.4565\n",
      "Model saved after episode 460\n",
      "Episode 461/500, Highest Score: 130, Episode Score: 71, Episode Reward: 8.0000, Episode Epsilon: 0.0992, Episode Loss: 0.0510, Mean Score: 56.1605, Mean Reward 8.4555\n",
      "Episode 462/500, Highest Score: 130, Episode Score: 50, Episode Reward: 5.6000, Episode Epsilon: 0.0987, Episode Loss: 0.0325, Mean Score: 56.1472, Mean Reward 8.4494\n",
      "Episode 463/500, Highest Score: 130, Episode Score: 90, Episode Reward: 11.4000, Episode Epsilon: 0.0982, Episode Loss: 0.0938, Mean Score: 56.2203, Mean Reward 8.4557\n",
      "Episode 464/500, Highest Score: 130, Episode Score: 53, Episode Reward: 4.1000, Episode Epsilon: 0.0977, Episode Loss: 0.1251, Mean Score: 56.2134, Mean Reward 8.4463\n",
      "Episode 465/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.0972, Episode Loss: 0.0002, Mean Score: 56.2022, Mean Reward 8.4376\n",
      "Episode 466/500, Highest Score: 130, Episode Score: 51, Episode Reward: 6.2000, Episode Epsilon: 0.0967, Episode Loss: 0.5445, Mean Score: 56.1910, Mean Reward 8.4328\n",
      "Episode 467/500, Highest Score: 130, Episode Score: 71, Episode Reward: 9.7000, Episode Epsilon: 0.0962, Episode Loss: 0.0054, Mean Score: 56.2227, Mean Reward 8.4355\n",
      "Episode 468/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.0958, Episode Loss: 0.6699, Mean Score: 56.2115, Mean Reward 8.4280\n",
      "Episode 469/500, Highest Score: 130, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.0953, Episode Loss: 0.1884, Mean Score: 56.2026, Mean Reward 8.4188\n",
      "Episode 470/500, Highest Score: 130, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.0948, Episode Loss: 0.1811, Mean Score: 56.1936, Mean Reward 8.4068\n",
      "Model saved after episode 470\n",
      "Episode 471/500, Highest Score: 130, Episode Score: 53, Episode Reward: 4.2000, Episode Epsilon: 0.0943, Episode Loss: 2.1669, Mean Score: 56.1868, Mean Reward 8.3979\n",
      "Episode 472/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.0939, Episode Loss: 0.3452, Mean Score: 56.1758, Mean Reward 8.3894\n",
      "Episode 473/500, Highest Score: 130, Episode Score: 51, Episode Reward: 3.6000, Episode Epsilon: 0.0934, Episode Loss: 3.4062, Mean Score: 56.1649, Mean Reward 8.3793\n",
      "Episode 474/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.0929, Episode Loss: 0.2070, Mean Score: 56.1540, Mean Reward 8.3700\n",
      "Episode 475/500, Highest Score: 130, Episode Score: 79, Episode Reward: 8.8000, Episode Epsilon: 0.0925, Episode Loss: 2.7825, Mean Score: 56.2021, Mean Reward 8.3709\n",
      "Episode 476/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.0920, Episode Loss: 0.0037, Mean Score: 56.1912, Mean Reward 8.3618\n",
      "Episode 477/500, Highest Score: 130, Episode Score: 65, Episode Reward: 9.7000, Episode Epsilon: 0.0915, Episode Loss: 7.8539, Mean Score: 56.2096, Mean Reward 8.3646\n",
      "Episode 478/500, Highest Score: 130, Episode Score: 85, Episode Reward: 11.8000, Episode Epsilon: 0.0911, Episode Loss: 1.1495, Mean Score: 56.2699, Mean Reward 8.3718\n",
      "Episode 479/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.8000, Episode Epsilon: 0.0906, Episode Loss: 0.4404, Mean Score: 56.2589, Mean Reward 8.3643\n",
      "Episode 480/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.0902, Episode Loss: 0.0005, Mean Score: 56.2479, Mean Reward 8.3556\n",
      "Model saved after episode 480\n",
      "Episode 481/500, Highest Score: 130, Episode Score: 52, Episode Reward: 4.9000, Episode Epsilon: 0.0897, Episode Loss: 0.2069, Mean Score: 56.2391, Mean Reward 8.3484\n",
      "Episode 482/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.9000, Episode Epsilon: 0.0893, Episode Loss: 0.0002, Mean Score: 56.2282, Mean Reward 8.3413\n",
      "Episode 483/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.0888, Episode Loss: 0.0000, Mean Score: 56.2174, Mean Reward 8.3329\n",
      "Episode 484/500, Highest Score: 130, Episode Score: 55, Episode Reward: 2.7000, Episode Epsilon: 0.0884, Episode Loss: 0.2265, Mean Score: 56.2149, Mean Reward 8.3213\n",
      "Episode 485/500, Highest Score: 130, Episode Score: 58, Episode Reward: 5.4000, Episode Epsilon: 0.0879, Episode Loss: 0.1671, Mean Score: 56.2186, Mean Reward 8.3153\n",
      "Episode 486/500, Highest Score: 130, Episode Score: 63, Episode Reward: 4.8000, Episode Epsilon: 0.0875, Episode Loss: 1.4406, Mean Score: 56.2325, Mean Reward 8.3080\n",
      "Episode 487/500, Highest Score: 130, Episode Score: 70, Episode Reward: 29.6000, Episode Epsilon: 0.0871, Episode Loss: 0.5942, Mean Score: 56.2608, Mean Reward 8.3517\n",
      "Episode 488/500, Highest Score: 130, Episode Score: 52, Episode Reward: 2.7000, Episode Epsilon: 0.0866, Episode Loss: 0.0119, Mean Score: 56.2520, Mean Reward 8.3402\n",
      "Episode 489/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.2000, Episode Epsilon: 0.0862, Episode Loss: 0.2430, Mean Score: 56.2413, Mean Reward 8.3317\n",
      "Episode 490/500, Highest Score: 130, Episode Score: 64, Episode Reward: 5.2000, Episode Epsilon: 0.0858, Episode Loss: 0.1425, Mean Score: 56.2571, Mean Reward 8.3253\n",
      "Model saved after episode 490\n",
      "Episode 491/500, Highest Score: 130, Episode Score: 58, Episode Reward: 3.8000, Episode Epsilon: 0.0853, Episode Loss: 180.5707, Mean Score: 56.2607, Mean Reward 8.3161\n",
      "Episode 492/500, Highest Score: 130, Episode Score: 80, Episode Reward: 11.7000, Episode Epsilon: 0.0849, Episode Loss: 0.2657, Mean Score: 56.3089, Mean Reward 8.3230\n",
      "Episode 493/500, Highest Score: 130, Episode Score: 52, Episode Reward: 4.2000, Episode Epsilon: 0.0845, Episode Loss: 0.0821, Mean Score: 56.3002, Mean Reward 8.3146\n",
      "Episode 494/500, Highest Score: 130, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.0841, Episode Loss: 1.3346, Mean Score: 56.2895, Mean Reward 8.3059\n",
      "Episode 495/500, Highest Score: 130, Episode Score: 81, Episode Reward: 11.5000, Episode Epsilon: 0.0836, Episode Loss: 16.9875, Mean Score: 56.3394, Mean Reward 8.3123\n",
      "Episode 496/500, Highest Score: 130, Episode Score: 63, Episode Reward: 14.4000, Episode Epsilon: 0.0832, Episode Loss: 204.4726, Mean Score: 56.3528, Mean Reward 8.3246\n",
      "Episode 497/500, Highest Score: 130, Episode Score: 52, Episode Reward: 5.2000, Episode Epsilon: 0.0828, Episode Loss: 10.8200, Mean Score: 56.3441, Mean Reward 8.3183\n",
      "Episode 498/500, Highest Score: 130, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.0824, Episode Loss: 324.4627, Mean Score: 56.3353, Mean Reward 8.3082\n",
      "Episode 499/500, Highest Score: 130, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.0820, Episode Loss: 606.8536, Mean Score: 56.3267, Mean Reward 8.2990\n",
      "Episode 500/500, Highest Score: 130, Episode Score: 54, Episode Reward: 3.7000, Episode Epsilon: 0.0816, Episode Loss: 5.4795, Mean Score: 56.3220, Mean Reward 8.2898\n",
      "Model saved after episode 500\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05144910-6083-4497-9db9-3f36dd8d2c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8afd46-99e3-41a9-82a2-49072b4e5db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85b3b016-4d07-4470-b19d-f68954e8cd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(agent, env, episodes, model_path, save_interval=50, log_to_wandb=False, older_model=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='dino_rl_agent', name='test_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    if older_model:\n",
    "        agent.model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        agent.load_model(model_path, for_training=False)\n",
    "        \n",
    "    \n",
    "    # Set exploration rate (epsilon) to 0 to only choose actions based on the model's predictions (exploit its knowledge)\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafc388-89d9-42a8-b9b1-ac8557b7d92f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b3054a3-6522-48a1-aca0-f58a9eb64637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models\\dino_dqn_episode_400.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6ebd52c-a8cd-46e9-9e29-7b5dbb66e9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28844155-561c-494c-bccd-685b6dde5758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:somauk7s) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▃▅▆█</td></tr><tr><td>episode_epsilon</td><td>▁▁▁▁▁</td></tr><tr><td>episode_reward</td><td>█▁▁▁▁</td></tr><tr><td>episode_score</td><td>██▁▁█</td></tr><tr><td>highest_score</td><td>▁▁▁▁▁</td></tr><tr><td>mean_current_score</td><td>██▃▁▂</td></tr><tr><td>mean_reward</td><td>█▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0</td></tr><tr><td>episode_reward</td><td>4.4</td></tr><tr><td>episode_score</td><td>52</td></tr><tr><td>highest_score</td><td>52</td></tr><tr><td>mean_current_score</td><td>51.6</td></tr><tr><td>mean_reward</td><td>26.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/bidmalvi/dino_rl_agent/runs/somauk7s' target=\"_blank\">https://wandb.ai/bidmalvi/dino_rl_agent/runs/somauk7s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_001925-somauk7s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:somauk7s). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_002124-0h3ysuwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/dino_rl_agent/runs/0h3ysuwv' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/bidmalvi/dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/dino_rl_agent/runs/0h3ysuwv' target=\"_blank\">https://wandb.ai/bidmalvi/dino_rl_agent/runs/0h3ysuwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 323, Episode Score: 323, Episode Reward: 600.1000, Episode Epsilon: 0.0000, Mean Score: 323.0000, Mean Reward 600.1000\n",
      "Episode 2/5, Highest Score: 323, Episode Score: 270, Episode Reward: 41.7000, Episode Epsilon: 0.0000, Mean Score: 296.5000, Mean Reward 320.9000\n",
      "Episode 3/5, Highest Score: 323, Episode Score: 254, Episode Reward: 44.3000, Episode Epsilon: 0.0000, Mean Score: 282.3333, Mean Reward 228.7000\n",
      "Episode 4/5, Highest Score: 323, Episode Score: 52, Episode Reward: 3.7000, Episode Epsilon: 0.0000, Mean Score: 224.7500, Mean Reward 172.4500\n",
      "Episode 5/5, Highest Score: 323, Episode Score: 287, Episode Reward: 49.9000, Episode Epsilon: 0.0000, Mean Score: 237.2000, Mean Reward 147.9400\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b0dc1-46c2-402e-b877-f1d5cc199b4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e422a356-b6b1-4f42-95e3-506d6752be7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"best_trained_models\\episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fbfb8b5-b0b5-45ce-bd31-a5dc29db5212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bbd7344-6506-40b2-b00d-e6f0a9691bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 526, Episode Score: 526, Episode Reward: 828.8000, Episode Epsilon: 0.0000, Mean Score: 526.0000, Mean Reward 828.8000\n",
      "Episode 2/5, Highest Score: 1294, Episode Score: 1294, Episode Reward: 1063.1000, Episode Epsilon: 0.0000, Mean Score: 910.0000, Mean Reward 945.9500\n",
      "Episode 3/5, Highest Score: 1618, Episode Score: 1618, Episode Reward: 602.8000, Episode Epsilon: 0.0000, Mean Score: 1146.0000, Mean Reward 831.5667\n",
      "Episode 4/5, Highest Score: 1618, Episode Score: 576, Episode Reward: 82.6000, Episode Epsilon: 0.0000, Mean Score: 1003.5000, Mean Reward 644.3250\n",
      "Episode 5/5, Highest Score: 1618, Episode Score: 147, Episode Reward: 24.9000, Episode Epsilon: 0.0000, Mean Score: 832.2000, Mean Reward 520.4400\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False, older_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7626983-195c-4243-85b6-1f0e7e3af761",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9aaac4-3718-4c0d-832e-619ade51568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "EPISODE_NUMS = 1000\n",
    "\n",
    "for episode in range(EPISODE_NUMS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    agent.replay()\n",
    "    print(f\"Episode {episode + 1}/{EPISODE_NUMS}, Reward: {episode_reward}, Current Score: {info['current_score']}, High Score: {info['high_score']}\")\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        model_file = os.path.join(OUTPUT_DIR, f\"episode_{episode + 1}.pth\")\n",
    "        agent.save_model(model_file)\n",
    "        print(f\"Model saved after episode {episode + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547af20d-98e6-4fd5-9c97-e0cde6076c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "# agent = DinoDQNAgent(env)\n",
    "agent.load_model(\"model_output\\dino\\episode_100.pth\")\n",
    "# Set agent's exploration rate (epsilon) to zero, so that it only chooses actions based on the model's predictions\n",
    "agent.epsilon = 0\n",
    "\n",
    "EPISODE_NUMS = 100\n",
    "\n",
    "# Test loop\n",
    "for episode in range(EPISODE_NUMS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode + 1}/{EPISODE_NUMS}, Total episode reward: {episode_reward}, Final score: {info['current_score']}, Highest score achieved: {info['high_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e43a6c-ab99-4441-82eb-c02a43feefa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
