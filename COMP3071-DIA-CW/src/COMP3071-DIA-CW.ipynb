{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dec060-bf08-4a6e-a1d4-1839b4730f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom Dino Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94c8c1-c5e5-4b8f-8c22-242647e9b880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64147db4-ea80-49a1-b50e-1fce4f97af98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Environment Components\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "# Selenium for automatically loading and play the game\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02f78b-c77c-4ac9-9a35-7ab41ed4e123",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DinoEnvironment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0937a3c-edcf-487a-af0d-a178a1f5b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dino Game Environment\n",
    "class DinoEnvironment(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "\n",
    "        self.driver = self._create_driver()\n",
    "\n",
    "        # Setup spaces\n",
    "        low_values = np.array(\n",
    "            [0, -1, -1, -1, -1, 0, 0, 6, -1, -1, -1, -1, -1, -1], dtype=np.float32)  # Initial speed is 6, while max speed is 13\n",
    "        high_values = np.array(\n",
    "            [150, 50, 50, 50, 60, 1, 1, 13, 600, 3, 600, 150, 50, 50], dtype=np.float32)  # Canvas dimensions are 600x150\n",
    "        self.observation_space = Box(\n",
    "            low=low_values, high=high_values, shape=(14,), dtype=np.float32)\n",
    "\n",
    "        # Start jumping, Start ducking, Stop ducking, Do nothing - Ducking has been divided into two actions because the agent should also learn the correct ducking duration\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.actions_map = [\n",
    "            (Keys.ARROW_UP, \"key_down\"),  # Start jumping\n",
    "            (Keys.ARROW_DOWN, \"key_down\"),  # Start ducking\n",
    "            (Keys.ARROW_DOWN, \"key_up\"),  # Stop ducking\n",
    "            (Keys.ARROW_RIGHT, \"key_down\")  # Do nothing\n",
    "        ]\n",
    "\n",
    "        # Keep track of number of obstacles the agent has passed\n",
    "        self.passed_obstacles = 0\n",
    "\n",
    "    # Create and return an instance of the Chrome Driver\n",
    "    def _create_driver(self):\n",
    "\n",
    "        # Set options for the WebDriver\n",
    "        options = Options()\n",
    "\n",
    "        # Turn off logging to keep terminal clean\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "        # Keep the browser running after the code finishes executing\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        # Create a Service instance for running the ChromeDriver executable\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "        # Create an instance of the Chrome WebDriver with the specified service and options - The driver object can be used to automate interactions with the Chrome browser\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Maximize the Chrome window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # Encode the obstacle type as an integer\n",
    "    def _encode_obstacle_type(self, obstacle_type):\n",
    "        if obstacle_type == 'CACTUS_SMALL':\n",
    "            return 0\n",
    "        elif obstacle_type == 'CACTUS_LARGE':\n",
    "            return 1\n",
    "        elif obstacle_type == 'PTERODACTYL':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown obstacle type: {obstacle_type}\")\n",
    "\n",
    "    # Get obstacles that are currently on the screen\n",
    "    def _get_obstacles(self):\n",
    "        obstacles = self.driver.execute_script(\n",
    "            \"return Runner.instance_.horizon.obstacles\")\n",
    "        obstacle_info = []\n",
    "        for obstacle in obstacles:\n",
    "            obstacle_type = obstacle['typeConfig']['type']\n",
    "            # Encode the obstacle type as an integer\n",
    "            encoded_obstacle_type = self._encode_obstacle_type(obstacle_type)\n",
    "            obstacle_x = obstacle['xPos']\n",
    "            obstacle_y = obstacle['yPos']\n",
    "            obstacle_width = obstacle['typeConfig']['width']\n",
    "            obstacle_height = obstacle['typeConfig']['height']\n",
    "            obstacle_info.append(\n",
    "                (encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height))\n",
    "        return obstacle_info\n",
    "\n",
    "    # Get Trex's state (Jumping, Ducking or Running/Do nothing)\n",
    "    def _get_trex_info(self):\n",
    "        trex = self.driver.execute_script(\"return Runner.instance_.tRex\")\n",
    "        # xpos remains the same throughout the game - don't need it\n",
    "        trex_y = trex['yPos']\n",
    "        trex_height = trex['config']['HEIGHT']\n",
    "        trex_width = trex['config']['WIDTH']\n",
    "        trex_duck_height = trex['config']['HEIGHT_DUCK']\n",
    "        trex_duck_width = trex['config']['WIDTH_DUCK']\n",
    "        trex_is_jumping = trex['jumping']\n",
    "        trex_is_ducking = trex['ducking']\n",
    "        return trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking\n",
    "\n",
    "    # Get current game speed\n",
    "    def _get_game_speed(self):\n",
    "        game_speed = self.driver.execute_script(\n",
    "            \"return Runner.instance_.currentSpeed\")\n",
    "        return game_speed\n",
    "\n",
    "    # Get the distance between the Trex and the next obstacle\n",
    "    def _get_distance_to_next_obstacle(self):\n",
    "        trex_x = self.driver.execute_script(\n",
    "            \"return Runner.instance_.tRex.xPos\")  # xpos of trex\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            next_obstacle = obstacles[0]\n",
    "            obstacle_x = next_obstacle[1]  # xpos of next obstacle\n",
    "            distance_to_next_obstacle = obstacle_x - trex_x\n",
    "        else:\n",
    "            distance_to_next_obstacle = None\n",
    "        return distance_to_next_obstacle\n",
    "\n",
    "    # Check if the agent has passed an obstacle\n",
    "    def _passed_obstacle(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            # next_obstacle: [encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height]\n",
    "            next_obstacle = obstacles[0]\n",
    "            trex_x = self.driver.execute_script(\n",
    "                \"return Runner.instance_.tRex.xPos\")\n",
    "            obstacle_x = next_obstacle[1]  # Next obstacles xpos\n",
    "            obstacle_width = next_obstacle[3]  # Next obstacles width\n",
    "            return obstacle_x + obstacle_width < trex_x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Get and return the score for the last game played\n",
    "    def _get_current_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.digits\")))\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Get and return the high score for all games played in current browser session\n",
    "    def _get_high_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.highScore.slice(-5)\")))  # MaxScore=99999, MaxScoreUnits=5\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Capture screenshot of current game state and return the image captured for rendering\n",
    "    def _get_image(self):\n",
    "        # Capture a screenshot of the game canvas as a data URL - string that represents the image in base64-encoded format\n",
    "        data_url = self.driver.execute_script(\n",
    "            \"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "\n",
    "        # Remove the leading text from the data URL using string slicing and decode the remaining base64-encoded data\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        image_data = base64.b64decode(data_url[len(LEADING_TEXT):])\n",
    "\n",
    "        # Convert the binary data in 'image_data' to a 1D NumPy array\n",
    "        image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "\n",
    "        # Decode the image data and create an OpenCV image object - OpenCV Image Shape format (H, W, C) ( rows, columns, and channels )\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Load and Reset the game environment\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Navigate to the Chrome Dino website\n",
    "            self.driver.get(\"chrome://dino/\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            # Ignore \"ERR_INTERNET_DISCONNECTED\" error thrown because this game is available offline\n",
    "            if \"ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                pass  # Ignore the exception.\n",
    "            else:\n",
    "                raise e  # Handle other WebDriverExceptions\n",
    "\n",
    "        # Avoid errors that can arise due to the 'runner-canvas' element not being present - Using WebDriverWait and EC together ensures that the code does not proceed until the required element is present\n",
    "        timeout = 10\n",
    "        WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
    "\n",
    "        # Start game\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the current state of the game and return it as the observation\n",
    "    def get_observation(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking = self._get_trex_info()\n",
    "        game_speed = self._get_game_speed()\n",
    "        distance_to_next_obstacle = self._get_distance_to_next_obstacle()\n",
    "\n",
    "        state = (\n",
    "            trex_y,\n",
    "            trex_height,\n",
    "            trex_width,\n",
    "            trex_duck_height,\n",
    "            trex_duck_width,\n",
    "            trex_is_jumping,\n",
    "            trex_is_ducking,\n",
    "            game_speed,\n",
    "            distance_to_next_obstacle,\n",
    "            # Unpack the tuple of the first obstacle\n",
    "            *(obstacles[0] if obstacles else (None, None, None, None, None))\n",
    "        )\n",
    "\n",
    "        # Set dtype for state to float32 for consistency and compatibility with the RL algorithm\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        # Replace NaN values with -1\n",
    "        state[np.isnan(state)] = -1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Check if the game is over and return True or False\n",
    "    def is_game_over(self):\n",
    "        # Done if either Trex crashed into an obstacle or reached max score which is 99999\n",
    "        # Check if Trex crashed\n",
    "        crashed = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "        # Get the maximum score from the game\n",
    "        max_score = self.driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.maxScore\")\n",
    "        current_score = self._get_current_score()\n",
    "\n",
    "        return crashed or (current_score >= max_score)\n",
    "\n",
    "    # Calculate and return the reward for the current state of the game\n",
    "    def get_reward(self, obs, done, info):\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        current_score = info['current_score']\n",
    "        high_score = info['high_score']\n",
    "        \n",
    "        if done:\n",
    "            # Penalize for crashing into an obstacle\n",
    "            reward -= 10\n",
    "\n",
    "            if current_score > high_score:\n",
    "                # Bonus reward for surpassing the high score\n",
    "                reward += 10\n",
    "        else:\n",
    "\n",
    "            # Reward for staying alive\n",
    "            reward += 1\n",
    "\n",
    "            trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking, game_speed, distance_to_next_obstacle, obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height = obs\n",
    "\n",
    "            # Penalize unnecessary jumps and ducks when there are no obstacles\n",
    "            if obstacle_type == -1:\n",
    "                if trex_is_jumping:\n",
    "                    # Penalize for jumping when there are no obstacles\n",
    "                    reward -= 0.5\n",
    "                if trex_is_ducking:\n",
    "                    # Penalize for ducking when there are no obstacles\n",
    "                    reward -= 0.5\n",
    "\n",
    "            if self._passed_obstacle():\n",
    "                # Reward for passing an obstacle\n",
    "                reward += 1\n",
    "                self.passed_obstacles += 1\n",
    "            \n",
    "            if current_score > high_score:\n",
    "                # Small reward for every step the current score surpasses high score \n",
    "                reward += 0.1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Take a step in the game environment based on the given action\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take action\n",
    "        # Get key and action mapping\n",
    "        key, action_type = self.actions_map[action]\n",
    "\n",
    "        # Create a new ActionChains object\n",
    "        action_chains = ActionChains(self.driver)\n",
    "\n",
    "        # Perform the key press action\n",
    "        if action_type == \"key_down\":\n",
    "            action_chains.key_down(key).perform()\n",
    "        # Perform the key release action\n",
    "        elif action_type == \"key_up\":\n",
    "            action_chains.key_up(key).perform()\n",
    "\n",
    "        # Get next observation\n",
    "        obs = self.get_observation()\n",
    "\n",
    "        # Check whether game is over\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        info = {\n",
    "            'current_score': self._get_current_score(),\n",
    "            'high_score': self._get_high_score()\n",
    "        }\n",
    "\n",
    "        # Get reward\n",
    "        reward = self.get_reward(obs, done, info)\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Visualise the game\n",
    "    def render(self, mode: str = 'human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb-array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            cv2.imshow('Dino Game', img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    # Close the game environment and the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42599e-123f-4932-bbe4-8e97663b4222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the Custom Game Environment\n",
    "\n",
    "This section is for testing the Game Environment to ensure it is defined correctly before using it with the Agent for RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bc403f4-7400-4656-ba2a-bd79673640d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper class to format and print observations properly\n",
    "def print_formatted_obs(observations):\n",
    "    obs_titles = [\"trex_y\", \"trex_h\",\"trex_w\",\"tr_duck_h\",\"tr_duck_w\",\"trex_jump\", \"trex_duck\", \"game_speed\", \"obst_dist\", \"obst_type\", \"obst_x\", \"obst_y\", \"obst_w\", \"obst_h\"]\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(observations, columns=obs_titles)\n",
    "\n",
    "    # Set the pandas display options for better readability (optional)\n",
    "    pd.set_option(\"display.width\", 140)\n",
    "    # pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfb1a4d7-bf2a-492e-bbea-96ff3ecda78c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = DinoEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc8c9b8-87a7-407e-bea4-bdbf7585fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0. -1. -1. -1. -1.  0.  0.  6. -1. -1. -1. -1. -1. -1.], [150.  50.  50.  50.  60.   1.   1.  13. 600.   3. 600. 150.  50.  50.], (14,), float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82fba9fa-0135-4ca0-ac02-e2620fe05f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd240adb-926c-4f23-b692-1063bf2942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fedc098e-9ef4-4fde-9ec2-1ed964c28bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908c10c2-7cbb-40e1-9289-aaa14a95c7b9",
   "metadata": {},
   "source": [
    "**Note:** Render function works better if using `.py` python files instead of the `.ipynb` notebook to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75ab681d-89a2-41bc-9491-ab58d48e497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trex_y  trex_h  trex_w  tr_duck_h  tr_duck_w  trex_jump  trex_duck  game_speed  obst_dist  obst_type  obst_x  obst_y  obst_w  obst_h\n",
      "0      68.0    47.0    44.0       25.0       59.0        1.0        0.0       6.017       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "1      50.0    47.0    44.0       25.0       59.0        1.0        0.0       6.029       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "2      35.0    47.0    44.0       25.0       59.0        1.0        0.0       6.039       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "3      24.0    47.0    44.0       25.0       59.0        1.0        0.0       6.049       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "4      33.0    47.0    44.0       25.0       59.0        1.0        0.0       6.059       -1.0       -1.0    -1.0    -1.0    -1.0    -1.0\n",
      "..      ...     ...     ...        ...        ...        ...        ...         ...        ...        ...     ...     ...     ...     ...\n",
      "138    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.511       99.0        0.0   104.0   105.0    17.0    35.0\n",
      "139    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.525       81.0        0.0    84.0   105.0    17.0    35.0\n",
      "140    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.537       64.0        0.0    67.0   105.0    17.0    35.0\n",
      "141    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.547       44.0        0.0    52.0   105.0    17.0    35.0\n",
      "142    93.0    47.0    44.0       25.0       59.0        1.0        0.0       7.557       33.0        0.0    33.0   105.0    17.0    35.0\n",
      "\n",
      "[143 rows x 14 columns]\n",
      "Episode: 0, Total Reward: -63.29999999999986, , Current Score: 51, High Score: 51\n"
     ]
    }
   ],
   "source": [
    "# Test loop - Play 1 game\n",
    "env = DinoEnvironment()\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = []\n",
    "    # images = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(obs)\n",
    "        all_observations.append(obs)  # Print obs formatted nicely in a table\n",
    "        total_reward += reward\n",
    "\n",
    "        # env.render(mode='human')\n",
    "        # img = env.render(mode='rgb-array')\n",
    "        # images.append(img) # Can use some image library to create a gif using collected images\n",
    "\n",
    "    print_formatted_obs(all_observations)\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, , Current Score: {info['current_score']}, High Score: {info['high_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf346759-391a-4a48-81c2-e5943ed23ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DQN Dino Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc910af-22e2-45ed-8022-ace974d90ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4efbeb-851d-4a23-b8c6-0f33f2a27645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0862b2-3e86-4c79-ad98-078ce2e5ddcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoDQNAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc2b9a48-e081-4727-b071-807a90f131e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoDQNAgent():\n",
    "    def __init__(self, env,\n",
    "                 gamma=0.95,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.990,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=32,\n",
    "                 memory_size=100000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]  # 14\n",
    "        self.action_size = env.action_space.n  # 4\n",
    "        self.hidden_sizes = [64, 128]  # number of hidden neurons for the model\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma  # discounting factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min  # min exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # exploration decay per step\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the DQN model architecture - This model will be used to approximate the Q-values of the agent's actions given a state.\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], self.action_size)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Store agents experiences as a tuple\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Determine which action to take given a state\n",
    "    def act(self, state):\n",
    "        # Explore randomly or exploit given the current epsilon value\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            return action\n",
    "\n",
    "    # Update the DQN model using a batch of experiences sampled from the memory\n",
    "    def replay(self):\n",
    "        # Check if the number of experiences (state, action, reward, next_state, done) in the memory is less than the batch size\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # Don't do anything since there's not enough data to create a minibatch for training\n",
    "            return\n",
    "\n",
    "        # Create minibatch from a random sample of experiences from the memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Calculate the expected Q-value for the current state-action pair (q_target)\n",
    "            # If done, - Game has ended, don't need to make predictions about future rewards\n",
    "            q_target = reward\n",
    "            if not done:\n",
    "                # Calculate the Q-values for the next state using the DQN model, i.e., estimate future reward\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "                q_values_next = self.model(next_state)\n",
    "                # Update the target value by adding the discounted maximum Q-value of the next state to the current reward\n",
    "                q_target = reward + self.gamma * \\\n",
    "                    torch.max(q_values_next).item()\n",
    "\n",
    "            # Calculate the Q-values for the current state using the DQN model\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "\n",
    "            # Update/Map the expected Q-value of the chosen action with the calculated target value\n",
    "            q_values_expected = q_values.clone().detach()\n",
    "\n",
    "            q_values_expected[action] = q_target\n",
    "\n",
    "            # Note: q_values_expected is the ground truth for the action that the agent took in the current state vs q_values is the models prediction of what should happen\n",
    "\n",
    "            # Reset the gradients of the optimizer before performing backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss using the Mean Squared Error (MSE) between the current Q-values and the expected Q-values\n",
    "            loss = self.loss_fn(q_values, q_values_expected)\n",
    "\n",
    "            # Perform backpropagation to calculate the gradients of the model's parameters with respect to the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's parameters using the calculated gradients and the optimizer's learning rate\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decrease episolon over time to reduce exploration and increase exploitation of the models learnt knowledge\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "    # Save the current state of the DQN model and optimizer to a file.\n",
    "    def save_model(self, model_name, model_output_dir, log_to_wandb):\n",
    "        # Create a dictionary to store the state of the model, optimizer and any other additional information\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            model_output_dir, model_name)\n",
    "\n",
    "        # Save the state dictionary to a file\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "        if log_to_wandb:\n",
    "            # Save model as a wandb artifact\n",
    "            artifact = wandb.Artifact(model_name, type='model')\n",
    "            artifact.add_file(save_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "    # Load the DQN model and optimizer state from a file.\n",
    "    def load_model(self, file_path, older_model, for_training):\n",
    "\n",
    "        if older_model:\n",
    "            self.model.load_state_dict(torch.load(file_path))\n",
    "        else:\n",
    "            # Load the state dictionary from the file using the torch.load() function\n",
    "            state = torch.load(file_path)\n",
    "\n",
    "            # Restore the state of the model and optimizer\n",
    "            self.model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "            # Set for_training to true if using the model to continue training from a previously saved state\n",
    "            if for_training:\n",
    "                self.optimizer.load_state_dict(state['optimizer_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1648970-0a2b-4fcd-b73f-e2c59c443464",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train and Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742893d-09ed-4817-bc9e-2381bba3a6b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c883a5b-c4ac-4d5f-a20c-2817711396b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392289b8-6d05-438e-b037-60f46efe93b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578dbe60-f461-4c8a-8303-3fcd1fd4ba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7d966df0-fd81-489d-b867-a35a62148678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, env, episodes, model_output_dir, save_interval=10, log_to_wandb=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_rl_agent', name='train_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Remember agents experience after every step\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Train/Update the model every episode\n",
    "        loss = agent.replay()\n",
    "        episode_loss.append(loss)\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_episode_loss = sum(episode_loss) / len(episode_loss)\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Episode Loss: {loss:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"episode_loss\": loss,\n",
    "                \"mean_loss\": mean_episode_loss,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "\n",
    "        # Save the model every save_interval episodes\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            model_name = f\"dino_dqn_episode_{episode + 1}.pth\"\n",
    "            agent.save_model(model_name, model_output_dir, log_to_wandb)\n",
    "            print(f\"Model saved after episode {episode + 1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de6bef-757a-49f3-bdb9-04dc2a6130db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16ad5ada-035b-493b-8ea7-23a567d7243b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e6cf8cd-45f6-408a-af41-fc49a96283d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18f92077-d7f1-4f8b-bb68-47a8c0bbaf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4op9rvzt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/4op9rvzt' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/4op9rvzt</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_191550-4op9rvzt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4op9rvzt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_191734-lybyf383</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/lybyf383' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/lybyf383' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/lybyf383</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Highest Score: 52, Episode Score: 52, Episode Reward: 97.7000, Episode Epsilon: 0.9900, Episode Loss: 1.6913, Mean Score: 52.0000, Mean Reward 97.7000\n",
      "Episode 2/200, Highest Score: 76, Episode Score: 76, Episode Reward: 237.2000, Episode Epsilon: 0.9801, Episode Loss: 231.9722, Mean Score: 64.0000, Mean Reward 167.4500\n",
      "Episode 3/200, Highest Score: 76, Episode Score: 52, Episode Reward: 83.5000, Episode Epsilon: 0.9703, Episode Loss: 2.0526, Mean Score: 60.0000, Mean Reward 139.4667\n",
      "Episode 4/200, Highest Score: 80, Episode Score: 80, Episode Reward: 202.3000, Episode Epsilon: 0.9606, Episode Loss: 1.8664, Mean Score: 65.0000, Mean Reward 155.1750\n",
      "Episode 5/200, Highest Score: 80, Episode Score: 71, Episode Reward: 343.0000, Episode Epsilon: 0.9510, Episode Loss: 2.4691, Mean Score: 66.2000, Mean Reward 192.7400\n",
      "Episode 6/200, Highest Score: 80, Episode Score: 52, Episode Reward: 73.0000, Episode Epsilon: 0.9415, Episode Loss: 0.2437, Mean Score: 63.8333, Mean Reward 172.7833\n",
      "Episode 7/200, Highest Score: 80, Episode Score: 52, Episode Reward: 92.5000, Episode Epsilon: 0.9321, Episode Loss: 13.4093, Mean Score: 62.1429, Mean Reward 161.3143\n",
      "Episode 8/200, Highest Score: 80, Episode Score: 51, Episode Reward: 78.5000, Episode Epsilon: 0.9227, Episode Loss: 0.0422, Mean Score: 60.7500, Mean Reward 150.9625\n",
      "Episode 9/200, Highest Score: 80, Episode Score: 52, Episode Reward: 89.0000, Episode Epsilon: 0.9135, Episode Loss: 0.0785, Mean Score: 59.7778, Mean Reward 144.0778\n",
      "Episode 10/200, Highest Score: 80, Episode Score: 51, Episode Reward: 101.0000, Episode Epsilon: 0.9044, Episode Loss: 0.0011, Mean Score: 58.9000, Mean Reward 139.7700\n",
      "Model saved after episode 10\n",
      "Episode 11/200, Highest Score: 80, Episode Score: 53, Episode Reward: 100.0000, Episode Epsilon: 0.8953, Episode Loss: 0.2065, Mean Score: 58.3636, Mean Reward 136.1545\n",
      "Episode 12/200, Highest Score: 80, Episode Score: 52, Episode Reward: 68.0000, Episode Epsilon: 0.8864, Episode Loss: 0.1159, Mean Score: 57.8333, Mean Reward 130.4750\n",
      "Episode 13/200, Highest Score: 80, Episode Score: 53, Episode Reward: 318.0000, Episode Epsilon: 0.8775, Episode Loss: 9.2321, Mean Score: 57.4615, Mean Reward 144.9000\n",
      "Episode 14/200, Highest Score: 80, Episode Score: 52, Episode Reward: 86.0000, Episode Epsilon: 0.8687, Episode Loss: 0.6089, Mean Score: 57.0714, Mean Reward 140.6929\n",
      "Episode 15/200, Highest Score: 80, Episode Score: 52, Episode Reward: 74.5000, Episode Epsilon: 0.8601, Episode Loss: 73.7542, Mean Score: 56.7333, Mean Reward 136.2800\n",
      "Episode 16/200, Highest Score: 80, Episode Score: 51, Episode Reward: 85.5000, Episode Epsilon: 0.8515, Episode Loss: 789.4749, Mean Score: 56.3750, Mean Reward 133.1062\n",
      "Episode 17/200, Highest Score: 80, Episode Score: 52, Episode Reward: 95.5000, Episode Epsilon: 0.8429, Episode Loss: 639.2774, Mean Score: 56.1176, Mean Reward 130.8941\n",
      "Episode 18/200, Highest Score: 80, Episode Score: 51, Episode Reward: 87.5000, Episode Epsilon: 0.8345, Episode Loss: 3.1755, Mean Score: 55.8333, Mean Reward 128.4833\n",
      "Episode 19/200, Highest Score: 80, Episode Score: 52, Episode Reward: 79.5000, Episode Epsilon: 0.8262, Episode Loss: 83.7428, Mean Score: 55.6316, Mean Reward 125.9053\n",
      "Episode 20/200, Highest Score: 80, Episode Score: 52, Episode Reward: 97.5000, Episode Epsilon: 0.8179, Episode Loss: 239.6081, Mean Score: 55.4500, Mean Reward 124.4850\n",
      "Model saved after episode 20\n",
      "Episode 21/200, Highest Score: 80, Episode Score: 63, Episode Reward: 106.5000, Episode Epsilon: 0.8097, Episode Loss: 286.6970, Mean Score: 55.8095, Mean Reward 123.6286\n",
      "Episode 22/200, Highest Score: 80, Episode Score: 52, Episode Reward: 63.5000, Episode Epsilon: 0.8016, Episode Loss: 1.2844, Mean Score: 55.6364, Mean Reward 120.8955\n",
      "Episode 23/200, Highest Score: 80, Episode Score: 52, Episode Reward: 57.0000, Episode Epsilon: 0.7936, Episode Loss: 18.1107, Mean Score: 55.4783, Mean Reward 118.1174\n",
      "Episode 24/200, Highest Score: 80, Episode Score: 59, Episode Reward: 147.0000, Episode Epsilon: 0.7857, Episode Loss: 17.8246, Mean Score: 55.6250, Mean Reward 119.3208\n",
      "Episode 25/200, Highest Score: 80, Episode Score: 57, Episode Reward: 103.0000, Episode Epsilon: 0.7778, Episode Loss: 24.1698, Mean Score: 55.6800, Mean Reward 118.6680\n",
      "Episode 26/200, Highest Score: 80, Episode Score: 61, Episode Reward: 222.0000, Episode Epsilon: 0.7700, Episode Loss: 2.5215, Mean Score: 55.8846, Mean Reward 122.6423\n",
      "Episode 27/200, Highest Score: 80, Episode Score: 52, Episode Reward: 71.0000, Episode Epsilon: 0.7623, Episode Loss: 34.1234, Mean Score: 55.7407, Mean Reward 120.7296\n",
      "Episode 28/200, Highest Score: 80, Episode Score: 52, Episode Reward: 86.0000, Episode Epsilon: 0.7547, Episode Loss: 0.1450, Mean Score: 55.6071, Mean Reward 119.4893\n",
      "Episode 29/200, Highest Score: 80, Episode Score: 52, Episode Reward: 84.5000, Episode Epsilon: 0.7472, Episode Loss: 0.0782, Mean Score: 55.4828, Mean Reward 118.2828\n",
      "Episode 30/200, Highest Score: 80, Episode Score: 59, Episode Reward: 86.0000, Episode Epsilon: 0.7397, Episode Loss: 0.8270, Mean Score: 55.6000, Mean Reward 117.2067\n",
      "Model saved after episode 30\n",
      "Episode 31/200, Highest Score: 80, Episode Score: 52, Episode Reward: 84.5000, Episode Epsilon: 0.7323, Episode Loss: 0.9116, Mean Score: 55.4839, Mean Reward 116.1516\n",
      "Episode 32/200, Highest Score: 80, Episode Score: 52, Episode Reward: 64.5000, Episode Epsilon: 0.7250, Episode Loss: 0.2513, Mean Score: 55.3750, Mean Reward 114.5375\n",
      "Episode 33/200, Highest Score: 80, Episode Score: 53, Episode Reward: 84.0000, Episode Epsilon: 0.7177, Episode Loss: 4.8415, Mean Score: 55.3030, Mean Reward 113.6121\n",
      "Episode 34/200, Highest Score: 80, Episode Score: 52, Episode Reward: 63.5000, Episode Epsilon: 0.7106, Episode Loss: 0.0426, Mean Score: 55.2059, Mean Reward 112.1382\n",
      "Episode 35/200, Highest Score: 80, Episode Score: 78, Episode Reward: 290.5000, Episode Epsilon: 0.7034, Episode Loss: 8.1337, Mean Score: 55.8571, Mean Reward 117.2343\n",
      "Episode 36/200, Highest Score: 80, Episode Score: 59, Episode Reward: 92.0000, Episode Epsilon: 0.6964, Episode Loss: 1.1894, Mean Score: 55.9444, Mean Reward 116.5333\n",
      "Episode 37/200, Highest Score: 80, Episode Score: 52, Episode Reward: 72.5000, Episode Epsilon: 0.6894, Episode Loss: 1.7759, Mean Score: 55.8378, Mean Reward 115.3432\n",
      "Episode 38/200, Highest Score: 80, Episode Score: 51, Episode Reward: 63.5000, Episode Epsilon: 0.6826, Episode Loss: 1.2988, Mean Score: 55.7105, Mean Reward 113.9789\n",
      "Episode 39/200, Highest Score: 80, Episode Score: 72, Episode Reward: 114.5000, Episode Epsilon: 0.6757, Episode Loss: 0.1634, Mean Score: 56.1282, Mean Reward 113.9923\n",
      "Episode 40/200, Highest Score: 80, Episode Score: 52, Episode Reward: 66.0000, Episode Epsilon: 0.6690, Episode Loss: 0.2337, Mean Score: 56.0250, Mean Reward 112.7925\n",
      "Model saved after episode 40\n",
      "Episode 41/200, Highest Score: 80, Episode Score: 52, Episode Reward: 68.0000, Episode Epsilon: 0.6623, Episode Loss: 0.0007, Mean Score: 55.9268, Mean Reward 111.7000\n",
      "Episode 42/200, Highest Score: 80, Episode Score: 52, Episode Reward: 74.0000, Episode Epsilon: 0.6557, Episode Loss: 85.2857, Mean Score: 55.8333, Mean Reward 110.8024\n",
      "Episode 43/200, Highest Score: 80, Episode Score: 52, Episode Reward: 69.0000, Episode Epsilon: 0.6491, Episode Loss: 3.0781, Mean Score: 55.7442, Mean Reward 109.8302\n",
      "Episode 44/200, Highest Score: 80, Episode Score: 52, Episode Reward: 61.5000, Episode Epsilon: 0.6426, Episode Loss: 0.1061, Mean Score: 55.6591, Mean Reward 108.7318\n",
      "Episode 45/200, Highest Score: 80, Episode Score: 52, Episode Reward: 59.0000, Episode Epsilon: 0.6362, Episode Loss: 0.0993, Mean Score: 55.5778, Mean Reward 107.6267\n",
      "Episode 46/200, Highest Score: 80, Episode Score: 53, Episode Reward: 59.5000, Episode Epsilon: 0.6298, Episode Loss: 4.2620, Mean Score: 55.5217, Mean Reward 106.5804\n",
      "Episode 47/200, Highest Score: 80, Episode Score: 54, Episode Reward: 147.0000, Episode Epsilon: 0.6235, Episode Loss: 0.0069, Mean Score: 55.4894, Mean Reward 107.4404\n",
      "Episode 48/200, Highest Score: 80, Episode Score: 63, Episode Reward: 108.5000, Episode Epsilon: 0.6173, Episode Loss: 0.0339, Mean Score: 55.6458, Mean Reward 107.4625\n",
      "Episode 49/200, Highest Score: 80, Episode Score: 52, Episode Reward: 66.5000, Episode Epsilon: 0.6111, Episode Loss: 0.0733, Mean Score: 55.5714, Mean Reward 106.6265\n",
      "Episode 50/200, Highest Score: 80, Episode Score: 53, Episode Reward: 78.0000, Episode Epsilon: 0.6050, Episode Loss: 0.1882, Mean Score: 55.5200, Mean Reward 106.0540\n",
      "Model saved after episode 50\n",
      "Episode 51/200, Highest Score: 80, Episode Score: 52, Episode Reward: 80.5000, Episode Epsilon: 0.5990, Episode Loss: 0.0143, Mean Score: 55.4510, Mean Reward 105.5529\n",
      "Episode 52/200, Highest Score: 80, Episode Score: 52, Episode Reward: 66.5000, Episode Epsilon: 0.5930, Episode Loss: 0.0174, Mean Score: 55.3846, Mean Reward 104.8019\n",
      "Episode 53/200, Highest Score: 80, Episode Score: 52, Episode Reward: 50.5000, Episode Epsilon: 0.5870, Episode Loss: 13.7758, Mean Score: 55.3208, Mean Reward 103.7774\n",
      "Episode 54/200, Highest Score: 80, Episode Score: 52, Episode Reward: 72.5000, Episode Epsilon: 0.5812, Episode Loss: 1.9868, Mean Score: 55.2593, Mean Reward 103.1981\n",
      "Episode 55/200, Highest Score: 80, Episode Score: 52, Episode Reward: 50.5000, Episode Epsilon: 0.5754, Episode Loss: 0.0037, Mean Score: 55.2000, Mean Reward 102.2400\n",
      "Episode 56/200, Highest Score: 80, Episode Score: 52, Episode Reward: 62.0000, Episode Epsilon: 0.5696, Episode Loss: 0.7226, Mean Score: 55.1429, Mean Reward 101.5214\n",
      "Episode 57/200, Highest Score: 80, Episode Score: 52, Episode Reward: 79.5000, Episode Epsilon: 0.5639, Episode Loss: 3.4982, Mean Score: 55.0877, Mean Reward 101.1351\n",
      "Episode 58/200, Highest Score: 80, Episode Score: 52, Episode Reward: 73.5000, Episode Epsilon: 0.5583, Episode Loss: 0.0327, Mean Score: 55.0345, Mean Reward 100.6586\n",
      "Episode 59/200, Highest Score: 80, Episode Score: 52, Episode Reward: 66.5000, Episode Epsilon: 0.5527, Episode Loss: 0.0162, Mean Score: 54.9831, Mean Reward 100.0797\n",
      "Episode 60/200, Highest Score: 80, Episode Score: 53, Episode Reward: 98.5000, Episode Epsilon: 0.5472, Episode Loss: 0.0506, Mean Score: 54.9500, Mean Reward 100.0533\n",
      "Model saved after episode 60\n",
      "Episode 61/200, Highest Score: 80, Episode Score: 51, Episode Reward: 73.5000, Episode Epsilon: 0.5417, Episode Loss: 0.0287, Mean Score: 54.8852, Mean Reward 99.6180\n",
      "Episode 62/200, Highest Score: 80, Episode Score: 53, Episode Reward: 60.5000, Episode Epsilon: 0.5363, Episode Loss: 0.0531, Mean Score: 54.8548, Mean Reward 98.9871\n",
      "Episode 63/200, Highest Score: 80, Episode Score: 63, Episode Reward: 108.0000, Episode Epsilon: 0.5309, Episode Loss: 0.0513, Mean Score: 54.9841, Mean Reward 99.1302\n",
      "Episode 64/200, Highest Score: 80, Episode Score: 52, Episode Reward: 63.5000, Episode Epsilon: 0.5256, Episode Loss: 0.7610, Mean Score: 54.9375, Mean Reward 98.5734\n",
      "Episode 65/200, Highest Score: 80, Episode Score: 51, Episode Reward: 73.0000, Episode Epsilon: 0.5203, Episode Loss: 0.0034, Mean Score: 54.8769, Mean Reward 98.1800\n",
      "Episode 66/200, Highest Score: 80, Episode Score: 52, Episode Reward: 69.0000, Episode Epsilon: 0.5151, Episode Loss: 0.0140, Mean Score: 54.8333, Mean Reward 97.7379\n",
      "Episode 67/200, Highest Score: 80, Episode Score: 53, Episode Reward: 84.5000, Episode Epsilon: 0.5100, Episode Loss: 0.4962, Mean Score: 54.8060, Mean Reward 97.5403\n",
      "Episode 68/200, Highest Score: 80, Episode Score: 54, Episode Reward: 96.0000, Episode Epsilon: 0.5049, Episode Loss: 0.8978, Mean Score: 54.7941, Mean Reward 97.5176\n",
      "Episode 69/200, Highest Score: 80, Episode Score: 52, Episode Reward: 106.5000, Episode Epsilon: 0.4998, Episode Loss: 0.0013, Mean Score: 54.7536, Mean Reward 97.6478\n",
      "Episode 70/200, Highest Score: 80, Episode Score: 52, Episode Reward: 89.5000, Episode Epsilon: 0.4948, Episode Loss: 0.0095, Mean Score: 54.7143, Mean Reward 97.5314\n",
      "Model saved after episode 70\n",
      "Episode 71/200, Highest Score: 80, Episode Score: 51, Episode Reward: 84.5000, Episode Epsilon: 0.4899, Episode Loss: 0.0474, Mean Score: 54.6620, Mean Reward 97.3479\n",
      "Episode 72/200, Highest Score: 80, Episode Score: 51, Episode Reward: 97.5000, Episode Epsilon: 0.4850, Episode Loss: 0.1081, Mean Score: 54.6111, Mean Reward 97.3500\n",
      "Episode 73/200, Highest Score: 80, Episode Score: 56, Episode Reward: 114.0000, Episode Epsilon: 0.4801, Episode Loss: 0.4180, Mean Score: 54.6301, Mean Reward 97.5781\n",
      "Episode 74/200, Highest Score: 80, Episode Score: 68, Episode Reward: 126.5000, Episode Epsilon: 0.4753, Episode Loss: 3.6632, Mean Score: 54.8108, Mean Reward 97.9689\n",
      "Episode 75/200, Highest Score: 80, Episode Score: 70, Episode Reward: 300.0000, Episode Epsilon: 0.4706, Episode Loss: 13.4376, Mean Score: 55.0133, Mean Reward 100.6627\n",
      "Episode 76/200, Highest Score: 80, Episode Score: 52, Episode Reward: 80.0000, Episode Epsilon: 0.4659, Episode Loss: 0.5735, Mean Score: 54.9737, Mean Reward 100.3908\n",
      "Episode 77/200, Highest Score: 80, Episode Score: 52, Episode Reward: 95.0000, Episode Epsilon: 0.4612, Episode Loss: 20.0431, Mean Score: 54.9351, Mean Reward 100.3208\n",
      "Episode 78/200, Highest Score: 80, Episode Score: 52, Episode Reward: 103.0000, Episode Epsilon: 0.4566, Episode Loss: 0.2345, Mean Score: 54.8974, Mean Reward 100.3551\n",
      "Episode 79/200, Highest Score: 80, Episode Score: 69, Episode Reward: 132.5000, Episode Epsilon: 0.4520, Episode Loss: 5.0955, Mean Score: 55.0759, Mean Reward 100.7620\n",
      "Episode 80/200, Highest Score: 80, Episode Score: 53, Episode Reward: 457.0000, Episode Epsilon: 0.4475, Episode Loss: 0.0088, Mean Score: 55.0500, Mean Reward 105.2150\n",
      "Model saved after episode 80\n",
      "Episode 81/200, Highest Score: 80, Episode Score: 59, Episode Reward: 238.0000, Episode Epsilon: 0.4430, Episode Loss: 1.2060, Mean Score: 55.0988, Mean Reward 106.8543\n",
      "Episode 82/200, Highest Score: 80, Episode Score: 49, Episode Reward: 97.0000, Episode Epsilon: 0.4386, Episode Loss: 0.0140, Mean Score: 55.0244, Mean Reward 106.7341\n",
      "Episode 83/200, Highest Score: 80, Episode Score: 50, Episode Reward: 89.5000, Episode Epsilon: 0.4342, Episode Loss: 0.0513, Mean Score: 54.9639, Mean Reward 106.5265\n",
      "Episode 84/200, Highest Score: 80, Episode Score: 78, Episode Reward: 166.0000, Episode Epsilon: 0.4299, Episode Loss: 0.0141, Mean Score: 55.2381, Mean Reward 107.2345\n",
      "Episode 85/200, Highest Score: 80, Episode Score: 52, Episode Reward: 130.5000, Episode Epsilon: 0.4256, Episode Loss: 0.0618, Mean Score: 55.2000, Mean Reward 107.5082\n",
      "Episode 86/200, Highest Score: 80, Episode Score: 53, Episode Reward: 82.0000, Episode Epsilon: 0.4213, Episode Loss: 0.0000, Mean Score: 55.1744, Mean Reward 107.2116\n",
      "Episode 87/200, Highest Score: 80, Episode Score: 52, Episode Reward: 98.0000, Episode Epsilon: 0.4171, Episode Loss: 0.0208, Mean Score: 55.1379, Mean Reward 107.1057\n",
      "Episode 88/200, Highest Score: 80, Episode Score: 55, Episode Reward: 143.5000, Episode Epsilon: 0.4129, Episode Loss: 0.2447, Mean Score: 55.1364, Mean Reward 107.5193\n",
      "Episode 89/200, Highest Score: 80, Episode Score: 52, Episode Reward: 109.5000, Episode Epsilon: 0.4088, Episode Loss: 0.1990, Mean Score: 55.1011, Mean Reward 107.5416\n",
      "Episode 90/200, Highest Score: 80, Episode Score: 55, Episode Reward: 144.5000, Episode Epsilon: 0.4047, Episode Loss: 0.1661, Mean Score: 55.1000, Mean Reward 107.9522\n",
      "Model saved after episode 90\n",
      "Episode 91/200, Highest Score: 80, Episode Score: 52, Episode Reward: 77.0000, Episode Epsilon: 0.4007, Episode Loss: 2.3874, Mean Score: 55.0659, Mean Reward 107.6121\n",
      "Episode 92/200, Highest Score: 80, Episode Score: 51, Episode Reward: 81.0000, Episode Epsilon: 0.3967, Episode Loss: 0.3976, Mean Score: 55.0217, Mean Reward 107.3228\n",
      "Episode 93/200, Highest Score: 80, Episode Score: 62, Episode Reward: 113.5000, Episode Epsilon: 0.3927, Episode Loss: 0.1438, Mean Score: 55.0968, Mean Reward 107.3892\n",
      "Episode 94/200, Highest Score: 80, Episode Score: 64, Episode Reward: 253.0000, Episode Epsilon: 0.3888, Episode Loss: 0.2621, Mean Score: 55.1915, Mean Reward 108.9383\n",
      "Episode 95/200, Highest Score: 80, Episode Score: 52, Episode Reward: 96.0000, Episode Epsilon: 0.3849, Episode Loss: 0.0020, Mean Score: 55.1579, Mean Reward 108.8021\n",
      "Episode 96/200, Highest Score: 80, Episode Score: 52, Episode Reward: 92.5000, Episode Epsilon: 0.3810, Episode Loss: 0.0008, Mean Score: 55.1250, Mean Reward 108.6323\n",
      "Episode 97/200, Highest Score: 80, Episode Score: 52, Episode Reward: 105.0000, Episode Epsilon: 0.3772, Episode Loss: 0.0157, Mean Score: 55.0928, Mean Reward 108.5948\n",
      "Episode 98/200, Highest Score: 80, Episode Score: 68, Episode Reward: 135.5000, Episode Epsilon: 0.3735, Episode Loss: 0.0115, Mean Score: 55.2245, Mean Reward 108.8694\n",
      "Episode 99/200, Highest Score: 80, Episode Score: 52, Episode Reward: 85.0000, Episode Epsilon: 0.3697, Episode Loss: 0.0120, Mean Score: 55.1919, Mean Reward 108.6283\n",
      "Episode 100/200, Highest Score: 80, Episode Score: 52, Episode Reward: 93.5000, Episode Epsilon: 0.3660, Episode Loss: 0.0571, Mean Score: 55.1600, Mean Reward 108.4770\n",
      "Model saved after episode 100\n",
      "Episode 101/200, Highest Score: 80, Episode Score: 52, Episode Reward: 99.0000, Episode Epsilon: 0.3624, Episode Loss: 0.0433, Mean Score: 55.1287, Mean Reward 108.3832\n",
      "Episode 102/200, Highest Score: 80, Episode Score: 54, Episode Reward: 92.5000, Episode Epsilon: 0.3587, Episode Loss: 0.3233, Mean Score: 55.1176, Mean Reward 108.2275\n",
      "Episode 103/200, Highest Score: 80, Episode Score: 53, Episode Reward: 105.5000, Episode Epsilon: 0.3552, Episode Loss: 0.0741, Mean Score: 55.0971, Mean Reward 108.2010\n",
      "Episode 104/200, Highest Score: 80, Episode Score: 51, Episode Reward: 99.0000, Episode Epsilon: 0.3516, Episode Loss: 0.0102, Mean Score: 55.0577, Mean Reward 108.1125\n",
      "Episode 105/200, Highest Score: 80, Episode Score: 52, Episode Reward: 95.0000, Episode Epsilon: 0.3481, Episode Loss: 0.2116, Mean Score: 55.0286, Mean Reward 107.9876\n",
      "Episode 106/200, Highest Score: 80, Episode Score: 51, Episode Reward: 81.0000, Episode Epsilon: 0.3446, Episode Loss: 0.0571, Mean Score: 54.9906, Mean Reward 107.7330\n",
      "Episode 107/200, Highest Score: 80, Episode Score: 52, Episode Reward: 91.0000, Episode Epsilon: 0.3412, Episode Loss: 0.2154, Mean Score: 54.9626, Mean Reward 107.5766\n",
      "Episode 108/200, Highest Score: 80, Episode Score: 51, Episode Reward: 102.0000, Episode Epsilon: 0.3378, Episode Loss: 0.0884, Mean Score: 54.9259, Mean Reward 107.5250\n",
      "Episode 109/200, Highest Score: 80, Episode Score: 52, Episode Reward: 104.0000, Episode Epsilon: 0.3344, Episode Loss: 4.4657, Mean Score: 54.8991, Mean Reward 107.4927\n",
      "Episode 110/200, Highest Score: 80, Episode Score: 51, Episode Reward: 105.5000, Episode Epsilon: 0.3310, Episode Loss: 0.0039, Mean Score: 54.8636, Mean Reward 107.4745\n",
      "Model saved after episode 110\n",
      "Episode 111/200, Highest Score: 80, Episode Score: 51, Episode Reward: 81.0000, Episode Epsilon: 0.3277, Episode Loss: 0.0002, Mean Score: 54.8288, Mean Reward 107.2360\n",
      "Episode 112/200, Highest Score: 80, Episode Score: 62, Episode Reward: 129.5000, Episode Epsilon: 0.3244, Episode Loss: 0.0789, Mean Score: 54.8929, Mean Reward 107.4348\n",
      "Episode 113/200, Highest Score: 80, Episode Score: 54, Episode Reward: 96.0000, Episode Epsilon: 0.3212, Episode Loss: 0.8491, Mean Score: 54.8850, Mean Reward 107.3336\n",
      "Episode 114/200, Highest Score: 80, Episode Score: 52, Episode Reward: 107.5000, Episode Epsilon: 0.3180, Episode Loss: 0.0153, Mean Score: 54.8596, Mean Reward 107.3351\n",
      "Episode 115/200, Highest Score: 80, Episode Score: 51, Episode Reward: 75.5000, Episode Epsilon: 0.3148, Episode Loss: 0.1396, Mean Score: 54.8261, Mean Reward 107.0583\n",
      "Episode 116/200, Highest Score: 80, Episode Score: 53, Episode Reward: 89.0000, Episode Epsilon: 0.3117, Episode Loss: 0.3225, Mean Score: 54.8103, Mean Reward 106.9026\n",
      "Episode 117/200, Highest Score: 80, Episode Score: 62, Episode Reward: 133.5000, Episode Epsilon: 0.3085, Episode Loss: 0.1049, Mean Score: 54.8718, Mean Reward 107.1299\n",
      "Episode 118/200, Highest Score: 80, Episode Score: 51, Episode Reward: 115.5000, Episode Epsilon: 0.3055, Episode Loss: 0.0581, Mean Score: 54.8390, Mean Reward 107.2008\n",
      "Episode 119/200, Highest Score: 80, Episode Score: 52, Episode Reward: 96.0000, Episode Epsilon: 0.3024, Episode Loss: 0.0226, Mean Score: 54.8151, Mean Reward 107.1067\n",
      "Episode 120/200, Highest Score: 80, Episode Score: 51, Episode Reward: 110.0000, Episode Epsilon: 0.2994, Episode Loss: 0.0075, Mean Score: 54.7833, Mean Reward 107.1308\n",
      "Model saved after episode 120\n",
      "Episode 121/200, Highest Score: 80, Episode Score: 51, Episode Reward: 88.0000, Episode Epsilon: 0.2964, Episode Loss: 0.0400, Mean Score: 54.7521, Mean Reward 106.9727\n",
      "Episode 122/200, Highest Score: 80, Episode Score: 51, Episode Reward: 108.0000, Episode Epsilon: 0.2934, Episode Loss: 0.0062, Mean Score: 54.7213, Mean Reward 106.9811\n",
      "Episode 123/200, Highest Score: 80, Episode Score: 59, Episode Reward: 127.5000, Episode Epsilon: 0.2905, Episode Loss: 0.0124, Mean Score: 54.7561, Mean Reward 107.1480\n",
      "Episode 124/200, Highest Score: 80, Episode Score: 52, Episode Reward: 181.0000, Episode Epsilon: 0.2876, Episode Loss: 0.1552, Mean Score: 54.7339, Mean Reward 107.7435\n",
      "Episode 125/200, Highest Score: 80, Episode Score: 68, Episode Reward: 149.5000, Episode Epsilon: 0.2847, Episode Loss: 0.0348, Mean Score: 54.8400, Mean Reward 108.0776\n",
      "Episode 126/200, Highest Score: 80, Episode Score: 51, Episode Reward: 80.0000, Episode Epsilon: 0.2819, Episode Loss: 0.0001, Mean Score: 54.8095, Mean Reward 107.8548\n",
      "Episode 127/200, Highest Score: 80, Episode Score: 50, Episode Reward: 157.0000, Episode Epsilon: 0.2790, Episode Loss: 0.0970, Mean Score: 54.7717, Mean Reward 108.2417\n",
      "Episode 128/200, Highest Score: 80, Episode Score: 48, Episode Reward: 91.0000, Episode Epsilon: 0.2763, Episode Loss: 0.1264, Mean Score: 54.7188, Mean Reward 108.1070\n",
      "Episode 129/200, Highest Score: 80, Episode Score: 51, Episode Reward: 102.5000, Episode Epsilon: 0.2735, Episode Loss: 4.4597, Mean Score: 54.6899, Mean Reward 108.0636\n",
      "Episode 130/200, Highest Score: 80, Episode Score: 52, Episode Reward: 88.5000, Episode Epsilon: 0.2708, Episode Loss: 3628.6477, Mean Score: 54.6692, Mean Reward 107.9131\n",
      "Model saved after episode 130\n",
      "Episode 131/200, Highest Score: 80, Episode Score: 52, Episode Reward: 196.0000, Episode Epsilon: 0.2680, Episode Loss: 4.6376, Mean Score: 54.6489, Mean Reward 108.5855\n",
      "Episode 132/200, Highest Score: 80, Episode Score: 52, Episode Reward: 102.5000, Episode Epsilon: 0.2654, Episode Loss: 2.0197, Mean Score: 54.6288, Mean Reward 108.5394\n",
      "Episode 133/200, Highest Score: 80, Episode Score: 64, Episode Reward: 119.5000, Episode Epsilon: 0.2627, Episode Loss: 7.1115, Mean Score: 54.6992, Mean Reward 108.6218\n",
      "Episode 134/200, Highest Score: 80, Episode Score: 52, Episode Reward: 80.5000, Episode Epsilon: 0.2601, Episode Loss: 0.8394, Mean Score: 54.6791, Mean Reward 108.4119\n",
      "Episode 135/200, Highest Score: 80, Episode Score: 52, Episode Reward: 100.0000, Episode Epsilon: 0.2575, Episode Loss: 0.1697, Mean Score: 54.6593, Mean Reward 108.3496\n",
      "Episode 136/200, Highest Score: 80, Episode Score: 52, Episode Reward: 72.0000, Episode Epsilon: 0.2549, Episode Loss: 2.0018, Mean Score: 54.6397, Mean Reward 108.0824\n",
      "Episode 137/200, Highest Score: 80, Episode Score: 54, Episode Reward: 93.5000, Episode Epsilon: 0.2524, Episode Loss: 0.0385, Mean Score: 54.6350, Mean Reward 107.9759\n",
      "Episode 138/200, Highest Score: 80, Episode Score: 51, Episode Reward: 114.0000, Episode Epsilon: 0.2498, Episode Loss: 0.2166, Mean Score: 54.6087, Mean Reward 108.0196\n",
      "Episode 139/200, Highest Score: 80, Episode Score: 52, Episode Reward: 92.5000, Episode Epsilon: 0.2473, Episode Loss: 0.5113, Mean Score: 54.5899, Mean Reward 107.9079\n",
      "Episode 140/200, Highest Score: 80, Episode Score: 58, Episode Reward: 147.0000, Episode Epsilon: 0.2449, Episode Loss: 0.0172, Mean Score: 54.6143, Mean Reward 108.1871\n",
      "Model saved after episode 140\n",
      "Episode 141/200, Highest Score: 87, Episode Score: 87, Episode Reward: 160.7000, Episode Epsilon: 0.2424, Episode Loss: 0.4463, Mean Score: 54.8440, Mean Reward 108.5596\n",
      "Episode 142/200, Highest Score: 87, Episode Score: 52, Episode Reward: 66.0000, Episode Epsilon: 0.2400, Episode Loss: 0.0015, Mean Score: 54.8239, Mean Reward 108.2599\n",
      "Episode 143/200, Highest Score: 87, Episode Score: 52, Episode Reward: 76.5000, Episode Epsilon: 0.2376, Episode Loss: 0.0005, Mean Score: 54.8042, Mean Reward 108.0378\n",
      "Episode 144/200, Highest Score: 87, Episode Score: 60, Episode Reward: 79.5000, Episode Epsilon: 0.2352, Episode Loss: 0.0706, Mean Score: 54.8403, Mean Reward 107.8396\n",
      "Episode 145/200, Highest Score: 87, Episode Score: 71, Episode Reward: 104.5000, Episode Epsilon: 0.2329, Episode Loss: 0.0191, Mean Score: 54.9517, Mean Reward 107.8166\n",
      "Episode 146/200, Highest Score: 87, Episode Score: 55, Episode Reward: 143.0000, Episode Epsilon: 0.2305, Episode Loss: 0.4936, Mean Score: 54.9521, Mean Reward 108.0575\n",
      "Episode 147/200, Highest Score: 87, Episode Score: 52, Episode Reward: 68.5000, Episode Epsilon: 0.2282, Episode Loss: 0.0282, Mean Score: 54.9320, Mean Reward 107.7884\n",
      "Episode 148/200, Highest Score: 87, Episode Score: 70, Episode Reward: 125.0000, Episode Epsilon: 0.2259, Episode Loss: 0.0073, Mean Score: 55.0338, Mean Reward 107.9047\n",
      "Episode 149/200, Highest Score: 87, Episode Score: 52, Episode Reward: 114.5000, Episode Epsilon: 0.2237, Episode Loss: 0.0468, Mean Score: 55.0134, Mean Reward 107.9490\n",
      "Episode 150/200, Highest Score: 87, Episode Score: 55, Episode Reward: 77.0000, Episode Epsilon: 0.2215, Episode Loss: 0.0910, Mean Score: 55.0133, Mean Reward 107.7427\n",
      "Model saved after episode 150\n",
      "Episode 151/200, Highest Score: 87, Episode Score: 52, Episode Reward: 80.5000, Episode Epsilon: 0.2192, Episode Loss: 0.1150, Mean Score: 54.9934, Mean Reward 107.5623\n",
      "Episode 152/200, Highest Score: 87, Episode Score: 52, Episode Reward: 49.5000, Episode Epsilon: 0.2170, Episode Loss: 0.1996, Mean Score: 54.9737, Mean Reward 107.1803\n",
      "Episode 153/200, Highest Score: 87, Episode Score: 51, Episode Reward: 78.5000, Episode Epsilon: 0.2149, Episode Loss: 0.3295, Mean Score: 54.9477, Mean Reward 106.9928\n",
      "Episode 154/200, Highest Score: 87, Episode Score: 53, Episode Reward: 95.5000, Episode Epsilon: 0.2127, Episode Loss: 0.1709, Mean Score: 54.9351, Mean Reward 106.9182\n",
      "Episode 155/200, Highest Score: 87, Episode Score: 51, Episode Reward: 87.0000, Episode Epsilon: 0.2106, Episode Loss: 0.2866, Mean Score: 54.9097, Mean Reward 106.7897\n",
      "Episode 156/200, Highest Score: 87, Episode Score: 52, Episode Reward: 61.5000, Episode Epsilon: 0.2085, Episode Loss: 0.1525, Mean Score: 54.8910, Mean Reward 106.4994\n",
      "Episode 157/200, Highest Score: 87, Episode Score: 51, Episode Reward: 86.5000, Episode Epsilon: 0.2064, Episode Loss: 0.0227, Mean Score: 54.8662, Mean Reward 106.3720\n",
      "Episode 158/200, Highest Score: 87, Episode Score: 52, Episode Reward: 95.0000, Episode Epsilon: 0.2043, Episode Loss: 0.0110, Mean Score: 54.8481, Mean Reward 106.3000\n",
      "Episode 159/200, Highest Score: 87, Episode Score: 52, Episode Reward: 89.5000, Episode Epsilon: 0.2023, Episode Loss: 2.0588, Mean Score: 54.8302, Mean Reward 106.1943\n",
      "Episode 160/200, Highest Score: 87, Episode Score: 52, Episode Reward: 83.0000, Episode Epsilon: 0.2003, Episode Loss: 1.2322, Mean Score: 54.8125, Mean Reward 106.0494\n",
      "Model saved after episode 160\n",
      "Episode 161/200, Highest Score: 87, Episode Score: 51, Episode Reward: 84.0000, Episode Epsilon: 0.1983, Episode Loss: 31.2019, Mean Score: 54.7888, Mean Reward 105.9124\n",
      "Episode 162/200, Highest Score: 87, Episode Score: 53, Episode Reward: 74.0000, Episode Epsilon: 0.1963, Episode Loss: 19.5609, Mean Score: 54.7778, Mean Reward 105.7154\n",
      "Episode 163/200, Highest Score: 87, Episode Score: 52, Episode Reward: 58.0000, Episode Epsilon: 0.1943, Episode Loss: 0.0786, Mean Score: 54.7607, Mean Reward 105.4227\n",
      "Episode 164/200, Highest Score: 87, Episode Score: 52, Episode Reward: 68.0000, Episode Epsilon: 0.1924, Episode Loss: 0.1009, Mean Score: 54.7439, Mean Reward 105.1945\n",
      "Episode 165/200, Highest Score: 87, Episode Score: 52, Episode Reward: 80.0000, Episode Epsilon: 0.1905, Episode Loss: 1.0666, Mean Score: 54.7273, Mean Reward 105.0418\n",
      "Episode 166/200, Highest Score: 87, Episode Score: 55, Episode Reward: 102.0000, Episode Epsilon: 0.1886, Episode Loss: 1.8450, Mean Score: 54.7289, Mean Reward 105.0235\n",
      "Episode 167/200, Highest Score: 87, Episode Score: 51, Episode Reward: 101.0000, Episode Epsilon: 0.1867, Episode Loss: 0.0053, Mean Score: 54.7066, Mean Reward 104.9994\n",
      "Episode 168/200, Highest Score: 87, Episode Score: 52, Episode Reward: 94.0000, Episode Epsilon: 0.1848, Episode Loss: 1.6619, Mean Score: 54.6905, Mean Reward 104.9339\n",
      "Episode 169/200, Highest Score: 87, Episode Score: 51, Episode Reward: 100.5000, Episode Epsilon: 0.1830, Episode Loss: 0.1940, Mean Score: 54.6686, Mean Reward 104.9077\n",
      "Episode 170/200, Highest Score: 87, Episode Score: 52, Episode Reward: 87.0000, Episode Epsilon: 0.1811, Episode Loss: 2.6113, Mean Score: 54.6529, Mean Reward 104.8024\n",
      "Model saved after episode 170\n",
      "Episode 171/200, Highest Score: 87, Episode Score: 52, Episode Reward: 101.0000, Episode Epsilon: 0.1793, Episode Loss: 1.4120, Mean Score: 54.6374, Mean Reward 104.7801\n",
      "Episode 172/200, Highest Score: 87, Episode Score: 55, Episode Reward: 119.0000, Episode Epsilon: 0.1775, Episode Loss: 0.0938, Mean Score: 54.6395, Mean Reward 104.8628\n",
      "Episode 173/200, Highest Score: 87, Episode Score: 52, Episode Reward: 69.0000, Episode Epsilon: 0.1757, Episode Loss: 0.2416, Mean Score: 54.6243, Mean Reward 104.6555\n",
      "Episode 174/200, Highest Score: 87, Episode Score: 53, Episode Reward: 78.0000, Episode Epsilon: 0.1740, Episode Loss: 0.1313, Mean Score: 54.6149, Mean Reward 104.5023\n",
      "Episode 175/200, Highest Score: 87, Episode Score: 60, Episode Reward: 89.0000, Episode Epsilon: 0.1722, Episode Loss: 0.0000, Mean Score: 54.6457, Mean Reward 104.4137\n",
      "Episode 176/200, Highest Score: 87, Episode Score: 51, Episode Reward: 86.5000, Episode Epsilon: 0.1705, Episode Loss: 0.2587, Mean Score: 54.6250, Mean Reward 104.3119\n",
      "Episode 177/200, Highest Score: 87, Episode Score: 52, Episode Reward: 67.5000, Episode Epsilon: 0.1688, Episode Loss: 0.0117, Mean Score: 54.6102, Mean Reward 104.1040\n",
      "Episode 178/200, Highest Score: 87, Episode Score: 51, Episode Reward: 71.0000, Episode Epsilon: 0.1671, Episode Loss: 0.6702, Mean Score: 54.5899, Mean Reward 103.9180\n",
      "Episode 179/200, Highest Score: 87, Episode Score: 52, Episode Reward: 72.5000, Episode Epsilon: 0.1655, Episode Loss: 0.0026, Mean Score: 54.5754, Mean Reward 103.7425\n",
      "Episode 180/200, Highest Score: 87, Episode Score: 52, Episode Reward: 89.0000, Episode Epsilon: 0.1638, Episode Loss: 0.1663, Mean Score: 54.5611, Mean Reward 103.6606\n",
      "Model saved after episode 180\n",
      "Episode 181/200, Highest Score: 87, Episode Score: 52, Episode Reward: 92.0000, Episode Epsilon: 0.1622, Episode Loss: 0.0625, Mean Score: 54.5470, Mean Reward 103.5961\n",
      "Episode 182/200, Highest Score: 87, Episode Score: 52, Episode Reward: 100.0000, Episode Epsilon: 0.1605, Episode Loss: 0.1022, Mean Score: 54.5330, Mean Reward 103.5764\n",
      "Episode 183/200, Highest Score: 87, Episode Score: 52, Episode Reward: 68.0000, Episode Epsilon: 0.1589, Episode Loss: 0.0895, Mean Score: 54.5191, Mean Reward 103.3820\n",
      "Episode 184/200, Highest Score: 87, Episode Score: 52, Episode Reward: 85.5000, Episode Epsilon: 0.1574, Episode Loss: 0.3446, Mean Score: 54.5054, Mean Reward 103.2848\n",
      "Episode 185/200, Highest Score: 87, Episode Score: 52, Episode Reward: 77.0000, Episode Epsilon: 0.1558, Episode Loss: 0.0030, Mean Score: 54.4919, Mean Reward 103.1427\n",
      "Episode 186/200, Highest Score: 87, Episode Score: 51, Episode Reward: 124.0000, Episode Epsilon: 0.1542, Episode Loss: 0.0030, Mean Score: 54.4731, Mean Reward 103.2548\n",
      "Episode 187/200, Highest Score: 87, Episode Score: 51, Episode Reward: 124.0000, Episode Epsilon: 0.1527, Episode Loss: 0.0134, Mean Score: 54.4545, Mean Reward 103.3658\n",
      "Episode 188/200, Highest Score: 87, Episode Score: 51, Episode Reward: 97.0000, Episode Epsilon: 0.1512, Episode Loss: 0.2775, Mean Score: 54.4362, Mean Reward 103.3319\n",
      "Episode 189/200, Highest Score: 87, Episode Score: 51, Episode Reward: 70.0000, Episode Epsilon: 0.1496, Episode Loss: 0.0079, Mean Score: 54.4180, Mean Reward 103.1556\n",
      "Episode 190/200, Highest Score: 87, Episode Score: 50, Episode Reward: 94.0000, Episode Epsilon: 0.1481, Episode Loss: 0.0152, Mean Score: 54.3947, Mean Reward 103.1074\n",
      "Model saved after episode 190\n",
      "Episode 191/200, Highest Score: 87, Episode Score: 52, Episode Reward: 94.0000, Episode Epsilon: 0.1467, Episode Loss: 0.0305, Mean Score: 54.3822, Mean Reward 103.0597\n",
      "Episode 192/200, Highest Score: 87, Episode Score: 52, Episode Reward: 104.0000, Episode Epsilon: 0.1452, Episode Loss: 0.0011, Mean Score: 54.3698, Mean Reward 103.0646\n",
      "Episode 193/200, Highest Score: 87, Episode Score: 51, Episode Reward: 111.0000, Episode Epsilon: 0.1437, Episode Loss: 0.0799, Mean Score: 54.3523, Mean Reward 103.1057\n",
      "Episode 194/200, Highest Score: 87, Episode Score: 53, Episode Reward: 125.0000, Episode Epsilon: 0.1423, Episode Loss: 0.0374, Mean Score: 54.3454, Mean Reward 103.2186\n",
      "Episode 195/200, Highest Score: 87, Episode Score: 51, Episode Reward: 106.0000, Episode Epsilon: 0.1409, Episode Loss: 0.0019, Mean Score: 54.3282, Mean Reward 103.2328\n",
      "Episode 196/200, Highest Score: 87, Episode Score: 54, Episode Reward: 97.0000, Episode Epsilon: 0.1395, Episode Loss: 0.0149, Mean Score: 54.3265, Mean Reward 103.2010\n",
      "Episode 197/200, Highest Score: 87, Episode Score: 52, Episode Reward: 94.0000, Episode Epsilon: 0.1381, Episode Loss: 0.9791, Mean Score: 54.3147, Mean Reward 103.1543\n",
      "Episode 198/200, Highest Score: 87, Episode Score: 52, Episode Reward: 89.5000, Episode Epsilon: 0.1367, Episode Loss: 0.0418, Mean Score: 54.3030, Mean Reward 103.0854\n",
      "Episode 199/200, Highest Score: 87, Episode Score: 51, Episode Reward: 83.5000, Episode Epsilon: 0.1353, Episode Loss: 0.1071, Mean Score: 54.2864, Mean Reward 102.9869\n",
      "Episode 200/200, Highest Score: 87, Episode Score: 52, Episode Reward: 109.0000, Episode Epsilon: 0.1340, Episode Loss: 0.5443, Mean Score: 54.2750, Mean Reward 103.0170\n",
      "Model saved after episode 200\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95228883-827c-4785-8f57-09f19d58a183",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2f0fbcd-552c-4eb4-8c8a-23901ac790b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "389c61ea-2ec2-4df1-9e9a-d1f8ff4961ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4da4d70-a7c3-495b-8ab4-9468f6b4a5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbidmalvi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_174455-wmzsjovb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/wmzsjovb' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/wmzsjovb' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/wmzsjovb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Highest Score: 76, Episode Score: 76, Episode Reward: -50.1000, Episode Epsilon: 0.9950, Episode Loss: 42.4994, Mean Score: 76.0000, Mean Reward -50.1000\n",
      "Episode 2/200, Highest Score: 76, Episode Score: 51, Episode Reward: -40.7000, Episode Epsilon: 0.9900, Episode Loss: 7.9702, Mean Score: 63.5000, Mean Reward -45.4000\n",
      "Episode 3/200, Highest Score: 76, Episode Score: 52, Episode Reward: -54.9000, Episode Epsilon: 0.9851, Episode Loss: 0.3879, Mean Score: 59.6667, Mean Reward -48.5667\n",
      "Episode 4/200, Highest Score: 76, Episode Score: 52, Episode Reward: -44.7000, Episode Epsilon: 0.9801, Episode Loss: 9.5477, Mean Score: 57.7500, Mean Reward -47.6000\n",
      "Episode 5/200, Highest Score: 76, Episode Score: 52, Episode Reward: -45.3000, Episode Epsilon: 0.9752, Episode Loss: 0.0764, Mean Score: 56.6000, Mean Reward -47.1400\n",
      "Episode 6/200, Highest Score: 76, Episode Score: 52, Episode Reward: -59.7000, Episode Epsilon: 0.9704, Episode Loss: 0.1993, Mean Score: 55.8333, Mean Reward -49.2333\n",
      "Episode 7/200, Highest Score: 99, Episode Score: 99, Episode Reward: -85.8000, Episode Epsilon: 0.9655, Episode Loss: 0.0001, Mean Score: 62.0000, Mean Reward -54.4571\n",
      "Episode 8/200, Highest Score: 99, Episode Score: 54, Episode Reward: -40.3000, Episode Epsilon: 0.9607, Episode Loss: 0.8150, Mean Score: 61.0000, Mean Reward -52.6875\n",
      "Episode 9/200, Highest Score: 99, Episode Score: 52, Episode Reward: -52.3000, Episode Epsilon: 0.9559, Episode Loss: 0.6129, Mean Score: 60.0000, Mean Reward -52.6444\n",
      "Episode 10/200, Highest Score: 99, Episode Score: 52, Episode Reward: -71.9000, Episode Epsilon: 0.9511, Episode Loss: 0.0265, Mean Score: 59.2000, Mean Reward -54.5700\n",
      "Model saved after episode 10\n",
      "Episode 11/200, Highest Score: 99, Episode Score: 54, Episode Reward: -61.4000, Episode Epsilon: 0.9464, Episode Loss: 0.5555, Mean Score: 58.7273, Mean Reward -55.1909\n",
      "Episode 12/200, Highest Score: 99, Episode Score: 67, Episode Reward: -69.1000, Episode Epsilon: 0.9416, Episode Loss: 6.9856, Mean Score: 59.4167, Mean Reward -56.3500\n",
      "Episode 13/200, Highest Score: 99, Episode Score: 52, Episode Reward: -60.5000, Episode Epsilon: 0.9369, Episode Loss: 25.7323, Mean Score: 58.8462, Mean Reward -56.6692\n",
      "Episode 14/200, Highest Score: 99, Episode Score: 51, Episode Reward: -57.1000, Episode Epsilon: 0.9322, Episode Loss: 4.8393, Mean Score: 58.2857, Mean Reward -56.7000\n",
      "Episode 15/200, Highest Score: 99, Episode Score: 53, Episode Reward: -41.6000, Episode Epsilon: 0.9276, Episode Loss: 5.0661, Mean Score: 57.9333, Mean Reward -55.6933\n",
      "Episode 16/200, Highest Score: 99, Episode Score: 52, Episode Reward: -61.1000, Episode Epsilon: 0.9229, Episode Loss: 0.3272, Mean Score: 57.5625, Mean Reward -56.0312\n",
      "Episode 17/200, Highest Score: 99, Episode Score: 52, Episode Reward: -43.8000, Episode Epsilon: 0.9183, Episode Loss: 0.5454, Mean Score: 57.2353, Mean Reward -55.3118\n",
      "Episode 18/200, Highest Score: 99, Episode Score: 52, Episode Reward: -56.5000, Episode Epsilon: 0.9137, Episode Loss: 2.8485, Mean Score: 56.9444, Mean Reward -55.3778\n",
      "Episode 19/200, Highest Score: 99, Episode Score: 52, Episode Reward: -65.2000, Episode Epsilon: 0.9092, Episode Loss: 1.6698, Mean Score: 56.6842, Mean Reward -55.8947\n",
      "Episode 20/200, Highest Score: 99, Episode Score: 51, Episode Reward: -48.6000, Episode Epsilon: 0.9046, Episode Loss: 0.4986, Mean Score: 56.4000, Mean Reward -55.5300\n",
      "Model saved after episode 20\n",
      "Episode 21/200, Highest Score: 99, Episode Score: 52, Episode Reward: -57.2000, Episode Epsilon: 0.9001, Episode Loss: 4.8817, Mean Score: 56.1905, Mean Reward -55.6095\n",
      "Episode 22/200, Highest Score: 99, Episode Score: 52, Episode Reward: -55.3000, Episode Epsilon: 0.8956, Episode Loss: 0.0518, Mean Score: 56.0000, Mean Reward -55.5955\n",
      "Episode 23/200, Highest Score: 99, Episode Score: 70, Episode Reward: -44.8000, Episode Epsilon: 0.8911, Episode Loss: 0.0042, Mean Score: 56.6087, Mean Reward -55.1261\n",
      "Episode 24/200, Highest Score: 99, Episode Score: 53, Episode Reward: -68.1000, Episode Epsilon: 0.8867, Episode Loss: 0.1287, Mean Score: 56.4583, Mean Reward -55.6667\n",
      "Episode 25/200, Highest Score: 99, Episode Score: 57, Episode Reward: -76.4000, Episode Epsilon: 0.8822, Episode Loss: 0.4692, Mean Score: 56.4800, Mean Reward -56.4960\n",
      "Episode 26/200, Highest Score: 99, Episode Score: 51, Episode Reward: -49.4000, Episode Epsilon: 0.8778, Episode Loss: 0.2169, Mean Score: 56.2692, Mean Reward -56.2231\n",
      "Episode 27/200, Highest Score: 99, Episode Score: 59, Episode Reward: -81.1000, Episode Epsilon: 0.8734, Episode Loss: 0.1350, Mean Score: 56.3704, Mean Reward -57.1444\n",
      "Episode 28/200, Highest Score: 99, Episode Score: 53, Episode Reward: -42.0000, Episode Epsilon: 0.8691, Episode Loss: 0.8156, Mean Score: 56.2500, Mean Reward -56.6036\n",
      "Episode 29/200, Highest Score: 99, Episode Score: 52, Episode Reward: -52.0000, Episode Epsilon: 0.8647, Episode Loss: 0.0067, Mean Score: 56.1034, Mean Reward -56.4448\n",
      "Episode 30/200, Highest Score: 99, Episode Score: 54, Episode Reward: -137.1000, Episode Epsilon: 0.8604, Episode Loss: 0.4030, Mean Score: 56.0333, Mean Reward -59.1333\n",
      "Model saved after episode 30\n",
      "Episode 31/200, Highest Score: 99, Episode Score: 53, Episode Reward: -84.2000, Episode Epsilon: 0.8561, Episode Loss: 92.4503, Mean Score: 55.9355, Mean Reward -59.9419\n",
      "Episode 32/200, Highest Score: 99, Episode Score: 51, Episode Reward: -47.6000, Episode Epsilon: 0.8518, Episode Loss: 0.0067, Mean Score: 55.7812, Mean Reward -59.5562\n",
      "Episode 33/200, Highest Score: 99, Episode Score: 66, Episode Reward: -69.1000, Episode Epsilon: 0.8475, Episode Loss: 0.3163, Mean Score: 56.0909, Mean Reward -59.8455\n",
      "Episode 34/200, Highest Score: 99, Episode Score: 52, Episode Reward: -48.0000, Episode Epsilon: 0.8433, Episode Loss: 0.0080, Mean Score: 55.9706, Mean Reward -59.4971\n",
      "Episode 35/200, Highest Score: 99, Episode Score: 54, Episode Reward: -42.9000, Episode Epsilon: 0.8391, Episode Loss: 0.6427, Mean Score: 55.9143, Mean Reward -59.0229\n",
      "Episode 36/200, Highest Score: 99, Episode Score: 52, Episode Reward: -78.4000, Episode Epsilon: 0.8349, Episode Loss: 0.0810, Mean Score: 55.8056, Mean Reward -59.5611\n",
      "Episode 37/200, Highest Score: 99, Episode Score: 55, Episode Reward: -70.6000, Episode Epsilon: 0.8307, Episode Loss: 0.0846, Mean Score: 55.7838, Mean Reward -59.8595\n",
      "Episode 38/200, Highest Score: 99, Episode Score: 52, Episode Reward: -141.7000, Episode Epsilon: 0.8266, Episode Loss: 0.8967, Mean Score: 55.6842, Mean Reward -62.0132\n",
      "Episode 39/200, Highest Score: 99, Episode Score: 52, Episode Reward: -45.6000, Episode Epsilon: 0.8224, Episode Loss: 11.6142, Mean Score: 55.5897, Mean Reward -61.5923\n",
      "Episode 40/200, Highest Score: 99, Episode Score: 63, Episode Reward: -81.4000, Episode Epsilon: 0.8183, Episode Loss: 5.3750, Mean Score: 55.7750, Mean Reward -62.0875\n",
      "Model saved after episode 40\n",
      "Episode 41/200, Highest Score: 99, Episode Score: 69, Episode Reward: -84.8000, Episode Epsilon: 0.8142, Episode Loss: 0.0123, Mean Score: 56.0976, Mean Reward -62.6415\n",
      "Episode 42/200, Highest Score: 99, Episode Score: 95, Episode Reward: -51.4000, Episode Epsilon: 0.8102, Episode Loss: 9871.1572, Mean Score: 57.0238, Mean Reward -62.3738\n",
      "Episode 43/200, Highest Score: 99, Episode Score: 54, Episode Reward: -65.5000, Episode Epsilon: 0.8061, Episode Loss: 0.9414, Mean Score: 56.9535, Mean Reward -62.4465\n",
      "Episode 44/200, Highest Score: 99, Episode Score: 52, Episode Reward: -46.7000, Episode Epsilon: 0.8021, Episode Loss: 44.3197, Mean Score: 56.8409, Mean Reward -62.0886\n",
      "Episode 45/200, Highest Score: 99, Episode Score: 51, Episode Reward: -51.6000, Episode Epsilon: 0.7981, Episode Loss: 0.5655, Mean Score: 56.7111, Mean Reward -61.8556\n",
      "Episode 46/200, Highest Score: 99, Episode Score: 57, Episode Reward: -66.6000, Episode Epsilon: 0.7941, Episode Loss: 924.4398, Mean Score: 56.7174, Mean Reward -61.9587\n",
      "Episode 47/200, Highest Score: 99, Episode Score: 63, Episode Reward: -52.2000, Episode Epsilon: 0.7901, Episode Loss: 0.3421, Mean Score: 56.8511, Mean Reward -61.7511\n",
      "Episode 48/200, Highest Score: 99, Episode Score: 52, Episode Reward: -55.9000, Episode Epsilon: 0.7862, Episode Loss: 2.8144, Mean Score: 56.7500, Mean Reward -61.6292\n",
      "Episode 49/200, Highest Score: 99, Episode Score: 53, Episode Reward: -63.4000, Episode Epsilon: 0.7822, Episode Loss: 13.7626, Mean Score: 56.6735, Mean Reward -61.6653\n",
      "Episode 50/200, Highest Score: 99, Episode Score: 58, Episode Reward: -66.2000, Episode Epsilon: 0.7783, Episode Loss: 9.3405, Mean Score: 56.7000, Mean Reward -61.7560\n",
      "Model saved after episode 50\n",
      "Episode 51/200, Highest Score: 99, Episode Score: 52, Episode Reward: -64.3000, Episode Epsilon: 0.7744, Episode Loss: 0.1952, Mean Score: 56.6078, Mean Reward -61.8059\n",
      "Episode 52/200, Highest Score: 99, Episode Score: 52, Episode Reward: -166.9000, Episode Epsilon: 0.7705, Episode Loss: 0.0048, Mean Score: 56.5192, Mean Reward -63.8269\n",
      "Episode 53/200, Highest Score: 99, Episode Score: 52, Episode Reward: -43.8000, Episode Epsilon: 0.7667, Episode Loss: 1.3023, Mean Score: 56.4340, Mean Reward -63.4491\n",
      "Episode 54/200, Highest Score: 99, Episode Score: 65, Episode Reward: -67.3000, Episode Epsilon: 0.7629, Episode Loss: 32.6916, Mean Score: 56.5926, Mean Reward -63.5204\n",
      "Episode 55/200, Highest Score: 99, Episode Score: 52, Episode Reward: -43.3000, Episode Epsilon: 0.7590, Episode Loss: 1.0503, Mean Score: 56.5091, Mean Reward -63.1527\n",
      "Episode 56/200, Highest Score: 99, Episode Score: 52, Episode Reward: -78.0000, Episode Epsilon: 0.7553, Episode Loss: 5.5619, Mean Score: 56.4286, Mean Reward -63.4179\n",
      "Episode 57/200, Highest Score: 99, Episode Score: 56, Episode Reward: -50.2000, Episode Epsilon: 0.7515, Episode Loss: 0.0826, Mean Score: 56.4211, Mean Reward -63.1860\n",
      "Episode 58/200, Highest Score: 99, Episode Score: 54, Episode Reward: -43.8000, Episode Epsilon: 0.7477, Episode Loss: 2.5898, Mean Score: 56.3793, Mean Reward -62.8517\n",
      "Episode 59/200, Highest Score: 99, Episode Score: 52, Episode Reward: -42.3000, Episode Epsilon: 0.7440, Episode Loss: 0.1642, Mean Score: 56.3051, Mean Reward -62.5034\n",
      "Episode 60/200, Highest Score: 99, Episode Score: 52, Episode Reward: -38.1000, Episode Epsilon: 0.7403, Episode Loss: 0.0227, Mean Score: 56.2333, Mean Reward -62.0967\n",
      "Model saved after episode 60\n",
      "Episode 61/200, Highest Score: 99, Episode Score: 53, Episode Reward: -34.9000, Episode Epsilon: 0.7366, Episode Loss: 0.0033, Mean Score: 56.1803, Mean Reward -61.6508\n",
      "Episode 62/200, Highest Score: 99, Episode Score: 52, Episode Reward: -53.5000, Episode Epsilon: 0.7329, Episode Loss: 0.1647, Mean Score: 56.1129, Mean Reward -61.5194\n",
      "Episode 63/200, Highest Score: 99, Episode Score: 52, Episode Reward: -60.6000, Episode Epsilon: 0.7292, Episode Loss: 3.8215, Mean Score: 56.0476, Mean Reward -61.5048\n",
      "Episode 64/200, Highest Score: 99, Episode Score: 52, Episode Reward: -53.2000, Episode Epsilon: 0.7256, Episode Loss: 0.0794, Mean Score: 55.9844, Mean Reward -61.3750\n",
      "Episode 65/200, Highest Score: 99, Episode Score: 60, Episode Reward: -41.6000, Episode Epsilon: 0.7219, Episode Loss: 0.4820, Mean Score: 56.0462, Mean Reward -61.0708\n",
      "Episode 66/200, Highest Score: 99, Episode Score: 63, Episode Reward: -52.8000, Episode Epsilon: 0.7183, Episode Loss: 0.0182, Mean Score: 56.1515, Mean Reward -60.9455\n",
      "Episode 67/200, Highest Score: 99, Episode Score: 52, Episode Reward: -37.9000, Episode Epsilon: 0.7147, Episode Loss: 0.1153, Mean Score: 56.0896, Mean Reward -60.6015\n",
      "Episode 68/200, Highest Score: 99, Episode Score: 52, Episode Reward: -32.9000, Episode Epsilon: 0.7112, Episode Loss: 0.6890, Mean Score: 56.0294, Mean Reward -60.1941\n",
      "Episode 69/200, Highest Score: 99, Episode Score: 52, Episode Reward: -62.9000, Episode Epsilon: 0.7076, Episode Loss: 0.2320, Mean Score: 55.9710, Mean Reward -60.2333\n",
      "Episode 70/200, Highest Score: 99, Episode Score: 52, Episode Reward: -30.1000, Episode Epsilon: 0.7041, Episode Loss: 0.1475, Mean Score: 55.9143, Mean Reward -59.8029\n",
      "Model saved after episode 70\n",
      "Episode 71/200, Highest Score: 99, Episode Score: 52, Episode Reward: -83.4000, Episode Epsilon: 0.7005, Episode Loss: 0.2362, Mean Score: 55.8592, Mean Reward -60.1352\n",
      "Episode 72/200, Highest Score: 99, Episode Score: 69, Episode Reward: -97.4000, Episode Epsilon: 0.6970, Episode Loss: 0.3727, Mean Score: 56.0417, Mean Reward -60.6528\n",
      "Episode 73/200, Highest Score: 99, Episode Score: 51, Episode Reward: -45.1000, Episode Epsilon: 0.6936, Episode Loss: 0.1102, Mean Score: 55.9726, Mean Reward -60.4397\n",
      "Episode 74/200, Highest Score: 99, Episode Score: 52, Episode Reward: -60.8000, Episode Epsilon: 0.6901, Episode Loss: 1.6376, Mean Score: 55.9189, Mean Reward -60.4446\n",
      "Episode 75/200, Highest Score: 99, Episode Score: 62, Episode Reward: -90.2000, Episode Epsilon: 0.6866, Episode Loss: 0.8584, Mean Score: 56.0000, Mean Reward -60.8413\n",
      "Episode 76/200, Highest Score: 99, Episode Score: 52, Episode Reward: -52.9000, Episode Epsilon: 0.6832, Episode Loss: 0.1356, Mean Score: 55.9474, Mean Reward -60.7368\n",
      "Episode 77/200, Highest Score: 99, Episode Score: 52, Episode Reward: -59.1000, Episode Epsilon: 0.6798, Episode Loss: 16.2053, Mean Score: 55.8961, Mean Reward -60.7156\n",
      "Episode 78/200, Highest Score: 99, Episode Score: 51, Episode Reward: -87.0000, Episode Epsilon: 0.6764, Episode Loss: 1.7302, Mean Score: 55.8333, Mean Reward -61.0526\n",
      "Episode 79/200, Highest Score: 99, Episode Score: 62, Episode Reward: -64.6000, Episode Epsilon: 0.6730, Episode Loss: 1.0963, Mean Score: 55.9114, Mean Reward -61.0975\n",
      "Episode 80/200, Highest Score: 99, Episode Score: 53, Episode Reward: -47.5000, Episode Epsilon: 0.6696, Episode Loss: 0.0113, Mean Score: 55.8750, Mean Reward -60.9275\n",
      "Model saved after episode 80\n",
      "Episode 81/200, Highest Score: 99, Episode Score: 53, Episode Reward: -44.0000, Episode Epsilon: 0.6663, Episode Loss: 0.1630, Mean Score: 55.8395, Mean Reward -60.7185\n",
      "Episode 82/200, Highest Score: 99, Episode Score: 52, Episode Reward: -47.9000, Episode Epsilon: 0.6630, Episode Loss: 0.4784, Mean Score: 55.7927, Mean Reward -60.5622\n",
      "Episode 83/200, Highest Score: 99, Episode Score: 58, Episode Reward: -43.8000, Episode Epsilon: 0.6597, Episode Loss: 0.0558, Mean Score: 55.8193, Mean Reward -60.3602\n",
      "Episode 84/200, Highest Score: 99, Episode Score: 52, Episode Reward: -63.1000, Episode Epsilon: 0.6564, Episode Loss: 9.6614, Mean Score: 55.7738, Mean Reward -60.3929\n",
      "Episode 85/200, Highest Score: 99, Episode Score: 52, Episode Reward: -37.2000, Episode Epsilon: 0.6531, Episode Loss: 0.0039, Mean Score: 55.7294, Mean Reward -60.1200\n",
      "Episode 86/200, Highest Score: 99, Episode Score: 52, Episode Reward: -46.7000, Episode Epsilon: 0.6498, Episode Loss: 100.1690, Mean Score: 55.6860, Mean Reward -59.9640\n",
      "Episode 87/200, Highest Score: 99, Episode Score: 53, Episode Reward: -50.8000, Episode Epsilon: 0.6466, Episode Loss: 0.1358, Mean Score: 55.6552, Mean Reward -59.8586\n",
      "Episode 88/200, Highest Score: 99, Episode Score: 62, Episode Reward: -29.5000, Episode Epsilon: 0.6433, Episode Loss: 0.0207, Mean Score: 55.7273, Mean Reward -59.5136\n",
      "Episode 89/200, Highest Score: 99, Episode Score: 54, Episode Reward: -43.1000, Episode Epsilon: 0.6401, Episode Loss: 0.5268, Mean Score: 55.7079, Mean Reward -59.3292\n",
      "Episode 90/200, Highest Score: 99, Episode Score: 52, Episode Reward: -74.7000, Episode Epsilon: 0.6369, Episode Loss: 0.1438, Mean Score: 55.6667, Mean Reward -59.5000\n",
      "Model saved after episode 90\n",
      "Episode 91/200, Highest Score: 99, Episode Score: 51, Episode Reward: -71.4000, Episode Epsilon: 0.6337, Episode Loss: 0.0397, Mean Score: 55.6154, Mean Reward -59.6308\n",
      "Episode 92/200, Highest Score: 99, Episode Score: 52, Episode Reward: -64.3000, Episode Epsilon: 0.6306, Episode Loss: 0.1300, Mean Score: 55.5761, Mean Reward -59.6815\n",
      "Episode 93/200, Highest Score: 99, Episode Score: 52, Episode Reward: -69.1000, Episode Epsilon: 0.6274, Episode Loss: 0.0842, Mean Score: 55.5376, Mean Reward -59.7828\n",
      "Episode 94/200, Highest Score: 99, Episode Score: 53, Episode Reward: -0.3000, Episode Epsilon: 0.6243, Episode Loss: 0.0039, Mean Score: 55.5106, Mean Reward -59.1500\n",
      "Episode 95/200, Highest Score: 99, Episode Score: 52, Episode Reward: -64.9000, Episode Epsilon: 0.6211, Episode Loss: 0.0660, Mean Score: 55.4737, Mean Reward -59.2105\n",
      "Episode 96/200, Highest Score: 99, Episode Score: 52, Episode Reward: -61.8000, Episode Epsilon: 0.6180, Episode Loss: 2.9789, Mean Score: 55.4375, Mean Reward -59.2375\n",
      "Episode 97/200, Highest Score: 99, Episode Score: 52, Episode Reward: -73.1000, Episode Epsilon: 0.6149, Episode Loss: 0.5558, Mean Score: 55.4021, Mean Reward -59.3804\n",
      "Episode 98/200, Highest Score: 99, Episode Score: 52, Episode Reward: -53.4000, Episode Epsilon: 0.6119, Episode Loss: 0.2359, Mean Score: 55.3673, Mean Reward -59.3194\n",
      "Episode 99/200, Highest Score: 99, Episode Score: 52, Episode Reward: -42.0000, Episode Epsilon: 0.6088, Episode Loss: 0.0001, Mean Score: 55.3333, Mean Reward -59.1444\n",
      "Episode 100/200, Highest Score: 99, Episode Score: 63, Episode Reward: -45.3000, Episode Epsilon: 0.6058, Episode Loss: 3.4293, Mean Score: 55.4100, Mean Reward -59.0060\n",
      "Model saved after episode 100\n",
      "Episode 101/200, Highest Score: 99, Episode Score: 52, Episode Reward: -54.3000, Episode Epsilon: 0.6027, Episode Loss: 0.0828, Mean Score: 55.3762, Mean Reward -58.9594\n",
      "Episode 102/200, Highest Score: 99, Episode Score: 52, Episode Reward: -35.7000, Episode Epsilon: 0.5997, Episode Loss: 0.7541, Mean Score: 55.3431, Mean Reward -58.7314\n",
      "Episode 103/200, Highest Score: 99, Episode Score: 52, Episode Reward: -60.6000, Episode Epsilon: 0.5967, Episode Loss: 4.5280, Mean Score: 55.3107, Mean Reward -58.7495\n",
      "Episode 104/200, Highest Score: 99, Episode Score: 53, Episode Reward: -44.1000, Episode Epsilon: 0.5937, Episode Loss: 0.0246, Mean Score: 55.2885, Mean Reward -58.6087\n",
      "Episode 105/200, Highest Score: 99, Episode Score: 52, Episode Reward: -58.1000, Episode Epsilon: 0.5908, Episode Loss: 2.2836, Mean Score: 55.2571, Mean Reward -58.6038\n",
      "Episode 106/200, Highest Score: 99, Episode Score: 52, Episode Reward: -42.3000, Episode Epsilon: 0.5878, Episode Loss: 0.0088, Mean Score: 55.2264, Mean Reward -58.4500\n",
      "Episode 107/200, Highest Score: 99, Episode Score: 90, Episode Reward: -44.7000, Episode Epsilon: 0.5849, Episode Loss: 0.0067, Mean Score: 55.5514, Mean Reward -58.3215\n",
      "Episode 108/200, Highest Score: 99, Episode Score: 53, Episode Reward: -50.5000, Episode Epsilon: 0.5820, Episode Loss: 0.3543, Mean Score: 55.5278, Mean Reward -58.2491\n",
      "Episode 109/200, Highest Score: 99, Episode Score: 52, Episode Reward: -54.8000, Episode Epsilon: 0.5790, Episode Loss: 0.2204, Mean Score: 55.4954, Mean Reward -58.2174\n",
      "Episode 110/200, Highest Score: 102, Episode Score: 102, Episode Reward: -38.9000, Episode Epsilon: 0.5762, Episode Loss: 0.6110, Mean Score: 55.9182, Mean Reward -58.0418\n",
      "Model saved after episode 110\n",
      "Episode 111/200, Highest Score: 102, Episode Score: 54, Episode Reward: -46.0000, Episode Epsilon: 0.5733, Episode Loss: 0.1391, Mean Score: 55.9009, Mean Reward -57.9333\n",
      "Episode 112/200, Highest Score: 102, Episode Score: 54, Episode Reward: -30.0000, Episode Epsilon: 0.5704, Episode Loss: 0.4333, Mean Score: 55.8839, Mean Reward -57.6839\n",
      "Episode 113/200, Highest Score: 102, Episode Score: 49, Episode Reward: -41.8000, Episode Epsilon: 0.5676, Episode Loss: 0.0018, Mean Score: 55.8230, Mean Reward -57.5434\n",
      "Episode 114/200, Highest Score: 102, Episode Score: 50, Episode Reward: -138.4000, Episode Epsilon: 0.5647, Episode Loss: 0.6902, Mean Score: 55.7719, Mean Reward -58.2526\n",
      "Episode 115/200, Highest Score: 102, Episode Score: 52, Episode Reward: -60.7000, Episode Epsilon: 0.5619, Episode Loss: 4.6569, Mean Score: 55.7391, Mean Reward -58.2739\n",
      "Episode 116/200, Highest Score: 102, Episode Score: 52, Episode Reward: -42.2000, Episode Epsilon: 0.5591, Episode Loss: 0.0129, Mean Score: 55.7069, Mean Reward -58.1353\n",
      "Episode 117/200, Highest Score: 106, Episode Score: 106, Episode Reward: -51.1000, Episode Epsilon: 0.5563, Episode Loss: 0.1415, Mean Score: 56.1368, Mean Reward -58.0752\n",
      "Episode 118/200, Highest Score: 106, Episode Score: 53, Episode Reward: -68.0000, Episode Epsilon: 0.5535, Episode Loss: 0.0064, Mean Score: 56.1102, Mean Reward -58.1593\n",
      "Episode 119/200, Highest Score: 106, Episode Score: 52, Episode Reward: -60.4000, Episode Epsilon: 0.5507, Episode Loss: 0.0002, Mean Score: 56.0756, Mean Reward -58.1782\n",
      "Episode 120/200, Highest Score: 106, Episode Score: 58, Episode Reward: -33.1000, Episode Epsilon: 0.5480, Episode Loss: 0.0859, Mean Score: 56.0917, Mean Reward -57.9692\n",
      "Model saved after episode 120\n",
      "Episode 121/200, Highest Score: 106, Episode Score: 49, Episode Reward: -50.0000, Episode Epsilon: 0.5452, Episode Loss: 0.0358, Mean Score: 56.0331, Mean Reward -57.9033\n",
      "Episode 122/200, Highest Score: 106, Episode Score: 52, Episode Reward: -40.3000, Episode Epsilon: 0.5425, Episode Loss: 0.3589, Mean Score: 56.0000, Mean Reward -57.7590\n",
      "Episode 123/200, Highest Score: 106, Episode Score: 66, Episode Reward: -54.8000, Episode Epsilon: 0.5398, Episode Loss: 0.7311, Mean Score: 56.0813, Mean Reward -57.7350\n",
      "Episode 124/200, Highest Score: 106, Episode Score: 53, Episode Reward: -57.8000, Episode Epsilon: 0.5371, Episode Loss: 0.0033, Mean Score: 56.0565, Mean Reward -57.7355\n",
      "Episode 125/200, Highest Score: 106, Episode Score: 64, Episode Reward: -50.8000, Episode Epsilon: 0.5344, Episode Loss: 0.0393, Mean Score: 56.1200, Mean Reward -57.6800\n",
      "Episode 126/200, Highest Score: 106, Episode Score: 66, Episode Reward: -75.7000, Episode Epsilon: 0.5318, Episode Loss: 0.0981, Mean Score: 56.1984, Mean Reward -57.8230\n",
      "Episode 127/200, Highest Score: 106, Episode Score: 62, Episode Reward: -26.7000, Episode Epsilon: 0.5291, Episode Loss: 0.0000, Mean Score: 56.2441, Mean Reward -57.5780\n",
      "Episode 128/200, Highest Score: 106, Episode Score: 52, Episode Reward: -58.0000, Episode Epsilon: 0.5264, Episode Loss: 0.0007, Mean Score: 56.2109, Mean Reward -57.5812\n",
      "Episode 129/200, Highest Score: 106, Episode Score: 51, Episode Reward: -60.5000, Episode Epsilon: 0.5238, Episode Loss: 0.1222, Mean Score: 56.1705, Mean Reward -57.6039\n",
      "Episode 130/200, Highest Score: 106, Episode Score: 52, Episode Reward: -32.1000, Episode Epsilon: 0.5212, Episode Loss: 0.0056, Mean Score: 56.1385, Mean Reward -57.4077\n",
      "Model saved after episode 130\n",
      "Episode 131/200, Highest Score: 106, Episode Score: 52, Episode Reward: -47.7000, Episode Epsilon: 0.5186, Episode Loss: 0.1362, Mean Score: 56.1069, Mean Reward -57.3336\n",
      "Episode 132/200, Highest Score: 106, Episode Score: 54, Episode Reward: -32.0000, Episode Epsilon: 0.5160, Episode Loss: 0.0002, Mean Score: 56.0909, Mean Reward -57.1417\n",
      "Episode 133/200, Highest Score: 106, Episode Score: 64, Episode Reward: -49.3000, Episode Epsilon: 0.5134, Episode Loss: 0.0125, Mean Score: 56.1504, Mean Reward -57.0827\n",
      "Episode 134/200, Highest Score: 106, Episode Score: 87, Episode Reward: -88.8000, Episode Epsilon: 0.5108, Episode Loss: 0.1100, Mean Score: 56.3806, Mean Reward -57.3194\n",
      "Episode 135/200, Highest Score: 106, Episode Score: 52, Episode Reward: -43.3000, Episode Epsilon: 0.5083, Episode Loss: 0.0205, Mean Score: 56.3481, Mean Reward -57.2156\n",
      "Episode 136/200, Highest Score: 106, Episode Score: 52, Episode Reward: -49.5000, Episode Epsilon: 0.5058, Episode Loss: 0.2347, Mean Score: 56.3162, Mean Reward -57.1588\n",
      "Episode 137/200, Highest Score: 106, Episode Score: 52, Episode Reward: -53.6000, Episode Epsilon: 0.5032, Episode Loss: 0.0153, Mean Score: 56.2847, Mean Reward -57.1328\n",
      "Episode 138/200, Highest Score: 106, Episode Score: 51, Episode Reward: -39.3000, Episode Epsilon: 0.5007, Episode Loss: 0.0234, Mean Score: 56.2464, Mean Reward -57.0036\n",
      "Episode 139/200, Highest Score: 106, Episode Score: 75, Episode Reward: -66.9000, Episode Epsilon: 0.4982, Episode Loss: 0.0012, Mean Score: 56.3813, Mean Reward -57.0748\n",
      "Episode 140/200, Highest Score: 106, Episode Score: 51, Episode Reward: -48.0000, Episode Epsilon: 0.4957, Episode Loss: 0.3548, Mean Score: 56.3429, Mean Reward -57.0100\n",
      "Model saved after episode 140\n",
      "Episode 141/200, Highest Score: 106, Episode Score: 91, Episode Reward: -104.8000, Episode Epsilon: 0.4932, Episode Loss: 0.0380, Mean Score: 56.5887, Mean Reward -57.3489\n",
      "Episode 142/200, Highest Score: 106, Episode Score: 51, Episode Reward: -63.6000, Episode Epsilon: 0.4908, Episode Loss: 0.4478, Mean Score: 56.5493, Mean Reward -57.3930\n",
      "Episode 143/200, Highest Score: 106, Episode Score: 52, Episode Reward: -50.8000, Episode Epsilon: 0.4883, Episode Loss: 0.6293, Mean Score: 56.5175, Mean Reward -57.3469\n",
      "Episode 144/200, Highest Score: 106, Episode Score: 52, Episode Reward: -36.0000, Episode Epsilon: 0.4859, Episode Loss: 0.4150, Mean Score: 56.4861, Mean Reward -57.1986\n",
      "Episode 145/200, Highest Score: 106, Episode Score: 51, Episode Reward: -27.9000, Episode Epsilon: 0.4834, Episode Loss: 0.1594, Mean Score: 56.4483, Mean Reward -56.9966\n",
      "Episode 146/200, Highest Score: 106, Episode Score: 53, Episode Reward: -40.5000, Episode Epsilon: 0.4810, Episode Loss: 1.1716, Mean Score: 56.4247, Mean Reward -56.8836\n",
      "Episode 147/200, Highest Score: 106, Episode Score: 53, Episode Reward: -49.0000, Episode Epsilon: 0.4786, Episode Loss: 0.7429, Mean Score: 56.4014, Mean Reward -56.8299\n",
      "Episode 148/200, Highest Score: 106, Episode Score: 52, Episode Reward: -95.0000, Episode Epsilon: 0.4762, Episode Loss: 0.1806, Mean Score: 56.3716, Mean Reward -57.0878\n",
      "Episode 149/200, Highest Score: 106, Episode Score: 85, Episode Reward: -74.6000, Episode Epsilon: 0.4738, Episode Loss: 0.0005, Mean Score: 56.5638, Mean Reward -57.2054\n",
      "Episode 150/200, Highest Score: 106, Episode Score: 52, Episode Reward: -100.7000, Episode Epsilon: 0.4715, Episode Loss: 0.0629, Mean Score: 56.5333, Mean Reward -57.4953\n",
      "Model saved after episode 150\n",
      "Episode 151/200, Highest Score: 106, Episode Score: 51, Episode Reward: -51.6000, Episode Epsilon: 0.4691, Episode Loss: 0.0097, Mean Score: 56.4967, Mean Reward -57.4563\n",
      "Episode 152/200, Highest Score: 106, Episode Score: 63, Episode Reward: -65.0000, Episode Epsilon: 0.4668, Episode Loss: 0.5469, Mean Score: 56.5395, Mean Reward -57.5059\n",
      "Episode 153/200, Highest Score: 106, Episode Score: 52, Episode Reward: -49.1000, Episode Epsilon: 0.4644, Episode Loss: 0.0048, Mean Score: 56.5098, Mean Reward -57.4510\n",
      "Episode 154/200, Highest Score: 106, Episode Score: 52, Episode Reward: -55.5000, Episode Epsilon: 0.4621, Episode Loss: 0.2924, Mean Score: 56.4805, Mean Reward -57.4383\n",
      "Episode 155/200, Highest Score: 106, Episode Score: 52, Episode Reward: -24.3000, Episode Epsilon: 0.4598, Episode Loss: 0.0941, Mean Score: 56.4516, Mean Reward -57.2245\n",
      "Episode 156/200, Highest Score: 106, Episode Score: 52, Episode Reward: -56.2000, Episode Epsilon: 0.4575, Episode Loss: 0.1640, Mean Score: 56.4231, Mean Reward -57.2179\n",
      "Episode 157/200, Highest Score: 106, Episode Score: 76, Episode Reward: -59.2000, Episode Epsilon: 0.4552, Episode Loss: 0.0539, Mean Score: 56.5478, Mean Reward -57.2306\n",
      "Episode 158/200, Highest Score: 106, Episode Score: 53, Episode Reward: -32.3000, Episode Epsilon: 0.4529, Episode Loss: 0.0266, Mean Score: 56.5253, Mean Reward -57.0728\n",
      "Episode 159/200, Highest Score: 106, Episode Score: 53, Episode Reward: -36.2000, Episode Epsilon: 0.4507, Episode Loss: 0.0073, Mean Score: 56.5031, Mean Reward -56.9415\n",
      "Episode 160/200, Highest Score: 106, Episode Score: 52, Episode Reward: -70.2000, Episode Epsilon: 0.4484, Episode Loss: 0.0080, Mean Score: 56.4750, Mean Reward -57.0244\n",
      "Model saved after episode 160\n",
      "Episode 161/200, Highest Score: 106, Episode Score: 53, Episode Reward: -59.2000, Episode Epsilon: 0.4462, Episode Loss: 0.0188, Mean Score: 56.4534, Mean Reward -57.0379\n",
      "Episode 162/200, Highest Score: 106, Episode Score: 78, Episode Reward: -67.1000, Episode Epsilon: 0.4440, Episode Loss: 1.8818, Mean Score: 56.5864, Mean Reward -57.1000\n",
      "Episode 163/200, Highest Score: 106, Episode Score: 68, Episode Reward: -86.7000, Episode Epsilon: 0.4417, Episode Loss: 1.6105, Mean Score: 56.6564, Mean Reward -57.2816\n",
      "Episode 164/200, Highest Score: 106, Episode Score: 52, Episode Reward: -46.8000, Episode Epsilon: 0.4395, Episode Loss: 0.4250, Mean Score: 56.6280, Mean Reward -57.2177\n",
      "Episode 165/200, Highest Score: 106, Episode Score: 67, Episode Reward: -75.4000, Episode Epsilon: 0.4373, Episode Loss: 0.3576, Mean Score: 56.6909, Mean Reward -57.3279\n",
      "Episode 166/200, Highest Score: 106, Episode Score: 57, Episode Reward: -79.3000, Episode Epsilon: 0.4351, Episode Loss: 0.0855, Mean Score: 56.6928, Mean Reward -57.4602\n",
      "Episode 167/200, Highest Score: 106, Episode Score: 52, Episode Reward: -30.1000, Episode Epsilon: 0.4330, Episode Loss: 0.1502, Mean Score: 56.6647, Mean Reward -57.2964\n",
      "Episode 168/200, Highest Score: 106, Episode Score: 52, Episode Reward: -43.8000, Episode Epsilon: 0.4308, Episode Loss: 0.1366, Mean Score: 56.6369, Mean Reward -57.2161\n",
      "Episode 169/200, Highest Score: 106, Episode Score: 60, Episode Reward: -70.5000, Episode Epsilon: 0.4286, Episode Loss: 0.0163, Mean Score: 56.6568, Mean Reward -57.2947\n",
      "Episode 170/200, Highest Score: 106, Episode Score: 63, Episode Reward: -50.1000, Episode Epsilon: 0.4265, Episode Loss: 1.4287, Mean Score: 56.6941, Mean Reward -57.2524\n",
      "Model saved after episode 170\n",
      "Episode 171/200, Highest Score: 106, Episode Score: 85, Episode Reward: -81.9000, Episode Epsilon: 0.4244, Episode Loss: 0.0036, Mean Score: 56.8596, Mean Reward -57.3965\n",
      "Episode 172/200, Highest Score: 106, Episode Score: 52, Episode Reward: -64.1000, Episode Epsilon: 0.4223, Episode Loss: 0.0710, Mean Score: 56.8314, Mean Reward -57.4355\n",
      "Episode 173/200, Highest Score: 106, Episode Score: 64, Episode Reward: -70.3000, Episode Epsilon: 0.4201, Episode Loss: 2.6817, Mean Score: 56.8728, Mean Reward -57.5098\n",
      "Episode 174/200, Highest Score: 106, Episode Score: 58, Episode Reward: -71.4000, Episode Epsilon: 0.4180, Episode Loss: 0.0011, Mean Score: 56.8793, Mean Reward -57.5897\n",
      "Episode 175/200, Highest Score: 106, Episode Score: 54, Episode Reward: -40.4000, Episode Epsilon: 0.4159, Episode Loss: 0.3003, Mean Score: 56.8629, Mean Reward -57.4914\n",
      "Episode 176/200, Highest Score: 106, Episode Score: 52, Episode Reward: -46.6000, Episode Epsilon: 0.4139, Episode Loss: 0.0552, Mean Score: 56.8352, Mean Reward -57.4295\n",
      "Episode 177/200, Highest Score: 106, Episode Score: 52, Episode Reward: -42.0000, Episode Epsilon: 0.4118, Episode Loss: 0.1761, Mean Score: 56.8079, Mean Reward -57.3424\n",
      "Episode 178/200, Highest Score: 106, Episode Score: 52, Episode Reward: -60.9000, Episode Epsilon: 0.4097, Episode Loss: 0.1313, Mean Score: 56.7809, Mean Reward -57.3624\n",
      "Episode 179/200, Highest Score: 106, Episode Score: 51, Episode Reward: -52.0000, Episode Epsilon: 0.4077, Episode Loss: 0.0044, Mean Score: 56.7486, Mean Reward -57.3324\n",
      "Episode 180/200, Highest Score: 106, Episode Score: 52, Episode Reward: -39.0000, Episode Epsilon: 0.4057, Episode Loss: 0.4135, Mean Score: 56.7222, Mean Reward -57.2306\n",
      "Model saved after episode 180\n",
      "Episode 181/200, Highest Score: 106, Episode Score: 51, Episode Reward: -55.8000, Episode Epsilon: 0.4036, Episode Loss: 0.0285, Mean Score: 56.6906, Mean Reward -57.2227\n",
      "Episode 182/200, Highest Score: 106, Episode Score: 56, Episode Reward: -85.9000, Episode Epsilon: 0.4016, Episode Loss: 0.0024, Mean Score: 56.6868, Mean Reward -57.3802\n",
      "Episode 183/200, Highest Score: 106, Episode Score: 58, Episode Reward: -53.4000, Episode Epsilon: 0.3996, Episode Loss: 0.3515, Mean Score: 56.6940, Mean Reward -57.3585\n",
      "Episode 184/200, Highest Score: 106, Episode Score: 53, Episode Reward: -37.8000, Episode Epsilon: 0.3976, Episode Loss: 0.3087, Mean Score: 56.6739, Mean Reward -57.2522\n",
      "Episode 185/200, Highest Score: 106, Episode Score: 51, Episode Reward: -57.8000, Episode Epsilon: 0.3956, Episode Loss: 0.0253, Mean Score: 56.6432, Mean Reward -57.2551\n",
      "Episode 186/200, Highest Score: 106, Episode Score: 51, Episode Reward: -66.4000, Episode Epsilon: 0.3936, Episode Loss: 1.4289, Mean Score: 56.6129, Mean Reward -57.3043\n",
      "Episode 187/200, Highest Score: 106, Episode Score: 51, Episode Reward: -37.1000, Episode Epsilon: 0.3917, Episode Loss: 0.2148, Mean Score: 56.5829, Mean Reward -57.1963\n",
      "Episode 188/200, Highest Score: 106, Episode Score: 52, Episode Reward: -43.9000, Episode Epsilon: 0.3897, Episode Loss: 2.8710, Mean Score: 56.5585, Mean Reward -57.1255\n",
      "Episode 189/200, Highest Score: 106, Episode Score: 70, Episode Reward: -26.4000, Episode Epsilon: 0.3878, Episode Loss: 0.1112, Mean Score: 56.6296, Mean Reward -56.9630\n",
      "Episode 190/200, Highest Score: 106, Episode Score: 62, Episode Reward: -79.8000, Episode Epsilon: 0.3858, Episode Loss: 1.0679, Mean Score: 56.6579, Mean Reward -57.0832\n",
      "Model saved after episode 190\n",
      "Episode 191/200, Highest Score: 106, Episode Score: 67, Episode Reward: -64.1000, Episode Epsilon: 0.3839, Episode Loss: 0.0471, Mean Score: 56.7120, Mean Reward -57.1199\n",
      "Episode 192/200, Highest Score: 106, Episode Score: 51, Episode Reward: -44.1000, Episode Epsilon: 0.3820, Episode Loss: 1.1701, Mean Score: 56.6823, Mean Reward -57.0521\n",
      "Episode 193/200, Highest Score: 106, Episode Score: 51, Episode Reward: -48.7000, Episode Epsilon: 0.3801, Episode Loss: 1.0384, Mean Score: 56.6528, Mean Reward -57.0088\n",
      "Episode 194/200, Highest Score: 106, Episode Score: 52, Episode Reward: -56.2000, Episode Epsilon: 0.3782, Episode Loss: 0.0077, Mean Score: 56.6289, Mean Reward -57.0046\n",
      "Episode 195/200, Highest Score: 106, Episode Score: 52, Episode Reward: -26.9000, Episode Epsilon: 0.3763, Episode Loss: 0.0009, Mean Score: 56.6051, Mean Reward -56.8503\n",
      "Episode 196/200, Highest Score: 106, Episode Score: 53, Episode Reward: -55.6000, Episode Epsilon: 0.3744, Episode Loss: 0.0362, Mean Score: 56.5867, Mean Reward -56.8439\n",
      "Episode 197/200, Highest Score: 106, Episode Score: 52, Episode Reward: -30.3000, Episode Epsilon: 0.3725, Episode Loss: 0.3913, Mean Score: 56.5635, Mean Reward -56.7091\n",
      "Episode 198/200, Highest Score: 106, Episode Score: 52, Episode Reward: -53.2000, Episode Epsilon: 0.3707, Episode Loss: 0.2653, Mean Score: 56.5404, Mean Reward -56.6914\n",
      "Episode 199/200, Highest Score: 106, Episode Score: 52, Episode Reward: -39.1000, Episode Epsilon: 0.3688, Episode Loss: 1.1783, Mean Score: 56.5176, Mean Reward -56.6030\n",
      "Episode 200/200, Highest Score: 106, Episode Score: 57, Episode Reward: -86.6000, Episode Epsilon: 0.3670, Episode Loss: 0.1439, Mean Score: 56.5200, Mean Reward -56.7530\n",
      "Model saved after episode 200\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05144910-6083-4497-9db9-3f36dd8d2c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8afd46-99e3-41a9-82a2-49072b4e5db0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "85b3b016-4d07-4470-b19d-f68954e8cd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(agent, env, episodes, model_path, log_to_wandb=False, older_model=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_rl_agent', name='test_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    agent.load_model(model_path, older_model, for_training=False)\n",
    "\n",
    "    # Set exploration rate (epsilon) to 0 to only choose actions based on the model's predictions (exploit its knowledge)\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafc388-89d9-42a8-b9b1-ac8557b7d92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b3054a3-6522-48a1-aca0-f58a9eb64637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/train_2/dino_dqn_episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c6ebd52c-a8cd-46e9-9e29-7b5dbb66e9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28844155-561c-494c-bccd-685b6dde5758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 52, Episode Score: 52, Episode Reward: 141.0000, Episode Epsilon: 0.0000, Mean Score: 52.0000, Mean Reward 141.0000\n",
      "Episode 2/5, Highest Score: 52, Episode Score: 52, Episode Reward: 142.0000, Episode Epsilon: 0.0000, Mean Score: 52.0000, Mean Reward 141.5000\n",
      "Episode 3/5, Highest Score: 52, Episode Score: 52, Episode Reward: 138.5000, Episode Epsilon: 0.0000, Mean Score: 52.0000, Mean Reward 140.5000\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=112.0.5615.139)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x00BDDCE3+50899]\n\t(No symbol) [0x00B6E111]\n\t(No symbol) [0x00A75588]\n\t(No symbol) [0x00A5D333]\n\t(No symbol) [0x00ABF4DB]\n\t(No symbol) [0x00ACDB33]\n\t(No symbol) [0x00ABB6F6]\n\t(No symbol) [0x00A97708]\n\t(No symbol) [0x00A9886D]\n\tGetHandleVerifier [0x00E43EAE+2566302]\n\tGetHandleVerifier [0x00E792B1+2784417]\n\tGetHandleVerifier [0x00E7327C+2759788]\n\tGetHandleVerifier [0x00C75740+672048]\n\t(No symbol) [0x00B78872]\n\t(No symbol) [0x00B741C8]\n\t(No symbol) [0x00B742AB]\n\t(No symbol) [0x00B671B7]\n\tBaseThreadInitThunk [0x76696B89+25]\n\tRtlGetFullPathName_UEx [0x777E8F9F+1215]\n\tRtlGetFullPathName_UEx [0x777E8F6D+1165]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m DinoDQNAgent(env)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Test model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_EPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_LOAD_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_to_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 27\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(agent, env, episodes, model_path, log_to_wandb, older_model, render)\u001b[0m\n\u001b[0;32m     24\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Take a step in the environment\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     30\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[80], line 290\u001b[0m, in \u001b[0;36mDinoEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    287\u001b[0m     action_chains\u001b[38;5;241m.\u001b[39mkey_up(key)\u001b[38;5;241m.\u001b[39mperform()\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Get next observation\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Check whether game is over\u001b[39;00m\n\u001b[0;32m    293\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_game_over()\n",
      "Cell \u001b[1;32mIn[80], line 190\u001b[0m, in \u001b[0;36mDinoEnvironment.get_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 190\u001b[0m     obstacles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obstacles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     trex_y, trex_height, trex_width, trex_duck_height, trex_duck_width, trex_is_jumping, trex_is_ducking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_trex_info()\n\u001b[0;32m    192\u001b[0m     game_speed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_game_speed()\n",
      "Cell \u001b[1;32mIn[80], line 68\u001b[0m, in \u001b[0;36mDinoEnvironment._get_obstacles\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_obstacles\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 68\u001b[0m     obstacles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_script\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn Runner.instance_.horizon.obstacles\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     obstacle_info \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obstacle \u001b[38;5;129;01min\u001b[39;00m obstacles:\n",
      "File \u001b[1;32m~\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:500\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    497\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[0;32m    498\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[1;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscript\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverted_args\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:440\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\venv\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:245\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    243\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=112.0.5615.139)\nStacktrace:\nBacktrace:\n\tGetHandleVerifier [0x00BDDCE3+50899]\n\t(No symbol) [0x00B6E111]\n\t(No symbol) [0x00A75588]\n\t(No symbol) [0x00A5D333]\n\t(No symbol) [0x00ABF4DB]\n\t(No symbol) [0x00ACDB33]\n\t(No symbol) [0x00ABB6F6]\n\t(No symbol) [0x00A97708]\n\t(No symbol) [0x00A9886D]\n\tGetHandleVerifier [0x00E43EAE+2566302]\n\tGetHandleVerifier [0x00E792B1+2784417]\n\tGetHandleVerifier [0x00E7327C+2759788]\n\tGetHandleVerifier [0x00C75740+672048]\n\t(No symbol) [0x00B78872]\n\t(No symbol) [0x00B741C8]\n\t(No symbol) [0x00B742AB]\n\t(No symbol) [0x00B671B7]\n\tBaseThreadInitThunk [0x76696B89+25]\n\tRtlGetFullPathName_UEx [0x777E8F9F+1215]\n\tRtlGetFullPathName_UEx [0x777E8F6D+1165]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96b0dc1-46c2-402e-b877-f1d5cc199b4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e422a356-b6b1-4f42-95e3-506d6752be7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"best_trained_models\\episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fbfb8b5-b0b5-45ce-bd31-a5dc29db5212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bbd7344-6506-40b2-b00d-e6f0a9691bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 526, Episode Score: 526, Episode Reward: 828.8000, Episode Epsilon: 0.0000, Mean Score: 526.0000, Mean Reward 828.8000\n",
      "Episode 2/5, Highest Score: 1294, Episode Score: 1294, Episode Reward: 1063.1000, Episode Epsilon: 0.0000, Mean Score: 910.0000, Mean Reward 945.9500\n",
      "Episode 3/5, Highest Score: 1618, Episode Score: 1618, Episode Reward: 602.8000, Episode Epsilon: 0.0000, Mean Score: 1146.0000, Mean Reward 831.5667\n",
      "Episode 4/5, Highest Score: 1618, Episode Score: 576, Episode Reward: 82.6000, Episode Epsilon: 0.0000, Mean Score: 1003.5000, Mean Reward 644.3250\n",
      "Episode 5/5, Highest Score: 1618, Episode Score: 147, Episode Reward: 24.9000, Episode Epsilon: 0.0000, Mean Score: 832.2000, Mean Reward 520.4400\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False, older_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7626983-195c-4243-85b6-1f0e7e3af761",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9aaac4-3718-4c0d-832e-619ade51568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "EPISODE_NUMS = 1000\n",
    "\n",
    "for episode in range(EPISODE_NUMS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    agent.replay()\n",
    "    print(f\"Episode {episode + 1}/{EPISODE_NUMS}, Reward: {episode_reward}, Current Score: {info['current_score']}, High Score: {info['high_score']}\")\n",
    "\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        model_file = os.path.join(OUTPUT_DIR, f\"episode_{episode + 1}.pth\")\n",
    "        agent.save_model(model_file)\n",
    "        print(f\"Model saved after episode {episode + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547af20d-98e6-4fd5-9c97-e0cde6076c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "\n",
    "# agent = DinoDQNAgent(env)\n",
    "agent.load_model(\"model_output\\dino\\episode_100.pth\")\n",
    "# Set agent's exploration rate (epsilon) to zero, so that it only chooses actions based on the model's predictions\n",
    "agent.epsilon = 0\n",
    "\n",
    "EPISODE_NUMS = 100\n",
    "\n",
    "# Test loop\n",
    "for episode in range(EPISODE_NUMS):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode + 1}/{EPISODE_NUMS}, Total episode reward: {episode_reward}, Final score: {info['current_score']}, Highest score achieved: {info['high_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e43a6c-ab99-4441-82eb-c02a43feefa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
