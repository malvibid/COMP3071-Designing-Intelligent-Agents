{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57dec060-bf08-4a6e-a1d4-1839b4730f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Custom Dino Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e94c8c1-c5e5-4b8f-8c22-242647e9b880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64147db4-ea80-49a1-b50e-1fce4f97af98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Environment Components\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "\n",
    "# Selenium for automatically loading and play the game\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa02f78b-c77c-4ac9-9a35-7ab41ed4e123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoEnvironment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b0937a3c-edcf-487a-af0d-a178a1f5b4ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dino Game Environment\n",
    "class DinoEnvironment(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Subclass model\n",
    "        super().__init__()\n",
    "\n",
    "        self.driver = self._create_driver()\n",
    "\n",
    "        # Setup spaces\n",
    "        low_values = np.array(\n",
    "            [0, 0, 0, 6, -1, -1, -1, -1, -1, -1], dtype=np.float32)  # Initial speed is 6, while max speed is 13\n",
    "        high_values = np.array(\n",
    "            [150, 1, 1, 13, 600, 3, 600, 150, 50, 50], dtype=np.float32)  # Canvas dimensions are 600x150\n",
    "        self.observation_space = Box(\n",
    "            low=low_values, high=high_values, shape=(10,), dtype=np.float32)\n",
    "\n",
    "        # Start jumping, Start ducking, Stop ducking, Do nothing - Ducking has been divided into two actions because the agent should also learn the correct ducking duration\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.actions_map = [\n",
    "            (Keys.ARROW_UP, \"key_down\"),  # Start jumping\n",
    "            (Keys.ARROW_DOWN, \"key_down\"),  # Start ducking\n",
    "            (Keys.ARROW_DOWN, \"key_up\"),  # Stop ducking\n",
    "            (Keys.ARROW_RIGHT, \"key_down\")  # Do nothing\n",
    "        ]\n",
    "\n",
    "        # Keep track of number of obstacles the agent has passed\n",
    "        self.passed_obstacles = 0\n",
    "\n",
    "    # Create and return an instance of the Chrome Driver\n",
    "    def _create_driver(self):\n",
    "\n",
    "        # Set options for the WebDriver\n",
    "        options = Options()\n",
    "\n",
    "        # Turn off logging to keep terminal clean\n",
    "        options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "        # Keep the browser running after the code finishes executing\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "        # Create a Service instance for running the ChromeDriver executable\n",
    "        service = Service(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "        # Create an instance of the Chrome WebDriver with the specified service and options - The driver object can be used to automate interactions with the Chrome browser\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "        # Maximize the Chrome window\n",
    "        driver.maximize_window()\n",
    "\n",
    "        return driver\n",
    "\n",
    "    # Encode the obstacle type as an integer\n",
    "    def _encode_obstacle_type(self, obstacle_type):\n",
    "        if obstacle_type == 'CACTUS_SMALL':\n",
    "            return 0\n",
    "        elif obstacle_type == 'CACTUS_LARGE':\n",
    "            return 1\n",
    "        elif obstacle_type == 'PTERODACTYL':\n",
    "            return 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown obstacle type: {obstacle_type}\")\n",
    "\n",
    "    # Get obstacles that are currently on the screen\n",
    "    def _get_obstacles(self):\n",
    "        obstacles = self.driver.execute_script(\n",
    "            \"return Runner.instance_.horizon.obstacles\")\n",
    "        obstacle_info = []\n",
    "        for obstacle in obstacles:\n",
    "            obstacle_type = obstacle['typeConfig']['type']\n",
    "            # Encode the obstacle type as an integer\n",
    "            encoded_obstacle_type = self._encode_obstacle_type(obstacle_type)\n",
    "            obstacle_x = obstacle['xPos']\n",
    "            obstacle_y = obstacle['yPos']\n",
    "            obstacle_width = obstacle['typeConfig']['width']\n",
    "            obstacle_height = obstacle['typeConfig']['height']\n",
    "            obstacle_info.append(\n",
    "                (encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height))\n",
    "        return obstacle_info\n",
    "\n",
    "    # Get Trex's state (Jumping, Ducking or Running/Do nothing)\n",
    "    def _get_trex_info(self):\n",
    "        trex = self.driver.execute_script(\"return Runner.instance_.tRex\")\n",
    "        # xpos remains the same throughout the game - don't need it\n",
    "        trex_y = trex['yPos']\n",
    "        trex_is_jumping = trex['jumping']\n",
    "        trex_is_ducking = trex['ducking']\n",
    "        return trex_y, trex_is_jumping, trex_is_ducking\n",
    "\n",
    "    # Get current game speed\n",
    "    def _get_game_speed(self):\n",
    "        game_speed = self.driver.execute_script(\n",
    "            \"return Runner.instance_.currentSpeed\")\n",
    "        return game_speed\n",
    "\n",
    "    # Get the distance between the Trex and the next obstacle\n",
    "    def _get_distance_to_next_obstacle(self):\n",
    "        trex_x = self.driver.execute_script(\n",
    "            \"return Runner.instance_.tRex.xPos\")  # xpos of trex\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            next_obstacle = obstacles[0]\n",
    "            obstacle_x = next_obstacle[1]  # xpos of next obstacle\n",
    "            distance_to_next_obstacle = obstacle_x - trex_x\n",
    "        else:\n",
    "            distance_to_next_obstacle = None\n",
    "        return distance_to_next_obstacle\n",
    "\n",
    "    # Check if the agent has passed an obstacle\n",
    "    def _passed_obstacle(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        if obstacles:\n",
    "            # next_obstacle: [encoded_obstacle_type, obstacle_x, obstacle_y, obstacle_width, obstacle_height]\n",
    "            next_obstacle = obstacles[0]\n",
    "            trex_x = self.driver.execute_script(\n",
    "                \"return Runner.instance_.tRex.xPos\")\n",
    "            obstacle_x = next_obstacle[1]  # Next obstacles xpos\n",
    "            obstacle_width = next_obstacle[3]  # Next obstacles width\n",
    "            return obstacle_x + obstacle_width < trex_x\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Get and return the score for the last game played\n",
    "    def _get_current_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.digits\")))\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Get and return the high score for all games played in current browser session\n",
    "    def _get_high_score(self):\n",
    "        try:\n",
    "            score = int(''.join(self.driver.execute_script(\n",
    "                \"return Runner.instance_.distanceMeter.highScore.slice(-5)\")))  # MaxScore=99999, MaxScoreUnits=5\n",
    "        except:\n",
    "            score = 0\n",
    "        return score\n",
    "\n",
    "    # Capture screenshot of current game state and return the image captured for rendering\n",
    "    def _get_image(self):\n",
    "        # Capture a screenshot of the game canvas as a data URL - string that represents the image in base64-encoded format\n",
    "        data_url = self.driver.execute_script(\n",
    "            \"return document.querySelector('canvas.runner-canvas').toDataURL()\")\n",
    "\n",
    "        # Remove the leading text from the data URL using string slicing and decode the remaining base64-encoded data\n",
    "        LEADING_TEXT = \"data:image/png;base64,\"\n",
    "        image_data = base64.b64decode(data_url[len(LEADING_TEXT):])\n",
    "\n",
    "        # Convert the binary data in 'image_data' to a 1D NumPy array\n",
    "        image_array = np.frombuffer(image_data, dtype=np.uint8)\n",
    "\n",
    "        # Decode the image data and create an OpenCV image object - OpenCV Image Shape format (H, W, C) ( rows, columns, and channels )\n",
    "        image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return image\n",
    "\n",
    "    # Load and Reset the game environment\n",
    "    def reset(self):\n",
    "        try:\n",
    "            # Navigate to the Chrome Dino website\n",
    "            self.driver.get(\"chrome://dino/\")\n",
    "\n",
    "        except WebDriverException as e:\n",
    "            # Ignore \"ERR_INTERNET_DISCONNECTED\" error thrown because this game is available offline\n",
    "            if \"ERR_INTERNET_DISCONNECTED\" in str(e):\n",
    "                pass  # Ignore the exception.\n",
    "            else:\n",
    "                raise e  # Handle other WebDriverExceptions\n",
    "\n",
    "        # Avoid errors that can arise due to the 'runner-canvas' element not being present - Using WebDriverWait and EC together ensures that the code does not proceed until the required element is present\n",
    "        timeout = 10\n",
    "        WebDriverWait(self.driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"runner-canvas\")))\n",
    "\n",
    "        # Start game\n",
    "        self.driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.SPACE)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    # Get the current state of the game and return it as the observation\n",
    "    def get_observation(self):\n",
    "        obstacles = self._get_obstacles()\n",
    "        trex_y, trex_is_jumping, trex_is_ducking = self._get_trex_info()\n",
    "        game_speed = self._get_game_speed()\n",
    "        distance_to_next_obstacle = self._get_distance_to_next_obstacle()\n",
    "\n",
    "        state = (\n",
    "            trex_y,\n",
    "            trex_is_jumping,\n",
    "            trex_is_ducking,\n",
    "            game_speed,\n",
    "            distance_to_next_obstacle,\n",
    "            # Unpack the tuple of the first obstacle\n",
    "            *(obstacles[0] if obstacles else (None, None, None, None, None))\n",
    "        )\n",
    "\n",
    "        # Set dtype for state to float32 for consistency and compatibility with the RL algorithm\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "\n",
    "        # Replace NaN values with -1\n",
    "        state[np.isnan(state)] = -1\n",
    "\n",
    "        return state\n",
    "\n",
    "    # Check if the game is over and return True or False\n",
    "    def is_game_over(self):\n",
    "        # Done if either Trex crashed into an obstacle or reached max score which is 99999\n",
    "        # Check if Trex crashed\n",
    "        crashed = self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "        # Get the maximum score from the game\n",
    "        max_score = self.driver.execute_script(\n",
    "            \"return Runner.instance_.distanceMeter.maxScore\")\n",
    "        current_score = self._get_current_score()\n",
    "\n",
    "        return crashed or (current_score >= max_score)\n",
    "\n",
    "    # Calculate and return the reward for the current state of the game\n",
    "    def get_reward(self, done):\n",
    "        # Must maintain the relative importance of different rewards so that the agent can differentiate between the various outcomes and is encouraged to learn a good policy\n",
    "        reward = 0\n",
    "        if done:\n",
    "            # Penalize for crashing into an obstacle\n",
    "            reward -= 10\n",
    "        else:\n",
    "            if self._passed_obstacle():\n",
    "                # Reward for passing an obstacle\n",
    "                reward += 0.5\n",
    "                self.passed_obstacles += 1\n",
    "            else:\n",
    "                # Small reward for staying alive\n",
    "                reward += 0.1\n",
    "\n",
    "        current_score = self._get_current_score()\n",
    "        high_score = self._get_high_score()\n",
    "\n",
    "        if current_score > high_score:\n",
    "            # Bonus reward for surpassing the high score\n",
    "            reward += 1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Take a step in the game environment based on the given action\n",
    "    def step(self, action):\n",
    "\n",
    "        # Take action\n",
    "        # Get key and action mapping\n",
    "        key, action_type = self.actions_map[action]\n",
    "\n",
    "        # Create a new ActionChains object\n",
    "        action_chains = ActionChains(self.driver)\n",
    "\n",
    "        # Perform the key press action\n",
    "        if action_type == \"key_down\":\n",
    "            action_chains.key_down(key).perform()\n",
    "        # Perform the key release action\n",
    "        elif action_type == \"key_up\":\n",
    "            action_chains.key_up(key).perform()\n",
    "\n",
    "        # Get next observation\n",
    "        obs = self.get_observation()\n",
    "\n",
    "        # Check whether game is over\n",
    "        done = self.is_game_over()\n",
    "\n",
    "        # Get reward\n",
    "        reward = self.get_reward(done)\n",
    "\n",
    "        info = {\n",
    "            'current_score': self._get_current_score(),\n",
    "            'high_score': self._get_high_score()\n",
    "        }\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    # Visualise the game\n",
    "    def render(self, mode: str = 'human'):\n",
    "        img = cv2.cvtColor(self._get_image(), cv2.COLOR_BGR2RGB)\n",
    "        if mode == 'rgb-array':\n",
    "            return img\n",
    "        elif mode == 'human':\n",
    "            cv2.imshow('Dino Game', img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "    # Close the game environment and the driver\n",
    "    def close(self):\n",
    "        self.driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a42599e-123f-4932-bbe4-8e97663b4222",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test the Custom Game Environment\n",
    "\n",
    "This section is for testing the Game Environment to ensure it is defined correctly before using it with the Agent for RL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7bc403f4-7400-4656-ba2a-bd79673640d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper class to format and print observations properly\n",
    "def print_formatted_obs(observations):\n",
    "    obs_titles = [\"trex_y\", \"trex_jumping\", \"trex_ducking\", \"game_speed\", \"obst_dist\", \"obst_type\", \"obst_x\", \"obst_y\", \"obst_width\", \"obst_height\"]\n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(observations, columns=obs_titles)\n",
    "\n",
    "    # Set the pandas display options for better readability (optional)\n",
    "    pd.set_option(\"display.width\", 140)\n",
    "    # pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cfb1a4d7-bf2a-492e-bbea-96ff3ecda78c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([83.   ,  1.   ,  0.   ,  6.011, -1.   , -1.   , -1.   , -1.   ,\n",
       "       -1.   , -1.   ], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DinoEnvironment()\n",
    "env.reset() # returns an observation from the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dcc8c9b8-87a7-407e-bea4-bdbf7585fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([ 0.  0.  0.  6. -1. -1. -1. -1. -1. -1.], [150.   1.   1.  13. 600.   3. 600. 150.  50.  50.], (10,), float32)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82fba9fa-0135-4ca0-ac02-e2620fe05f34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bd240adb-926c-4f23-b692-1063bf2942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fedc098e-9ef4-4fde-9ec2-1ed964c28bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "908c10c2-7cbb-40e1-9289-aaa14a95c7b9",
   "metadata": {},
   "source": [
    "**Note:** Render function works better if using `.py` python files instead of the `.ipynb` notebook to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75ab681d-89a2-41bc-9491-ab58d48e497d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     trex_y  trex_jumping  trex_ducking  game_speed  obst_dist  obst_type  obst_x  obst_y  obst_width  obst_height\n",
      "0      73.0           1.0           0.0       6.017       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "1      51.0           1.0           0.0       6.029       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "2      33.0           1.0           0.0       6.041       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "3      19.0           1.0           0.0       6.051       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "4      13.0           1.0           0.0       6.063       -1.0       -1.0    -1.0    -1.0        -1.0         -1.0\n",
      "..      ...           ...           ...         ...        ...        ...     ...     ...         ...          ...\n",
      "137    93.0           0.0           0.0       7.719       99.0        0.0   105.0   105.0        17.0         35.0\n",
      "138    93.0           0.0           0.0       7.731       84.0        0.0    87.0   105.0        17.0         35.0\n",
      "139    93.0           0.0           0.0       7.741       66.0        0.0    72.0   105.0        17.0         35.0\n",
      "140    93.0           0.0           1.0       7.755       48.0        0.0    51.0   105.0        17.0         35.0\n",
      "141    93.0           0.0           1.0       7.759       38.0        0.0    39.0   105.0        17.0         35.0\n",
      "\n",
      "[142 rows x 10 columns]\n",
      "Episode: 0, Total Reward: 134.09999999999965, , Current Score: 60, High Score: 60\n"
     ]
    }
   ],
   "source": [
    "# Test loop - Play 1 game\n",
    "env = DinoEnvironment()\n",
    "for episode in range(1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    all_observations = []\n",
    "    # images = []\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Take random actions\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # print(obs)\n",
    "        all_observations.append(obs)  # Print obs formatted nicely in a table\n",
    "        total_reward += reward\n",
    "\n",
    "        # env.render(mode='human')\n",
    "        # img = env.render(mode='rgb-array')\n",
    "        # images.append(img) # Can use some image library to create a gif using collected images\n",
    "\n",
    "    print_formatted_obs(all_observations)\n",
    "    print(f\"Episode: {episode}, Total Reward: {total_reward}, , Current Score: {info['current_score']}, High Score: {info['high_score']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf346759-391a-4a48-81c2-e5943ed23ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DQN Dino Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc910af-22e2-45ed-8022-ace974d90ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4efbeb-851d-4a23-b8c6-0f33f2a27645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c0862b2-3e86-4c79-ad98-078ce2e5ddcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DinoDQNAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dc2b9a48-e081-4727-b071-807a90f131e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DinoDQNAgent():\n",
    "    def __init__(self, env,\n",
    "                 gamma=0.95,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_min=0.01,\n",
    "                 epsilon_decay=0.995,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=32,\n",
    "                 memory_size=100000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]  # 10\n",
    "        self.action_size = env.action_space.n  # 4\n",
    "        self.hidden_sizes = [64, 128]  # number of hidden neurons for the model\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma  # discounting factor\n",
    "        self.epsilon = epsilon  # exploration rate\n",
    "        self.epsilon_min = epsilon_min  # min exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # exploration decay per step\n",
    "        self.batch_size = batch_size\n",
    "        self.model = self._build_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Define the DQN model architecture - This model will be used to approximate the Q-values of the agent's actions given a state.\n",
    "    def _build_model(self):\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], self.action_size)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    # Store agents experiences as a tuple\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Determine which action to take given a state\n",
    "    def act(self, state):\n",
    "        # Explore randomly or exploit given the current epsilon value\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            return action\n",
    "\n",
    "    # Update the DQN model using a batch of experiences sampled from the memory\n",
    "    def replay(self):\n",
    "        # Check if the number of experiences (state, action, reward, next_state, done) in the memory is less than the batch size\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # Don't do anything since there's not enough data to create a minibatch for training\n",
    "            return\n",
    "\n",
    "        # Create minibatch from a random sample of experiences from the memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Calculate the expected Q-value for the current state-action pair (q_target)\n",
    "            # If done, - Game has ended, don't need to make predictions about future rewards\n",
    "            q_target = reward\n",
    "            if not done:\n",
    "                # Calculate the Q-values for the next state using the DQN model, i.e., estimate future reward\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "                q_values_next = self.model(next_state)\n",
    "                # Update the target value by adding the discounted maximum Q-value of the next state to the current reward\n",
    "                q_target = reward + self.gamma * \\\n",
    "                    torch.max(q_values_next).item()\n",
    "\n",
    "            # Calculate the Q-values for the current state using the DQN model\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = self.model(state)\n",
    "\n",
    "            # Update/Map the expected Q-value of the chosen action with the calculated target value\n",
    "            q_values_expected = q_values.clone().detach()\n",
    "\n",
    "            q_values_expected[action] = q_target\n",
    "\n",
    "            # Note: q_values_expected is the ground truth for the action that the agent took in the current state vs q_values is the models prediction of what should happen\n",
    "\n",
    "            # Reset the gradients of the optimizer before performing backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calculate the loss using the Mean Squared Error (MSE) between the current Q-values and the expected Q-values\n",
    "            loss = self.loss_fn(q_values, q_values_expected)\n",
    "\n",
    "            # Perform backpropagation to calculate the gradients of the model's parameters with respect to the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's parameters using the calculated gradients and the optimizer's learning rate\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decrease episolon over time to reduce exploration and increase exploitation of the models learnt knowledge\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        # Return the loss value\n",
    "        return loss.item()\n",
    "\n",
    "    # Save the current state of the DQN model and optimizer to a file.\n",
    "    def save_model(self, model_name, model_output_dir, log_to_wandb):\n",
    "        # Create a dictionary to store the state of the model, optimizer and any other additional information\n",
    "        state = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        save_path = os.path.join(\n",
    "            model_output_dir, model_name)\n",
    "\n",
    "        # Save the state dictionary to a file\n",
    "        torch.save(state, save_path)\n",
    "\n",
    "        if log_to_wandb:\n",
    "            # Save model as a wandb artifact\n",
    "            artifact = wandb.Artifact(model_name, type='model')\n",
    "            artifact.add_file(save_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "    # Load the DQN model and optimizer state from a file.\n",
    "    def load_model(self, file_path, older_model, for_training):\n",
    "\n",
    "        if older_model:\n",
    "            self.model.load_state_dict(torch.load(file_path))\n",
    "        else:\n",
    "            # Load the state dictionary from the file using the torch.load() function\n",
    "            state = torch.load(file_path)\n",
    "\n",
    "            # Restore the state of the model and optimizer\n",
    "            self.model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "            # Set for_training to true if using the model to continue training from a previously saved state\n",
    "            if for_training:\n",
    "                self.optimizer.load_state_dict(state['optimizer_state_dict'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1648970-0a2b-4fcd-b73f-e2c59c443464",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train and Test Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b742893d-09ed-4817-bc9e-2381bba3a6b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c883a5b-c4ac-4d5f-a20c-2817711396b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392289b8-6d05-438e-b037-60f46efe93b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "578dbe60-f461-4c8a-8303-3fcd1fd4ba9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7d966df0-fd81-489d-b867-a35a62148678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(agent, env, episodes, model_output_dir, save_interval=10, log_to_wandb=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_rl_agent', name='train_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Remember agents experience after every step\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Train/Update the model every episode\n",
    "        loss = agent.replay()\n",
    "        episode_loss.append(loss)\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_episode_loss = sum(episode_loss) / len(episode_loss)\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Episode Loss: {loss:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"episode_loss\": loss,\n",
    "                \"mean_loss\": mean_episode_loss,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "\n",
    "        # Save the model every save_interval episodes\n",
    "        if (episode + 1) % save_interval == 0:\n",
    "            model_name = f\"dino_dqn_episode_{episode + 1}.pth\"\n",
    "            agent.save_model(model_name, model_output_dir, log_to_wandb)\n",
    "            print(f\"Model saved after episode {episode + 1}\")\n",
    "            \n",
    "    # Finish wandb logging\n",
    "    if log_to_wandb:\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3de6bef-757a-49f3-bdb9-04dc2a6130db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16ad5ada-035b-493b-8ea7-23a567d7243b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify directory to save model\n",
    "OUTPUT_DIR = \"trained_models/\"\n",
    "\n",
    "# Create directories if they don't exist on the path\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4e6cf8cd-45f6-408a-af41-fc49a96283d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to train the agent\n",
    "TRAIN_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "18f92077-d7f1-4f8b-bb68-47a8c0bbaf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_214120-cwainy14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">train_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Highest Score: 107, Episode Score: 107, Episode Reward: 97.3000, Episode Epsilon: 0.9950, Episode Loss: 2.1888, Mean Score: 107.0000, Mean Reward 97.3000\n",
      "Episode 2/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.3000, Episode Epsilon: 0.9900, Episode Loss: 16.3613, Mean Score: 80.0000, Mean Reward 49.3000\n",
      "Episode 3/200, Highest Score: 107, Episode Score: 54, Episode Reward: 2.6000, Episode Epsilon: 0.9851, Episode Loss: 26.8944, Mean Score: 71.3333, Mean Reward 33.7333\n",
      "Episode 4/200, Highest Score: 107, Episode Score: 54, Episode Reward: 2.5000, Episode Epsilon: 0.9801, Episode Loss: 19.7332, Mean Score: 67.0000, Mean Reward 25.9250\n",
      "Episode 5/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.9752, Episode Loss: 46.6583, Mean Score: 64.0000, Mean Reward 20.9600\n",
      "Episode 6/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.9704, Episode Loss: 46.9283, Mean Score: 62.0000, Mean Reward 17.8667\n",
      "Episode 7/200, Highest Score: 107, Episode Score: 74, Episode Reward: 14.4000, Episode Epsilon: 0.9655, Episode Loss: 2.0431, Mean Score: 63.7143, Mean Reward 17.3714\n",
      "Episode 8/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.0000, Episode Epsilon: 0.9607, Episode Loss: 0.2355, Mean Score: 62.1250, Mean Reward 15.4500\n",
      "Episode 9/200, Highest Score: 107, Episode Score: 65, Episode Reward: 6.5000, Episode Epsilon: 0.9559, Episode Loss: 39.5528, Mean Score: 62.4444, Mean Reward 14.4556\n",
      "Episode 10/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.9511, Episode Loss: 11.6301, Mean Score: 61.4000, Mean Reward 13.0800\n",
      "Model saved after episode 10\n",
      "Episode 11/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.4000, Episode Epsilon: 0.9464, Episode Loss: 2.8436, Mean Score: 60.5455, Mean Reward 11.9273\n",
      "Episode 12/200, Highest Score: 107, Episode Score: 61, Episode Reward: 9.1000, Episode Epsilon: 0.9416, Episode Loss: 0.3191, Mean Score: 60.5833, Mean Reward 11.6917\n",
      "Episode 13/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.9369, Episode Loss: 3.7230, Mean Score: 59.9231, Mean Reward 10.9154\n",
      "Episode 14/200, Highest Score: 107, Episode Score: 62, Episode Reward: 8.2000, Episode Epsilon: 0.9322, Episode Loss: 0.1091, Mean Score: 60.0714, Mean Reward 10.7214\n",
      "Episode 15/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.9276, Episode Loss: 1.0112, Mean Score: 59.5333, Mean Reward 10.0600\n",
      "Episode 16/200, Highest Score: 107, Episode Score: 55, Episode Reward: 5.4000, Episode Epsilon: 0.9229, Episode Loss: 0.0656, Mean Score: 59.2500, Mean Reward 9.7687\n",
      "Episode 17/200, Highest Score: 107, Episode Score: 62, Episode Reward: 2.1000, Episode Epsilon: 0.9183, Episode Loss: 0.0057, Mean Score: 59.4118, Mean Reward 9.3176\n",
      "Episode 18/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.8000, Episode Epsilon: 0.9137, Episode Loss: 0.2931, Mean Score: 59.0556, Mean Reward 8.9000\n",
      "Episode 19/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.9092, Episode Loss: 0.0705, Mean Score: 58.6842, Mean Reward 8.5526\n",
      "Episode 20/200, Highest Score: 107, Episode Score: 55, Episode Reward: 8.2000, Episode Epsilon: 0.9046, Episode Loss: 0.7471, Mean Score: 58.5000, Mean Reward 8.5350\n",
      "Model saved after episode 20\n",
      "Episode 21/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.8000, Episode Epsilon: 0.9001, Episode Loss: 1.9995, Mean Score: 58.2381, Mean Reward 8.2619\n",
      "Episode 22/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.8956, Episode Loss: 8.4445, Mean Score: 57.9545, Mean Reward 7.9955\n",
      "Episode 23/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.8911, Episode Loss: 0.5921, Mean Score: 57.6522, Mean Reward 7.7696\n",
      "Episode 24/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.8867, Episode Loss: 2.8903, Mean Score: 57.4167, Mean Reward 7.5250\n",
      "Episode 25/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.8822, Episode Loss: 0.6163, Mean Score: 57.2000, Mean Reward 7.3000\n",
      "Episode 26/200, Highest Score: 107, Episode Score: 71, Episode Reward: 11.4000, Episode Epsilon: 0.8778, Episode Loss: 0.2983, Mean Score: 57.7308, Mean Reward 7.4577\n",
      "Episode 27/200, Highest Score: 107, Episode Score: 53, Episode Reward: 7.9000, Episode Epsilon: 0.8734, Episode Loss: 13.5564, Mean Score: 57.5556, Mean Reward 7.4741\n",
      "Episode 28/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.8691, Episode Loss: 6.6913, Mean Score: 57.3571, Mean Reward 7.3214\n",
      "Episode 29/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.3000, Episode Epsilon: 0.8647, Episode Loss: 16.3935, Mean Score: 57.1724, Mean Reward 7.0793\n",
      "Episode 30/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.3000, Episode Epsilon: 0.8604, Episode Loss: 106.1849, Mean Score: 57.0000, Mean Reward 6.8867\n",
      "Model saved after episode 30\n",
      "Episode 31/200, Highest Score: 107, Episode Score: 64, Episode Reward: 15.1000, Episode Epsilon: 0.8561, Episode Loss: 0.0719, Mean Score: 57.2258, Mean Reward 7.1516\n",
      "Episode 32/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.8518, Episode Loss: 6.6869, Mean Score: 57.0625, Mean Reward 6.9500\n",
      "Episode 33/200, Highest Score: 107, Episode Score: 54, Episode Reward: 4.6000, Episode Epsilon: 0.8475, Episode Loss: 54.3699, Mean Score: 56.9697, Mean Reward 6.8788\n",
      "Episode 34/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.8433, Episode Loss: 10151.1592, Mean Score: 56.8235, Mean Reward 6.6941\n",
      "Episode 35/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.8391, Episode Loss: 48.6221, Mean Score: 56.6857, Mean Reward 6.5286\n",
      "Episode 36/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.2000, Episode Epsilon: 0.8349, Episode Loss: 12.2669, Mean Score: 56.5278, Mean Reward 6.3806\n",
      "Episode 37/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.8307, Episode Loss: 1.3879, Mean Score: 56.4054, Mean Reward 6.2243\n",
      "Episode 38/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.8266, Episode Loss: 190.8391, Mean Score: 56.2895, Mean Reward 6.0816\n",
      "Episode 39/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.8224, Episode Loss: 0.0000, Mean Score: 56.1795, Mean Reward 5.9513\n",
      "Episode 40/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.8183, Episode Loss: 3.3555, Mean Score: 56.0750, Mean Reward 5.8325\n",
      "Model saved after episode 40\n",
      "Episode 41/200, Highest Score: 107, Episode Score: 71, Episode Reward: 4.6000, Episode Epsilon: 0.8142, Episode Loss: 0.0797, Mean Score: 56.4390, Mean Reward 5.8024\n",
      "Episode 42/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.6000, Episode Epsilon: 0.8102, Episode Loss: 19.2836, Mean Score: 56.3095, Mean Reward 5.7024\n",
      "Episode 43/200, Highest Score: 107, Episode Score: 65, Episode Reward: 3.4000, Episode Epsilon: 0.8061, Episode Loss: 0.5337, Mean Score: 56.5116, Mean Reward 5.6488\n",
      "Episode 44/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.8021, Episode Loss: 36.2273, Mean Score: 56.4091, Mean Reward 5.5614\n",
      "Episode 45/200, Highest Score: 107, Episode Score: 73, Episode Reward: 5.9000, Episode Epsilon: 0.7981, Episode Loss: 1.3809, Mean Score: 56.7778, Mean Reward 5.5689\n",
      "Episode 46/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7941, Episode Loss: 1.4089, Mean Score: 56.6739, Mean Reward 5.4674\n",
      "Episode 47/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.7901, Episode Loss: 4.5656, Mean Score: 56.5745, Mean Reward 5.3851\n",
      "Episode 48/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.7862, Episode Loss: 1.9580, Mean Score: 56.4792, Mean Reward 5.3042\n",
      "Episode 49/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.7822, Episode Loss: 48.9907, Mean Score: 56.3878, Mean Reward 5.2265\n",
      "Episode 50/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.7783, Episode Loss: 0.9510, Mean Score: 56.3000, Mean Reward 5.1620\n",
      "Model saved after episode 50\n",
      "Episode 51/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7744, Episode Loss: 0.5005, Mean Score: 56.2157, Mean Reward 5.0843\n",
      "Episode 52/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.7705, Episode Loss: 0.3931, Mean Score: 56.1346, Mean Reward 5.0250\n",
      "Episode 53/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7667, Episode Loss: 0.4724, Mean Score: 56.0566, Mean Reward 4.9453\n",
      "Episode 54/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7629, Episode Loss: 2.6204, Mean Score: 55.9815, Mean Reward 4.8722\n",
      "Episode 55/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7590, Episode Loss: 153.3139, Mean Score: 55.9091, Mean Reward 4.8000\n",
      "Episode 56/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.7553, Episode Loss: 1.7403, Mean Score: 55.8393, Mean Reward 4.7339\n",
      "Episode 57/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.9000, Episode Epsilon: 0.7515, Episode Loss: 2.1220, Mean Score: 55.7544, Mean Reward 4.6667\n",
      "Episode 58/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7477, Episode Loss: 28.7264, Mean Score: 55.6897, Mean Reward 4.6034\n",
      "Episode 59/200, Highest Score: 107, Episode Score: 52, Episode Reward: -0.3000, Episode Epsilon: 0.7440, Episode Loss: 0.9971, Mean Score: 55.6271, Mean Reward 4.5203\n",
      "Episode 60/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7403, Episode Loss: 0.6906, Mean Score: 55.5667, Mean Reward 4.4650\n",
      "Model saved after episode 60\n",
      "Episode 61/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7366, Episode Loss: 0.0223, Mean Score: 55.5082, Mean Reward 4.4082\n",
      "Episode 62/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.7329, Episode Loss: 0.1377, Mean Score: 55.4516, Mean Reward 4.3516\n",
      "Episode 63/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.5000, Episode Epsilon: 0.7292, Episode Loss: 2.1757, Mean Score: 55.3968, Mean Reward 4.2905\n",
      "Episode 64/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7256, Episode Loss: 1.8199, Mean Score: 55.3438, Mean Reward 4.2359\n",
      "Episode 65/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.4000, Episode Epsilon: 0.7219, Episode Loss: 10.8715, Mean Score: 55.2923, Mean Reward 4.1769\n",
      "Episode 66/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.7183, Episode Loss: 3.8364, Mean Score: 55.2424, Mean Reward 4.1303\n",
      "Episode 67/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.7147, Episode Loss: 1.2343, Mean Score: 55.1940, Mean Reward 4.0836\n",
      "Episode 68/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.1000, Episode Epsilon: 0.7112, Episode Loss: 107.9281, Mean Score: 55.1471, Mean Reward 4.0250\n",
      "Episode 69/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.7076, Episode Loss: 14.4213, Mean Score: 55.1014, Mean Reward 3.9841\n",
      "Episode 70/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.7000, Episode Epsilon: 0.7041, Episode Loss: 65.9295, Mean Score: 55.0429, Mean Reward 3.9371\n",
      "Model saved after episode 70\n",
      "Episode 71/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.7005, Episode Loss: 49.1321, Mean Score: 55.0000, Mean Reward 3.8930\n",
      "Episode 72/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.9000, Episode Epsilon: 0.6970, Episode Loss: 33.8774, Mean Score: 54.9583, Mean Reward 3.8514\n",
      "Episode 73/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.6936, Episode Loss: 24.5962, Mean Score: 54.9178, Mean Reward 3.8068\n",
      "Episode 74/200, Highest Score: 107, Episode Score: 54, Episode Reward: 1.5000, Episode Epsilon: 0.6901, Episode Loss: 8.5556, Mean Score: 54.9054, Mean Reward 3.7757\n",
      "Episode 75/200, Highest Score: 107, Episode Score: 54, Episode Reward: 1.4000, Episode Epsilon: 0.6866, Episode Loss: 22.9704, Mean Score: 54.8933, Mean Reward 3.7440\n",
      "Episode 76/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.2000, Episode Epsilon: 0.6832, Episode Loss: 1.0469, Mean Score: 54.8553, Mean Reward 3.6974\n",
      "Episode 77/200, Highest Score: 107, Episode Score: 58, Episode Reward: 5.4000, Episode Epsilon: 0.6798, Episode Loss: 0.0365, Mean Score: 54.8961, Mean Reward 3.7195\n",
      "Episode 78/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.6764, Episode Loss: 0.0982, Mean Score: 54.8590, Mean Reward 3.7115\n",
      "Episode 79/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.2000, Episode Epsilon: 0.6730, Episode Loss: 4.5786, Mean Score: 54.8228, Mean Reward 3.6671\n",
      "Episode 80/200, Highest Score: 107, Episode Score: 73, Episode Reward: 9.8000, Episode Epsilon: 0.6696, Episode Loss: 0.5954, Mean Score: 55.0500, Mean Reward 3.7437\n",
      "Model saved after episode 80\n",
      "Episode 81/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.6663, Episode Loss: 0.2715, Mean Score: 55.0000, Mean Reward 3.7321\n",
      "Episode 82/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.6630, Episode Loss: 0.6947, Mean Score: 54.9634, Mean Reward 3.7159\n",
      "Episode 83/200, Highest Score: 107, Episode Score: 57, Episode Reward: 4.0000, Episode Epsilon: 0.6597, Episode Loss: 0.0337, Mean Score: 54.9880, Mean Reward 3.7193\n",
      "Episode 84/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.6564, Episode Loss: 0.8716, Mean Score: 54.9524, Mean Reward 3.6833\n",
      "Episode 85/200, Highest Score: 107, Episode Score: 51, Episode Reward: 5.1000, Episode Epsilon: 0.6531, Episode Loss: 0.0562, Mean Score: 54.9059, Mean Reward 3.7000\n",
      "Episode 86/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.6498, Episode Loss: 0.8472, Mean Score: 54.8721, Mean Reward 3.6686\n",
      "Episode 87/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.6466, Episode Loss: 3.2860, Mean Score: 54.8391, Mean Reward 3.6356\n",
      "Episode 88/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.6433, Episode Loss: 0.3698, Mean Score: 54.8068, Mean Reward 3.6068\n",
      "Episode 89/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.3000, Episode Epsilon: 0.6401, Episode Loss: 0.0724, Mean Score: 54.7865, Mean Reward 3.5809\n",
      "Episode 90/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.6369, Episode Loss: 0.0005, Mean Score: 54.7556, Mean Reward 3.5489\n",
      "Model saved after episode 90\n",
      "Episode 91/200, Highest Score: 107, Episode Score: 80, Episode Reward: 11.2000, Episode Epsilon: 0.6337, Episode Loss: 0.4506, Mean Score: 55.0330, Mean Reward 3.6330\n",
      "Episode 92/200, Highest Score: 107, Episode Score: 59, Episode Reward: 2.2000, Episode Epsilon: 0.6306, Episode Loss: 40.8070, Mean Score: 55.0761, Mean Reward 3.6174\n",
      "Episode 93/200, Highest Score: 107, Episode Score: 64, Episode Reward: 5.2000, Episode Epsilon: 0.6274, Episode Loss: 0.7932, Mean Score: 55.1720, Mean Reward 3.6344\n",
      "Episode 94/200, Highest Score: 107, Episode Score: 59, Episode Reward: 4.6000, Episode Epsilon: 0.6243, Episode Loss: 3.4156, Mean Score: 55.2128, Mean Reward 3.6447\n",
      "Episode 95/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.2000, Episode Epsilon: 0.6211, Episode Loss: 3.1900, Mean Score: 55.1789, Mean Reward 3.6189\n",
      "Episode 96/200, Highest Score: 107, Episode Score: 87, Episode Reward: 8.4000, Episode Epsilon: 0.6180, Episode Loss: 0.0785, Mean Score: 55.5104, Mean Reward 3.6687\n",
      "Episode 97/200, Highest Score: 107, Episode Score: 64, Episode Reward: 7.2000, Episode Epsilon: 0.6149, Episode Loss: 1.0329, Mean Score: 55.5979, Mean Reward 3.7052\n",
      "Episode 98/200, Highest Score: 107, Episode Score: 68, Episode Reward: 6.7000, Episode Epsilon: 0.6119, Episode Loss: 0.3804, Mean Score: 55.7245, Mean Reward 3.7357\n",
      "Episode 99/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.9000, Episode Epsilon: 0.6088, Episode Loss: 0.0000, Mean Score: 55.6768, Mean Reward 3.7172\n",
      "Episode 100/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.6000, Episode Epsilon: 0.6058, Episode Loss: 3.5528, Mean Score: 55.6400, Mean Reward 3.6960\n",
      "Model saved after episode 100\n",
      "Episode 101/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.5000, Episode Epsilon: 0.6027, Episode Loss: 3.0433, Mean Score: 55.6139, Mean Reward 3.6941\n",
      "Episode 102/200, Highest Score: 107, Episode Score: 72, Episode Reward: 7.4000, Episode Epsilon: 0.5997, Episode Loss: 0.1833, Mean Score: 55.7745, Mean Reward 3.7304\n",
      "Episode 103/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5967, Episode Loss: 7.4107, Mean Score: 55.7379, Mean Reward 3.7039\n",
      "Episode 104/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5937, Episode Loss: 0.0599, Mean Score: 55.7019, Mean Reward 3.6779\n",
      "Episode 105/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.0000, Episode Epsilon: 0.5908, Episode Loss: 0.0004, Mean Score: 55.6762, Mean Reward 3.6619\n",
      "Episode 106/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.5000, Episode Epsilon: 0.5878, Episode Loss: 0.2960, Mean Score: 55.6415, Mean Reward 3.6321\n",
      "Episode 107/200, Highest Score: 107, Episode Score: 62, Episode Reward: 6.8000, Episode Epsilon: 0.5849, Episode Loss: 2.5164, Mean Score: 55.7009, Mean Reward 3.6617\n",
      "Episode 108/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.7000, Episode Epsilon: 0.5820, Episode Loss: 8.9337, Mean Score: 55.6667, Mean Reward 3.6435\n",
      "Episode 109/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.3000, Episode Epsilon: 0.5790, Episode Loss: 17.4534, Mean Score: 55.6239, Mean Reward 3.6220\n",
      "Episode 110/200, Highest Score: 107, Episode Score: 51, Episode Reward: 1.4000, Episode Epsilon: 0.5762, Episode Loss: 11.2138, Mean Score: 55.5818, Mean Reward 3.6018\n",
      "Model saved after episode 110\n",
      "Episode 111/200, Highest Score: 107, Episode Score: 75, Episode Reward: 5.0000, Episode Epsilon: 0.5733, Episode Loss: 0.1608, Mean Score: 55.7568, Mean Reward 3.6144\n",
      "Episode 112/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.3000, Episode Epsilon: 0.5704, Episode Loss: 0.2990, Mean Score: 55.7321, Mean Reward 3.6027\n",
      "Episode 113/200, Highest Score: 107, Episode Score: 64, Episode Reward: 7.1000, Episode Epsilon: 0.5676, Episode Loss: 1.0937, Mean Score: 55.8053, Mean Reward 3.6336\n",
      "Episode 114/200, Highest Score: 107, Episode Score: 53, Episode Reward: 1.7000, Episode Epsilon: 0.5647, Episode Loss: 0.3800, Mean Score: 55.7807, Mean Reward 3.6167\n",
      "Episode 115/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.5619, Episode Loss: 1.3537, Mean Score: 55.7478, Mean Reward 3.6052\n",
      "Episode 116/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.5591, Episode Loss: 0.3103, Mean Score: 55.7155, Mean Reward 3.5957\n",
      "Episode 117/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.6000, Episode Epsilon: 0.5563, Episode Loss: 0.4667, Mean Score: 55.6838, Mean Reward 3.5701\n",
      "Episode 118/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.5535, Episode Loss: 3.3259, Mean Score: 55.6525, Mean Reward 3.5551\n",
      "Episode 119/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.1000, Episode Epsilon: 0.5507, Episode Loss: 8.9104, Mean Score: 55.6218, Mean Reward 3.5345\n",
      "Episode 120/200, Highest Score: 107, Episode Score: 55, Episode Reward: 2.6000, Episode Epsilon: 0.5480, Episode Loss: 1.7029, Mean Score: 55.6167, Mean Reward 3.5267\n",
      "Model saved after episode 120\n",
      "Episode 121/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.5452, Episode Loss: 8.9010, Mean Score: 55.5868, Mean Reward 3.5033\n",
      "Episode 122/200, Highest Score: 107, Episode Score: 51, Episode Reward: 0.6000, Episode Epsilon: 0.5425, Episode Loss: 0.1592, Mean Score: 55.5492, Mean Reward 3.4795\n",
      "Episode 123/200, Highest Score: 107, Episode Score: 69, Episode Reward: 16.5000, Episode Epsilon: 0.5398, Episode Loss: 1.2421, Mean Score: 55.6585, Mean Reward 3.5854\n",
      "Episode 124/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.0000, Episode Epsilon: 0.5371, Episode Loss: 3.6546, Mean Score: 55.6290, Mean Reward 3.5645\n",
      "Episode 125/200, Highest Score: 107, Episode Score: 53, Episode Reward: 0.7000, Episode Epsilon: 0.5344, Episode Loss: 0.6453, Mean Score: 55.6080, Mean Reward 3.5416\n",
      "Episode 126/200, Highest Score: 107, Episode Score: 65, Episode Reward: 6.9000, Episode Epsilon: 0.5318, Episode Loss: 0.0496, Mean Score: 55.6825, Mean Reward 3.5683\n",
      "Episode 127/200, Highest Score: 107, Episode Score: 52, Episode Reward: 0.7000, Episode Epsilon: 0.5291, Episode Loss: 0.7174, Mean Score: 55.6535, Mean Reward 3.5457\n",
      "Episode 128/200, Highest Score: 107, Episode Score: 62, Episode Reward: 8.8000, Episode Epsilon: 0.5264, Episode Loss: 0.0052, Mean Score: 55.7031, Mean Reward 3.5867\n",
      "Episode 129/200, Highest Score: 107, Episode Score: 58, Episode Reward: 4.7000, Episode Epsilon: 0.5238, Episode Loss: 0.6841, Mean Score: 55.7209, Mean Reward 3.5953\n",
      "Episode 130/200, Highest Score: 107, Episode Score: 51, Episode Reward: -0.1000, Episode Epsilon: 0.5212, Episode Loss: 0.0960, Mean Score: 55.6846, Mean Reward 3.5669\n",
      "Model saved after episode 130\n",
      "Episode 131/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.5186, Episode Loss: 0.2749, Mean Score: 55.6565, Mean Reward 3.5534\n",
      "Episode 132/200, Highest Score: 107, Episode Score: 56, Episode Reward: 7.2000, Episode Epsilon: 0.5160, Episode Loss: 0.0727, Mean Score: 55.6591, Mean Reward 3.5811\n",
      "Episode 133/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.5134, Episode Loss: 0.0260, Mean Score: 55.6316, Mean Reward 3.5752\n",
      "Episode 134/200, Highest Score: 107, Episode Score: 52, Episode Reward: 4.4000, Episode Epsilon: 0.5108, Episode Loss: 0.5561, Mean Score: 55.6045, Mean Reward 3.5813\n",
      "Episode 135/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.5083, Episode Loss: 4.7097, Mean Score: 55.5778, Mean Reward 3.5719\n",
      "Episode 136/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.5058, Episode Loss: 4.4444, Mean Score: 55.5515, Mean Reward 3.5669\n",
      "Episode 137/200, Highest Score: 107, Episode Score: 55, Episode Reward: 7.9000, Episode Epsilon: 0.5032, Episode Loss: 0.9956, Mean Score: 55.5474, Mean Reward 3.5985\n",
      "Episode 138/200, Highest Score: 107, Episode Score: 66, Episode Reward: 18.2000, Episode Epsilon: 0.5007, Episode Loss: 31.1139, Mean Score: 55.6232, Mean Reward 3.7043\n",
      "Episode 139/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.4982, Episode Loss: 1.7099, Mean Score: 55.5971, Mean Reward 3.7014\n",
      "Episode 140/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.3000, Episode Epsilon: 0.4957, Episode Loss: 1.6931, Mean Score: 55.5643, Mean Reward 3.6986\n",
      "Model saved after episode 140\n",
      "Episode 141/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.4932, Episode Loss: 0.0301, Mean Score: 55.5390, Mean Reward 3.6943\n",
      "Episode 142/200, Highest Score: 107, Episode Score: 59, Episode Reward: 9.6000, Episode Epsilon: 0.4908, Episode Loss: 0.0639, Mean Score: 55.5634, Mean Reward 3.7359\n",
      "Episode 143/200, Highest Score: 107, Episode Score: 56, Episode Reward: 7.7000, Episode Epsilon: 0.4883, Episode Loss: 0.0147, Mean Score: 55.5664, Mean Reward 3.7636\n",
      "Episode 144/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.3000, Episode Epsilon: 0.4859, Episode Loss: 31.5657, Mean Score: 55.5417, Mean Reward 3.7604\n",
      "Episode 145/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.4000, Episode Epsilon: 0.4834, Episode Loss: 3.4315, Mean Score: 55.5241, Mean Reward 3.7579\n",
      "Episode 146/200, Highest Score: 107, Episode Score: 52, Episode Reward: 4.1000, Episode Epsilon: 0.4810, Episode Loss: 281.8986, Mean Score: 55.5000, Mean Reward 3.7603\n",
      "Episode 147/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.3000, Episode Epsilon: 0.4786, Episode Loss: 9.0104, Mean Score: 55.4694, Mean Reward 3.7639\n",
      "Episode 148/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.4000, Episode Epsilon: 0.4762, Episode Loss: 1.4367, Mean Score: 55.4392, Mean Reward 3.7682\n",
      "Episode 149/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4738, Episode Loss: 0.5401, Mean Score: 55.4094, Mean Reward 3.7698\n",
      "Episode 150/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4715, Episode Loss: 1.2505, Mean Score: 55.3800, Mean Reward 3.7713\n",
      "Model saved after episode 150\n",
      "Episode 151/200, Highest Score: 107, Episode Score: 51, Episode Reward: 5.0000, Episode Epsilon: 0.4691, Episode Loss: 0.6032, Mean Score: 55.3510, Mean Reward 3.7795\n",
      "Episode 152/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.9000, Episode Epsilon: 0.4668, Episode Loss: 0.1157, Mean Score: 55.3289, Mean Reward 3.7803\n",
      "Episode 153/200, Highest Score: 107, Episode Score: 53, Episode Reward: 3.3000, Episode Epsilon: 0.4644, Episode Loss: 0.0260, Mean Score: 55.3137, Mean Reward 3.7771\n",
      "Episode 154/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.4621, Episode Loss: 1.3185, Mean Score: 55.2922, Mean Reward 3.7727\n",
      "Episode 155/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4598, Episode Loss: 0.0984, Mean Score: 55.2710, Mean Reward 3.7671\n",
      "Episode 156/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.3000, Episode Epsilon: 0.4575, Episode Loss: 0.0113, Mean Score: 55.2564, Mean Reward 3.7577\n",
      "Episode 157/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.9000, Episode Epsilon: 0.4552, Episode Loss: 0.3035, Mean Score: 55.2293, Mean Reward 3.7522\n",
      "Episode 158/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4529, Episode Loss: 1.5886, Mean Score: 55.2089, Mean Reward 3.7487\n",
      "Episode 159/200, Highest Score: 107, Episode Score: 81, Episode Reward: 15.3000, Episode Epsilon: 0.4507, Episode Loss: 0.5645, Mean Score: 55.3711, Mean Reward 3.8214\n",
      "Episode 160/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.3000, Episode Epsilon: 0.4484, Episode Loss: 0.0000, Mean Score: 55.3500, Mean Reward 3.8119\n",
      "Model saved after episode 160\n",
      "Episode 161/200, Highest Score: 107, Episode Score: 53, Episode Reward: 2.5000, Episode Epsilon: 0.4462, Episode Loss: 0.3420, Mean Score: 55.3354, Mean Reward 3.8037\n",
      "Episode 162/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.4000, Episode Epsilon: 0.4440, Episode Loss: 0.1420, Mean Score: 55.3086, Mean Reward 3.8012\n",
      "Episode 163/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4417, Episode Loss: 0.0057, Mean Score: 55.2883, Mean Reward 3.7975\n",
      "Episode 164/200, Highest Score: 107, Episode Score: 51, Episode Reward: 4.0000, Episode Epsilon: 0.4395, Episode Loss: 0.1781, Mean Score: 55.2622, Mean Reward 3.7988\n",
      "Episode 165/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.7000, Episode Epsilon: 0.4373, Episode Loss: 0.0252, Mean Score: 55.2364, Mean Reward 3.7982\n",
      "Episode 166/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4351, Episode Loss: 0.4278, Mean Score: 55.2169, Mean Reward 3.7928\n",
      "Episode 167/200, Highest Score: 107, Episode Score: 52, Episode Reward: 6.1000, Episode Epsilon: 0.4330, Episode Loss: 1.0723, Mean Score: 55.1976, Mean Reward 3.8066\n",
      "Episode 168/200, Highest Score: 107, Episode Score: 54, Episode Reward: 3.7000, Episode Epsilon: 0.4308, Episode Loss: 6.9759, Mean Score: 55.1905, Mean Reward 3.8060\n",
      "Episode 169/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.2000, Episode Epsilon: 0.4286, Episode Loss: 5.2515, Mean Score: 55.1716, Mean Reward 3.8024\n",
      "Episode 170/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.0000, Episode Epsilon: 0.4265, Episode Loss: 0.3292, Mean Score: 55.1529, Mean Reward 3.7918\n",
      "Model saved after episode 170\n",
      "Episode 171/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.4244, Episode Loss: 0.4206, Mean Score: 55.1345, Mean Reward 3.7836\n",
      "Episode 172/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.4223, Episode Loss: 1.5237, Mean Score: 55.1163, Mean Reward 3.7756\n",
      "Episode 173/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4201, Episode Loss: 1.0142, Mean Score: 55.0983, Mean Reward 3.7665\n",
      "Episode 174/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.4180, Episode Loss: 0.2204, Mean Score: 55.0805, Mean Reward 3.7552\n",
      "Episode 175/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.8000, Episode Epsilon: 0.4159, Episode Loss: 13.9334, Mean Score: 55.0629, Mean Reward 3.7497\n",
      "Episode 176/200, Highest Score: 107, Episode Score: 70, Episode Reward: 6.7000, Episode Epsilon: 0.4139, Episode Loss: 0.5093, Mean Score: 55.1477, Mean Reward 3.7665\n",
      "Episode 177/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.9000, Episode Epsilon: 0.4118, Episode Loss: 0.0824, Mean Score: 55.1299, Mean Reward 3.7616\n",
      "Episode 178/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4097, Episode Loss: 29.8914, Mean Score: 55.1124, Mean Reward 3.7528\n",
      "Episode 179/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.6000, Episode Epsilon: 0.4077, Episode Loss: 74.7894, Mean Score: 55.0950, Mean Reward 3.7464\n",
      "Episode 180/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.9000, Episode Epsilon: 0.4057, Episode Loss: 1.7137, Mean Score: 55.0722, Mean Reward 3.7417\n",
      "Model saved after episode 180\n",
      "Episode 181/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.4036, Episode Loss: 64.7228, Mean Score: 55.0552, Mean Reward 3.7331\n",
      "Episode 182/200, Highest Score: 107, Episode Score: 66, Episode Reward: 5.1000, Episode Epsilon: 0.4016, Episode Loss: 1.3572, Mean Score: 55.1154, Mean Reward 3.7407\n",
      "Episode 183/200, Highest Score: 107, Episode Score: 51, Episode Reward: 3.0000, Episode Epsilon: 0.3996, Episode Loss: 13.2291, Mean Score: 55.0929, Mean Reward 3.7366\n",
      "Episode 184/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.9000, Episode Epsilon: 0.3976, Episode Loss: 301.9651, Mean Score: 55.0761, Mean Reward 3.7266\n",
      "Episode 185/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.3956, Episode Loss: 2.0377, Mean Score: 55.0595, Mean Reward 3.7184\n",
      "Episode 186/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3936, Episode Loss: 15.9504, Mean Score: 55.0430, Mean Reward 3.7151\n",
      "Episode 187/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.6000, Episode Epsilon: 0.3917, Episode Loss: 2.0032, Mean Score: 55.0214, Mean Reward 3.7091\n",
      "Episode 188/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.3897, Episode Loss: 32.3000, Mean Score: 55.0053, Mean Reward 3.6989\n",
      "Episode 189/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.0000, Episode Epsilon: 0.3878, Episode Loss: 1.7364, Mean Score: 54.9841, Mean Reward 3.6899\n",
      "Episode 190/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.5000, Episode Epsilon: 0.3858, Episode Loss: 171.5635, Mean Score: 54.9684, Mean Reward 3.6837\n",
      "Model saved after episode 190\n",
      "Episode 191/200, Highest Score: 107, Episode Score: 52, Episode Reward: 3.1000, Episode Epsilon: 0.3839, Episode Loss: 3.4848, Mean Score: 54.9529, Mean Reward 3.6806\n",
      "Episode 192/200, Highest Score: 107, Episode Score: 51, Episode Reward: 2.8000, Episode Epsilon: 0.3820, Episode Loss: 0.4712, Mean Score: 54.9323, Mean Reward 3.6760\n",
      "Episode 193/200, Highest Score: 107, Episode Score: 52, Episode Reward: 1.8000, Episode Epsilon: 0.3801, Episode Loss: 21.8032, Mean Score: 54.9171, Mean Reward 3.6663\n",
      "Episode 194/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.2000, Episode Epsilon: 0.3782, Episode Loss: 4.0838, Mean Score: 54.9021, Mean Reward 3.6588\n",
      "Episode 195/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3763, Episode Loss: 0.2070, Mean Score: 54.8872, Mean Reward 3.6508\n",
      "Episode 196/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3744, Episode Loss: 2512.4690, Mean Score: 54.8724, Mean Reward 3.6429\n",
      "Episode 197/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.1000, Episode Epsilon: 0.3725, Episode Loss: 1.1048, Mean Score: 54.8579, Mean Reward 3.6350\n",
      "Episode 198/200, Highest Score: 107, Episode Score: 71, Episode Reward: 6.8000, Episode Epsilon: 0.3707, Episode Loss: 0.5458, Mean Score: 54.9394, Mean Reward 3.6510\n",
      "Episode 199/200, Highest Score: 107, Episode Score: 63, Episode Reward: 4.0000, Episode Epsilon: 0.3688, Episode Loss: 0.2220, Mean Score: 54.9799, Mean Reward 3.6528\n",
      "Episode 200/200, Highest Score: 107, Episode Score: 52, Episode Reward: 2.4000, Episode Epsilon: 0.3670, Episode Loss: 1.2379, Mean Score: 54.9650, Mean Reward 3.6465\n",
      "Model saved after episode 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>episode_epsilon</td><td>██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>episode_loss</td><td>▁▂▁▁▁▁▁▁▁▁▁▁▁▄▂▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂█▂▁▁</td></tr><tr><td>episode_reward</td><td>▁▂▄▃▂▅▁▁▂▁▂▁▁▁▁▂▂▁▂▄▄▂▂▂▁▄▃█▂▃▂▂▃▂▂▂▂▂▂▂</td></tr><tr><td>episode_score</td><td>▂▁▄▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▄▇█▁▂▁▁▅▁▆▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>highest_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_current_score</td><td>█▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_loss</td><td>▁▂▁▁▁▁▁▁▁▁▁▁▁▄▂▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂█▂▁▁</td></tr><tr><td>mean_reward</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0.36696</td></tr><tr><td>episode_loss</td><td>1.23786</td></tr><tr><td>episode_reward</td><td>2.4</td></tr><tr><td>episode_score</td><td>52</td></tr><tr><td>highest_score</td><td>107</td></tr><tr><td>mean_current_score</td><td>54.965</td></tr><tr><td>mean_loss</td><td>1.23786</td></tr><tr><td>mean_reward</td><td>3.6465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/cwainy14</a><br/>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_214120-cwainy14\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Train Model\n",
    "train(agent, env, TRAIN_EPISODES, OUTPUT_DIR, log_to_wandb=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05144910-6083-4497-9db9-3f36dd8d2c1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e8afd46-99e3-41a9-82a2-49072b4e5db0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "85b3b016-4d07-4470-b19d-f68954e8cd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(agent, env, episodes, model_path, log_to_wandb=False, older_model=False, render=False):\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.init(project='chrome_dino_rl_agent', name='test_run')\n",
    "\n",
    "    total_rewards = []\n",
    "    total_scores = []\n",
    "\n",
    "    agent.load_model(model_path, older_model, for_training=False)\n",
    "\n",
    "    # Set exploration rate (epsilon) to 0 to only choose actions based on the model's predictions (exploit its knowledge)\n",
    "    agent.epsilon = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render(mode='human')\n",
    "\n",
    "            # Use agent to predict action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_scores.append(info[\"current_score\"])\n",
    "\n",
    "        # Calculate overall training metrics\n",
    "        mean_reward = sum(total_rewards) / len(total_rewards)\n",
    "        mean_score = sum(total_scores) / len(total_scores)\n",
    "\n",
    "        # Log metrics\n",
    "        print(\n",
    "            f\"Episode {episode + 1}/{episodes}, Highest Score: {info['high_score']}, Episode Score: {info['current_score']}, Episode Reward: {episode_reward:.4f}, Episode Epsilon: {agent.epsilon:.4f}, Mean Score: {mean_score:.4f}, Mean Reward {mean_reward:.4f}\")\n",
    "\n",
    "        if log_to_wandb:\n",
    "            wandb.log({\n",
    "                \"episode\": (episode + 1)/episodes,\n",
    "                \"highest_score\": info[\"high_score\"],\n",
    "                \"episode_score\": info[\"current_score\"],\n",
    "                \"episode_reward\": episode_reward,\n",
    "                \"episode_epsilon\": agent.epsilon,\n",
    "                \"mean_reward\": mean_reward,\n",
    "                \"mean_current_score\": mean_score\n",
    "            })\n",
    "            \n",
    "    if log_to_wandb:\n",
    "        # Finish wandb logging        \n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fafc388-89d9-42a8-b9b1-ac8557b7d92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c6ebd52c-a8cd-46e9-9e29-7b5dbb66e9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3b3054a3-6522-48a1-aca0-f58a9eb64637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"trained_models/dino_dqn_episode_200.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "28844155-561c-494c-bccd-685b6dde5758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Highest Score: 64, Episode Score: 64, Episode Reward: 110.6000, Episode Epsilon: 0.0000, Mean Score: 64.0000, Mean Reward 110.6000\n",
      "Episode 2/5, Highest Score: 64, Episode Score: 52, Episode Reward: 1.5000, Episode Epsilon: 0.0000, Mean Score: 58.0000, Mean Reward 56.0500\n",
      "Episode 3/5, Highest Score: 64, Episode Score: 52, Episode Reward: 1.4000, Episode Epsilon: 0.0000, Mean Score: 56.0000, Mean Reward 37.8333\n",
      "Episode 4/5, Highest Score: 64, Episode Score: 54, Episode Reward: 2.0000, Episode Epsilon: 0.0000, Mean Score: 55.5000, Mean Reward 28.8750\n",
      "Episode 5/5, Highest Score: 64, Episode Score: 52, Episode Reward: 0.8000, Episode Epsilon: 0.0000, Mean Score: 54.8000, Mean Reward 23.2600\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b96b0dc1-46c2-402e-b877-f1d5cc199b4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Best Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e422a356-b6b1-4f42-95e3-506d6752be7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify path to load a model\n",
    "MODEL_LOAD_PATH = \"best_trained_models\\episode_100.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3fbfb8b5-b0b5-45ce-bd31-a5dc29db5212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of episodes to test the agent\n",
    "TEST_EPISODES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0bbd7344-6506-40b2-b00d-e6f0a9691bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\malvi\\Desktop\\COMP3071-Designing-Intelligent-Agents\\COMP3071-DIA-CW\\src\\wandb\\run-20230507_222901-bsp2lmb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">test_run</a></strong> to <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50, Highest Score: 362, Episode Score: 362, Episode Reward: 508.0000, Episode Epsilon: 0.0000, Mean Score: 362.0000, Mean Reward 508.0000\n",
      "Episode 2/50, Highest Score: 403, Episode Score: 403, Episode Reward: 84.7000, Episode Epsilon: 0.0000, Mean Score: 382.5000, Mean Reward 296.3500\n",
      "Episode 3/50, Highest Score: 442, Episode Score: 442, Episode Reward: 54.4000, Episode Epsilon: 0.0000, Mean Score: 402.3333, Mean Reward 215.7000\n",
      "Episode 4/50, Highest Score: 663, Episode Score: 663, Episode Reward: 286.5000, Episode Epsilon: 0.0000, Mean Score: 467.5000, Mean Reward 233.4000\n",
      "Episode 5/50, Highest Score: 663, Episode Score: 260, Episode Reward: 31.4000, Episode Epsilon: 0.0000, Mean Score: 426.0000, Mean Reward 193.0000\n",
      "Episode 6/50, Highest Score: 663, Episode Score: 282, Episode Reward: 33.6000, Episode Epsilon: 0.0000, Mean Score: 402.0000, Mean Reward 166.4333\n",
      "Episode 7/50, Highest Score: 663, Episode Score: 287, Episode Reward: 33.3000, Episode Epsilon: 0.0000, Mean Score: 385.5714, Mean Reward 147.4143\n",
      "Episode 8/50, Highest Score: 663, Episode Score: 221, Episode Reward: 27.0000, Episode Epsilon: 0.0000, Mean Score: 365.0000, Mean Reward 132.3625\n",
      "Episode 9/50, Highest Score: 663, Episode Score: 284, Episode Reward: 35.6000, Episode Epsilon: 0.0000, Mean Score: 356.0000, Mean Reward 121.6111\n",
      "Episode 10/50, Highest Score: 711, Episode Score: 711, Episode Reward: 124.0000, Episode Epsilon: 0.0000, Mean Score: 391.5000, Mean Reward 121.8500\n",
      "Episode 11/50, Highest Score: 711, Episode Score: 271, Episode Reward: 31.9000, Episode Epsilon: 0.0000, Mean Score: 380.5455, Mean Reward 113.6727\n",
      "Episode 12/50, Highest Score: 711, Episode Score: 269, Episode Reward: 30.6000, Episode Epsilon: 0.0000, Mean Score: 371.2500, Mean Reward 106.7500\n",
      "Episode 13/50, Highest Score: 711, Episode Score: 588, Episode Reward: 63.5000, Episode Epsilon: 0.0000, Mean Score: 387.9231, Mean Reward 103.4231\n",
      "Episode 14/50, Highest Score: 711, Episode Score: 408, Episode Reward: 45.8000, Episode Epsilon: 0.0000, Mean Score: 389.3571, Mean Reward 99.3071\n",
      "Episode 15/50, Highest Score: 711, Episode Score: 551, Episode Reward: 61.2000, Episode Epsilon: 0.0000, Mean Score: 400.1333, Mean Reward 96.7667\n",
      "Episode 16/50, Highest Score: 711, Episode Score: 362, Episode Reward: 44.0000, Episode Epsilon: 0.0000, Mean Score: 397.7500, Mean Reward 93.4688\n",
      "Episode 17/50, Highest Score: 711, Episode Score: 488, Episode Reward: 55.8000, Episode Epsilon: 0.0000, Mean Score: 403.0588, Mean Reward 91.2529\n",
      "Episode 18/50, Highest Score: 711, Episode Score: 176, Episode Reward: 22.6000, Episode Epsilon: 0.0000, Mean Score: 390.4444, Mean Reward 87.4389\n",
      "Episode 19/50, Highest Score: 711, Episode Score: 389, Episode Reward: 45.0000, Episode Epsilon: 0.0000, Mean Score: 390.3684, Mean Reward 85.2053\n",
      "Episode 20/50, Highest Score: 711, Episode Score: 425, Episode Reward: 54.7000, Episode Epsilon: 0.0000, Mean Score: 392.1000, Mean Reward 83.6800\n",
      "Episode 21/50, Highest Score: 711, Episode Score: 55, Episode Reward: 29.0000, Episode Epsilon: 0.0000, Mean Score: 376.0476, Mean Reward 81.0762\n",
      "Episode 22/50, Highest Score: 711, Episode Score: 445, Episode Reward: 71.1000, Episode Epsilon: 0.0000, Mean Score: 379.1818, Mean Reward 80.6227\n",
      "Episode 23/50, Highest Score: 778, Episode Score: 778, Episode Reward: 160.8000, Episode Epsilon: 0.0000, Mean Score: 396.5217, Mean Reward 84.1087\n",
      "Episode 24/50, Highest Score: 778, Episode Score: 517, Episode Reward: 79.7000, Episode Epsilon: 0.0000, Mean Score: 401.5417, Mean Reward 83.9250\n",
      "Episode 25/50, Highest Score: 778, Episode Score: 286, Episode Reward: 47.7000, Episode Epsilon: 0.0000, Mean Score: 396.9200, Mean Reward 82.4760\n",
      "Episode 26/50, Highest Score: 1107, Episode Score: 1107, Episode Reward: 575.1000, Episode Epsilon: 0.0000, Mean Score: 424.2308, Mean Reward 101.4231\n",
      "Episode 27/50, Highest Score: 1107, Episode Score: 593, Episode Reward: 88.6000, Episode Epsilon: 0.0000, Mean Score: 430.4815, Mean Reward 100.9481\n",
      "Episode 28/50, Highest Score: 1107, Episode Score: 492, Episode Reward: 77.4000, Episode Epsilon: 0.0000, Mean Score: 432.6786, Mean Reward 100.1071\n",
      "Episode 29/50, Highest Score: 1107, Episode Score: 323, Episode Reward: 53.6000, Episode Epsilon: 0.0000, Mean Score: 428.8966, Mean Reward 98.5034\n",
      "Episode 30/50, Highest Score: 1107, Episode Score: 299, Episode Reward: 50.7000, Episode Epsilon: 0.0000, Mean Score: 424.5667, Mean Reward 96.9100\n",
      "Episode 31/50, Highest Score: 1107, Episode Score: 384, Episode Reward: 61.6000, Episode Epsilon: 0.0000, Mean Score: 423.2581, Mean Reward 95.7710\n",
      "Episode 32/50, Highest Score: 1107, Episode Score: 55, Episode Reward: 61.8000, Episode Epsilon: 0.0000, Mean Score: 411.7500, Mean Reward 94.7094\n",
      "Episode 33/50, Highest Score: 1107, Episode Score: 55, Episode Reward: 12.2000, Episode Epsilon: 0.0000, Mean Score: 400.9394, Mean Reward 92.2091\n",
      "Episode 34/50, Highest Score: 1467, Episode Score: 1467, Episode Reward: 626.9000, Episode Epsilon: 0.0000, Mean Score: 432.2941, Mean Reward 107.9353\n",
      "Episode 35/50, Highest Score: 1467, Episode Score: 539, Episode Reward: 80.8000, Episode Epsilon: 0.0000, Mean Score: 435.3429, Mean Reward 107.1600\n",
      "Episode 36/50, Highest Score: 1467, Episode Score: 780, Episode Reward: 121.0000, Episode Epsilon: 0.0000, Mean Score: 444.9167, Mean Reward 107.5444\n",
      "Episode 37/50, Highest Score: 1467, Episode Score: 1186, Episode Reward: 170.4000, Episode Epsilon: 0.0000, Mean Score: 464.9459, Mean Reward 109.2432\n",
      "Episode 38/50, Highest Score: 1467, Episode Score: 634, Episode Reward: 96.9000, Episode Epsilon: 0.0000, Mean Score: 469.3947, Mean Reward 108.9184\n",
      "Episode 39/50, Highest Score: 1467, Episode Score: 1333, Episode Reward: 189.4000, Episode Epsilon: 0.0000, Mean Score: 491.5385, Mean Reward 110.9821\n",
      "Episode 40/50, Highest Score: 1467, Episode Score: 432, Episode Reward: 70.5000, Episode Epsilon: 0.0000, Mean Score: 490.0500, Mean Reward 109.9700\n",
      "Episode 41/50, Highest Score: 1467, Episode Score: 313, Episode Reward: 51.7000, Episode Epsilon: 0.0000, Mean Score: 485.7317, Mean Reward 108.5488\n",
      "Episode 42/50, Highest Score: 1467, Episode Score: 242, Episode Reward: 43.4000, Episode Epsilon: 0.0000, Mean Score: 479.9286, Mean Reward 106.9976\n",
      "Episode 43/50, Highest Score: 1467, Episode Score: 934, Episode Reward: 137.5000, Episode Epsilon: 0.0000, Mean Score: 490.4884, Mean Reward 107.7070\n",
      "Episode 44/50, Highest Score: 1467, Episode Score: 368, Episode Reward: 59.1000, Episode Epsilon: 0.0000, Mean Score: 487.7045, Mean Reward 106.6023\n",
      "Episode 45/50, Highest Score: 1467, Episode Score: 520, Episode Reward: 77.4000, Episode Epsilon: 0.0000, Mean Score: 488.4222, Mean Reward 105.9533\n",
      "Episode 46/50, Highest Score: 1467, Episode Score: 265, Episode Reward: 45.5000, Episode Epsilon: 0.0000, Mean Score: 483.5652, Mean Reward 104.6391\n",
      "Episode 47/50, Highest Score: 1467, Episode Score: 1121, Episode Reward: 162.3000, Episode Epsilon: 0.0000, Mean Score: 497.1277, Mean Reward 105.8660\n",
      "Episode 48/50, Highest Score: 1467, Episode Score: 838, Episode Reward: 122.2000, Episode Epsilon: 0.0000, Mean Score: 504.2292, Mean Reward 106.2062\n",
      "Episode 49/50, Highest Score: 1467, Episode Score: 821, Episode Reward: 123.8000, Episode Epsilon: 0.0000, Mean Score: 510.6939, Mean Reward 106.5653\n",
      "Episode 50/50, Highest Score: 1467, Episode Score: 261, Episode Reward: 42.3000, Episode Epsilon: 0.0000, Mean Score: 505.7000, Mean Reward 105.2800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_epsilon</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode_reward</td><td>▇▂▁▄▁▁▁▁▁▁▂▁▁▁▁▁▁▂▃▂▇▂▂▁▂▂▁█▂▃▂▃▁▁▂▂▁▃▂▁</td></tr><tr><td>episode_score</td><td>▃▃▃▄▂▂▂▂▂▂▄▃▃▃▂▃▁▃▅▃▆▄▃▂▃▁▁█▅▇▄▇▂▂▅▃▂▆▅▂</td></tr><tr><td>highest_score</td><td>▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▆▆▆▆▆▆▆█████████████</td></tr><tr><td>mean_current_score</td><td>▁▂▃▆▃▂▁▁▂▂▂▃▃▃▃▃▂▂▃▃▄▄▅▄▄▄▃▅▅▆▆▇▇▇▇▇▇███</td></tr><tr><td>mean_reward</td><td>█▅▃▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>1.0</td></tr><tr><td>episode_epsilon</td><td>0</td></tr><tr><td>episode_reward</td><td>42.3</td></tr><tr><td>episode_score</td><td>261</td></tr><tr><td>highest_score</td><td>1467</td></tr><tr><td>mean_current_score</td><td>505.7</td></tr><tr><td>mean_reward</td><td>105.28</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_run</strong> at: <a href='https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4' target=\"_blank\">https://wandb.ai/bidmalvi/chrome_dino_rl_agent/runs/bsp2lmb4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230507_222901-bsp2lmb4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiate Environment and Agent\n",
    "env = DinoEnvironment()\n",
    "agent = DinoDQNAgent(env)\n",
    "\n",
    "# Test model\n",
    "test(agent, env, TEST_EPISODES, MODEL_LOAD_PATH, log_to_wandb=True, older_model=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
